[{"authors":["andreashandel"],"categories":null,"content":"I am a Professor in the Department of Epidemiology and Biostatistics, College of Public Health, at the University of Georgia (UGA).\nThis website contains a short bio and products such as blog posts, presentations, and various tools and resources related to teaching and research.\nMore research related details can be found on my research group site. Some overlap between the two sites might occur.\nI send out a monthly (or less frequent) newsletter with updates and musings on my various projects. If interested, you can subscribe here.\n","date":1632441600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632441600,"objectID":"ac8f6cd865a390b0b63aca8a75624b4b","permalink":"https://www.andreashandel.com/author/andreas-handel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/andreas-handel/","section":"authors","summary":"I am a Professor in the Department of Epidemiology and Biostatistics, College of Public Health, at the University of Georgia (UGA).\nThis website contains a short bio and products such as blog posts, presentations, and various tools and resources related to teaching and research.","tags":null,"title":"Andreas Handel","type":"authors"},{"authors":null,"categories":["R","Data Analysis"],"content":"This analysis was performed as part of an exercise for my Modern Applied Data Analysis course taught in fall 2021.\nOne of the weekly assignments for the students is to participate in Tidy Tuesday. I did the exercise as well, this is my product. You can get the R Markdown file to re-run the analysis here.\nIntroduction If you are not familiar with Tidy Tuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week\u0026rsquo;s data was about an analysis of economic working papers catalogued by NBER. More on the data is here.\nLoading packages Make sure they are installed. Note: I don\u0026rsquo;t like loading meta-packages, such as the tidyverse. Doing so makes it really hard to figure our which packages are actually used. So I prefer to only load what I need.\nlibrary('ggplot2')\rlibrary('readr')\rlibrary('dplyr')\rlibrary('stringr')\rlibrary('tidytuesdayR')\r Data loading Apparently there is now a tidytuesdayR package that makes data loading very easy!\n#tuesdata \u0026lt;- tidytuesdayR::tt_load('2021-09-28')\r Well, that command above failed, claiming that date didn\u0026rsquo;t exist. So loading the data manually after all.\npapers \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')\r ## Rows: 29434 Columns: 4\r ## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (2): paper, title\r## dbl (2): year, month\r ## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r authors \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/authors.csv')\r ## Rows: 15437 Columns: 4\r ## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (4): author, name, user_nber, user_repec\r ## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r programs \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')\r ## Rows: 21 Columns: 3\r ## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (3): program, program_desc, program_category\r ## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r paper_authors \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')\r ## Rows: 67090 Columns: 2\r ## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (2): paper, author\r ## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r paper_programs \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')\r ## Rows: 53996 Columns: 2\r ## -- Column specification --------------------------------------------------------\r## Delimiter: \u0026quot;,\u0026quot;\r## chr (2): paper, program\r ## ## i Use `spec()` to retrieve the full column specification for this data.\r## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\r Understanding the data The explanations on the Tidy Tuesday website were a bit confusing. Looks like each CSV file only has a few variables, and papers.csv does not contain all the variables listed in the readme.\ncolnames(papers)\r ## [1] \u0026quot;paper\u0026quot; \u0026quot;year\u0026quot; \u0026quot;month\u0026quot; \u0026quot;title\u0026quot;\r colnames(authors)\r ## [1] \u0026quot;author\u0026quot; \u0026quot;name\u0026quot; \u0026quot;user_nber\u0026quot; \u0026quot;user_repec\u0026quot;\r colnames(programs)\r ## [1] \u0026quot;program\u0026quot; \u0026quot;program_desc\u0026quot; \u0026quot;program_category\u0026quot;\r colnames(paper_authors)\r ## [1] \u0026quot;paper\u0026quot; \u0026quot;author\u0026quot;\r colnames(paper_programs)\r ## [1] \u0026quot;paper\u0026quot; \u0026quot;program\u0026quot;\r Papers seem to be linked to authors by the paper_authors file and to programs (areas of work) by the paper_programs file. Probably best to combine all into one data frame. The Tidy Tuesday website has some code for that already, let\u0026rsquo;s see if it works\ndf \u0026lt;- left_join(papers, paper_authors) %\u0026gt;% left_join(authors) %\u0026gt;% left_join(paper_programs) %\u0026gt;% left_join(programs)%\u0026gt;% mutate(\rcatalogue_group = str_sub(paper, 1, 1),\rcatalogue_group = case_when(\rcatalogue_group == \u0026quot;h\u0026quot; ~ \u0026quot;Historical\u0026quot;,\rcatalogue_group == \u0026quot;t\u0026quot; ~ \u0026quot;Technical\u0026quot;,\rcatalogue_group == \u0026quot;w\u0026quot; ~ \u0026quot;General\u0026quot;\r),\r.after = paper\r)  ## Joining, by = \u0026quot;paper\u0026quot;\r ## Joining, by = \u0026quot;author\u0026quot;\r ## Joining, by = \u0026quot;paper\u0026quot;\r ## Joining, by = \u0026quot;program\u0026quot;\r glimpse(df)\r ## Rows: 130,081\r## Columns: 12\r## $ paper \u0026lt;chr\u0026gt; \u0026quot;w0001\u0026quot;, \u0026quot;w0002\u0026quot;, \u0026quot;w0003\u0026quot;, \u0026quot;w0004\u0026quot;, \u0026quot;w0005\u0026quot;, \u0026quot;w0006\u0026quot;,~\r## $ catalogue_group \u0026lt;chr\u0026gt; \u0026quot;General\u0026quot;, \u0026quot;General\u0026quot;, \u0026quot;General\u0026quot;, \u0026quot;General\u0026quot;, \u0026quot;General\u0026quot;~\r## $ year \u0026lt;dbl\u0026gt; 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973,~\r## $ month \u0026lt;dbl\u0026gt; 6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 1~\r## $ title \u0026lt;chr\u0026gt; \u0026quot;Education, Information, and Efficiency\u0026quot;, \u0026quot;Hospital U~\r## $ author \u0026lt;chr\u0026gt; \u0026quot;w0001.1\u0026quot;, \u0026quot;w0002.1\u0026quot;, \u0026quot;w0003.1\u0026quot;, \u0026quot;w0004.1\u0026quot;, \u0026quot;w0005.1\u0026quot;~\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Finis Welch\u0026quot;, \u0026quot;Barry R Chiswick\u0026quot;, \u0026quot;Swarnjit S Arora\u0026quot;~\r## $ user_nber \u0026lt;chr\u0026gt; \u0026quot;finis_welch\u0026quot;, \u0026quot;barry_chiswick\u0026quot;, \u0026quot;swarnjit_arora\u0026quot;, NA~\r## $ user_repec \u0026lt;chr\u0026gt; NA, \u0026quot;pch425\u0026quot;, NA, \u0026quot;pli669\u0026quot;, \u0026quot;psm28\u0026quot;, NA, NA, NA, \u0026quot;pli~\r## $ program \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r## $ program_desc \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r## $ program_category \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r That seems to have worked, now all data is in one data frame.\nExploration 1 My first idea is very idiosyncratic. A friend of mine is an Econ/Finance professor at Bocconi University. I\u0026rsquo;m going to see if he has any papers in that dataset.\nsmalldf \u0026lt;- df %\u0026gt;% filter(grepl(\u0026quot;Wagner\u0026quot;,name))\rprint(unique(smalldf$name))\r ## [1] \u0026quot;Joachim Wagner\u0026quot; \u0026quot;Alexander F Wagner\u0026quot; \u0026quot;Todd H Wagner\u0026quot; ## [4] \u0026quot;Stefan Wagner\u0026quot; \u0026quot;Mathis Wagner\u0026quot; \u0026quot;Ulrich J Wagner\u0026quot; ## [7] \u0026quot;Wolf Wagner\u0026quot; \u0026quot;Gernot Wagner\u0026quot; \u0026quot;Zachary Wagner\u0026quot; ## [10] \u0026quot;Katherine R H Wagner\u0026quot; \u0026quot;Myles Wagner\u0026quot; \u0026quot;Kathryn L Wagner\u0026quot;\r Not seeing him in there. Just to make sure, a check on first name.\nsmalldf \u0026lt;- df %\u0026gt;% filter(grepl(\u0026quot;Hannes\u0026quot;,name))\rprint(unique(smalldf$name))\r ## [1] \u0026quot;Hannes Schwandt\u0026quot;\r Ok, nothing. Might be that his area, finance, is not indexed by NBER. I don\u0026rsquo;t know enough about the econ/business/finance fields enough to know what is and isn\u0026rsquo;t part of NBER. So I guess moving on.\nExploration 2 In most areas of science and when looking at publication records, one finds that most people publish very little (e.g. a student who is a co-author on a paper, then goes into the \u0026ldquo;real world\u0026rdquo; and never publishes again) and a few people publish a lot (super-star and/or old faculty). One usually sees an 80/20 pattern or a distribution that follows a power law. Let\u0026rsquo;s see what we find here.\nFirst, I\u0026rsquo;m doing a few more checks.\n#looking at missing data for each variable\rnas \u0026lt;- colSums(is.na(df))\rprint(nas)\r ## paper catalogue_group year month ## 0 0 0 0 ## title author name user_nber ## 0 0 0 2112 ## user_repec program program_desc program_category ## 47158 530 530 1516\r Missing data pattern seems ok. To be expected that some users don\u0026rsquo;t have those NBER or REPEC IDs.\nLet\u0026rsquo;s look at number of unique papers and unique authors.\nn_authors = length(unique(df$author))\rn_papers = length(unique(df$title))\rprint(n_authors)\r ## [1] 15437\r print(n_papers)\r ## [1] 29419\r Comparing those numbers to the original data frames, we see that the number of authors is same as in original authors data frame, that\u0026rsquo;s good. Number of papers (or at least unique titles) is less. Seems like some papers have the same titles? Non-unique titles is confirmed by checking the ID for each paper, which is the same as the one in the original papers CSV file.\nLet\u0026rsquo;s look at those titles that show up more than once. Note that we need to do that with the original papers data frame, since the merged one contains many duplicates since each author gets their own row.\n#using base R here, can of course also do that with tidyverse syntax\rdfdup \u0026lt;- papers[duplicated(papers$title),]\rprint(dfdup$title)\r ## [1] \u0026quot;The Wealth of Cohorts: Retirement Saving and the Changing Assets of Older Americans\u0026quot; ## [2] \u0026quot;Empirical Patterns of Firm Growth and R\u0026amp;D Investment: A Quality Ladder Model Interpretation\u0026quot;\r## [3] \u0026quot;The Market for Catastrophe Risk: A Clinical Examination\u0026quot; ## [4] \u0026quot;Taxation and Corporate Financial Policy\u0026quot; ## [5] \u0026quot;Asset Pricing with Heterogeneous Consumers and Limited Participation: Empirical Evidence\u0026quot; ## [6] \u0026quot;Tax Incidence\u0026quot; ## [7] \u0026quot;Liquidity Shortages and Banking Crises\u0026quot; ## [8] \u0026quot;Legal Institutions and Financial Development\u0026quot; ## [9] \u0026quot;Inequality\u0026quot; ## [10] \u0026quot;Predictive Systems: Living with Imperfect Predictors\u0026quot; ## [11] \u0026quot;Corruption\u0026quot; ## [12] \u0026quot;The Dynamic Properties of Financial-Market Equilibrium with Trading Fees\u0026quot; ## [13] \u0026quot;Forward Guidance\u0026quot; ## [14] \u0026quot;Labor Market Integration Before the Civil War\u0026quot; ## [15] \u0026quot;The Impact of Globalization on Pre-Industrial, Technologically Quiescent Economies\u0026quot;\r Some titles I can easily seen being used more than once, e.g. a paper called \u0026ldquo;Corruption\u0026rdquo;. Others sound very unique, so not sure why they show up as duplicates. If this were a serious analysis, I would look more closely into that. But for this exercise, and since it\u0026rsquo;s just a few titles, I\u0026rsquo;ll ignore and move on.\nI want to look at publications per author. Since names might not be unique but NBER ID should be, I\u0026rsquo;m just going to remove those authors that don\u0026rsquo;t have an NBER ID (likely most of them have co-authored very few papers) and focus on the remaining authors. For each, I\u0026rsquo;ll count their total papers by counting how often they show up.\ndfnew \u0026lt;- df %\u0026gt;% filter(!is.na(user_nber)) %\u0026gt;% group_by(user_nber) %\u0026gt;% summarise(n_papers = n() ) %\u0026gt;%\rarrange(desc(n_papers)) %\u0026gt;%\rmutate(allpapers = cumsum(n_papers)) dfnew$id = 1:nrow(dfnew) #add an ID variable for plotting\r Looking at the histogram of number of papers for each author.\np1 \u0026lt;- dfnew %\u0026gt;% ggplot(aes(n_papers)) + geom_histogram() plot(p1)\r ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r Looks like expected, most authors have published only a few papers, a few have a lot.\nAnother way to look at this is with a violin plot\np1a \u0026lt;- dfnew %\u0026gt;% ggplot(aes(x=1, y=n_papers)) + geom_violin() plot(p1a)\r That\u0026rsquo;s a very flat violin plot, with almost all the density close to 1.\nDoing a quick numerical summary\nsummary(dfnew$n_papers)\r ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 3.000 8.983 7.000 359.000\r Of course one needs to have at least 1 paper published to be in there, so that\u0026rsquo;s the minimum. The median is 3, so half of individuals have published 3 or less. The mean is higher, as expected for skewed data, and the max is at 359 papers, confirming the histogram which shows a few individuals wrote a lot of papers.\nJust because, let\u0026rsquo;s look at the 10 most published authors\nhead(dfnew,n=10)\r ## # A tibble: 10 x 4\r## user_nber n_papers allpapers id\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 jonathan_gruber 359 359 1\r## 2 james_heckman 331 690 2\r## 3 daron_acemoglu 308 998 3\r## 4 janet_currie 306 1304 4\r## 5 michael_bordo 297 1601 5\r## 6 edward_glaeser 291 1892 6\r## 7 joshua_aizenman 284 2176 7\r## 8 martin_feldstein 272 2448 8\r## 9 andrei_shleifer 242 2690 9\r## 10 alan_taylor 239 2929 10\r Since this is not my field, I don\u0026rsquo;t know any of those individuals. But looks like the 1st one, Gruber, is somewhat famous and also worked at NBER, so maybe not surprising that his papers are in there. Not sure, I don\u0026rsquo;t know how exactly NBER works, but it\u0026rsquo;s a quick consistency check and no red flag.\nThis looks at the accumulation of papers for the first N authors, with number of authors on the x-axis and total papers on the y axis. If every author were to contribute the same number of papers, we\u0026rsquo;d see a straight line up the diagonal. The fact that some authors write more papers, and most just a few, pushes the curve to the upper left corner. I\u0026rsquo;m also plotting a few lines that show the 80/20 idea, i.e. the vertical line indicates 20% of authors, the horizontal indicates 80% of all published papers.\nnobs = nrow(dfnew)\rtotpapers = max(dfnew$allpapers)\rp2 \u0026lt;- dfnew %\u0026gt;% ggplot(aes(x=id, y=allpapers)) + geom_point() + theme(legend.position=\u0026quot;none\u0026quot;) + geom_segment(aes(x = floor(nobs*0.2), y = 1, xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \u0026quot;red\u0026quot;, linetype=\u0026quot;dashed\u0026quot;, size=1) +\rgeom_segment(aes(x = 1, y = floor(totpapers*0.8), xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \u0026quot;red\u0026quot;, linetype=\u0026quot;dashed\u0026quot;, size=1) plot(p2)\r Looks like the NBER papers are fairly close to that 80/20 distribution, with few authors contributing the bulk, and most authors contributing little.\nNote that this does not account for co-authorship, just doing a per-author count.\nFurther explorations I\u0026rsquo;m going to leave it at this for now. In contrast to my 2019 Tidy Tuesday exploration I won\u0026rsquo;t try a fake statistical analysis here.\nBut I can think of a few other ideas and things to explore Here are a few:\n  The Tidy Tuesday website had a link to this article which looks at gender representation among the papers/authors. We could do that here too, e.g. follow their approach to try and guess gender for authors, then could stratify number of papers by gender of author.\n  Another possible exploration would be to look at the numbers of papers per author based on the area of research, i.e. the programs variable.\n  Yet another analysis one could do is to look at the pattern of publication for those that publish a good bit (say over 50 papers) and see how numbers of papers per year changes, or how number of co-authors changes over the years.\n  Summary Patterns of authorship have been explored often. Sometimes they lead to useful information. At other times, one just needs more or less meaningful numbers for career advancement purposes. See e.g. an analysis of my own papers I did and wrote up in this post covering Google scholar data and this post using the bibliometrix package.\nMy exploration here was not too thorough, but some expected patterns showed up, namely the 80/20 skewed distribution in authorship.\n","date":1632960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632960000,"objectID":"eab33ad3462332b76782f0533e759d58","permalink":"https://www.andreashandel.com/posts/tidytuesday-analysis-2021/","publishdate":"2021-09-30T00:00:00Z","relpermalink":"/posts/tidytuesday-analysis-2021/","section":"posts","summary":"An exploration of some papers catalogued in NBER","tags":["R","Data Analysis","Tidy Tuesday"],"title":"Analysis of economic papers","type":"posts"},{"authors":["Andreas Handel"],"categories":["presentation","research","data science"],"content":"These are the slides for a presentation I gave at a (virtual) meetup event of the Data Science \u0026amp; Business Intelligence Society of Atlanta.\n","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"2d95599753c0576756d1b50d887ba38a","permalink":"https://www.andreashandel.com/presentations/2021-09-dsatl/","publishdate":"2021-09-20T00:00:00Z","relpermalink":"/presentations/2021-09-dsatl/","section":"presentations","summary":"A presentation given at a Data Science \u0026 Business Intelligence Society of Atlanta (virtual) meeting","tags":null,"title":"Adventures in Public Health Data Analytics - COVID-19 and beyond","type":"presentations"},{"authors":["Andreas Handel"],"categories":["workshop","teaching","modeling","immunology"],"content":"We placed all materials for this workshop online, you can find them here.\n","date":1626220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626220800,"objectID":"65a6b4cbd3b41c38fd42f695a537f7b0","permalink":"https://www.andreashandel.com/presentations/2021-07-sismid/","publishdate":"2021-07-13T00:00:00Z","relpermalink":"/presentations/2021-07-sismid/","section":"presentations","summary":"For the 13th time, and 2nd time online, my colleague Paul Thomas and I taught our annual SISMID workshop.","tags":null,"title":"Infectious Diseases, Immunology, and Within-Host Models","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","software","R package"],"content":"The pre-recorded 5 minute video presentation can be found here. And check out the package website if you want to learn more.\n","date":1625616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"a7a2b1ee50f4b004500ae38fdd43a2c9","permalink":"https://www.andreashandel.com/presentations/2021-07-user/","publishdate":"2021-07-07T00:00:00Z","relpermalink":"/presentations/2021-07-user/","section":"presentations","summary":"A pre-recorded presentation given at useR! 2021 introducing one of our new R packages","tags":null,"title":"An R package to flexibly generate simulation model flow diagrams","type":"presentations"},{"authors":null,"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://www.andreashandel.com/projects/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Past and Current Projects","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"6a637a40b03078eb86b806248fd2413a","permalink":"https://www.andreashandel.com/subscribe/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/subscribe/","section":"","summary":"Monthly newsletter subscription","tags":null,"title":"Subscribe","type":"widget_page"},{"authors":["Andreas Handel"],"categories":["presentation","research","influenza","vaccines","inoculum dose"],"content":"These are slides for a talk discussing the role of dose for the FluZone influenza vaccine.The presentation slides are here.\n","date":1619568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619568000,"objectID":"c631816cd7c287c46ceda30ef4768eb1","permalink":"https://www.andreashandel.com/presentations/2021-04-civic-meeting/","publishdate":"2021-04-26T00:00:00Z","relpermalink":"/presentations/2021-04-civic-meeting/","section":"presentations","summary":"A (virtual) presentation discussing the role of dose for influenza vaccines.","tags":null,"title":"A Comparison of High-Dose and Regular-Dose Seasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous Immunity","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","career","mentoring"],"content":"These are slides for a talk/discussion I led with our graduate students discussing mentorship, both how to be a good mentee and mentor. The presentation slides are here.\n","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"c0c3c7e2ff72da42b82209a925298e32","permalink":"https://www.andreashandel.com/presentations/2021-04-mentorship/","publishdate":"2021-03-05T00:00:00Z","relpermalink":"/presentations/2021-04-mentorship/","section":"presentations","summary":"A presentation for our departmental PhD and MS students on mentorship","tags":null,"title":"How to be a good mentee and mentor","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","career","project management"],"content":"These are slides for a talk/discussion I led with our graduate students discussing how to come up and organize projects in a good way. The talk covered both projects in general and academic projects in particular. The presentation slides are here.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"cdf41e5817c3bec6bec513bf87063a51","permalink":"https://www.andreashandel.com/presentations/2021-04-good-projects/","publishdate":"2021-03-25T00:00:00Z","relpermalink":"/presentations/2021-04-good-projects/","section":"presentations","summary":"A presentation for our departmental PhD and MS students on how to find and implement good (academic) projects","tags":null,"title":"Coming up with good (research) projects","type":"presentations"},{"authors":null,"categories":["R","website"],"content":"The following blog post provides step-by-step instructions for creating a website using R Markdown, the distill R package and GitHub.\nMotivation The distill R package is a fairly new R package from the RStudio folks, it is under heavy development. One of its purposes is to create simple but still somewhat flexible websites. If you are trying your first website, or just need a fairly simple one that is quick to set up, this is a good place to start. To see a few examples of websites created with distill, check out the cleverly named the distillery website.\nRequired skills I assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nWhat this document covers This document is meant to provide you with the minimum required instructions to get your own website up and running quickly. As such, instructions and background information are kept at a minimum. I used a recipe-like approach by giving hopefully detailed and specific enough instructions to get things to work. I\u0026rsquo;m not covering any why here or provide much further explanations. If you decide you like to use this setup for your website, you will likely want to go beyond this document and learn a bit more about the various tools involved in the process. To that end, links to further resources are provided.\nWho this is (not) for This way of making and hosting a website might be for you if:\n You are (or would like to be) an R, R Markdown and GitHub user. This is a method of creating a website using those tools which very efficiently fits into such a work flow. You want a way to host a website where all the content is fully controlled by you, and the website can be hosted without much resources (and for free). You want something that\u0026rsquo;s fairly simple and easy to set up and maintain, but still gives you some ability to configure things.  This way of making and hosting a website might not be for you if:\n Your main work flow is MS Word, Powerpoint, etc. and you are not interested in R Markdown or GitHub. You want everything accessible through a graphical interface. You need a complex setup with lots of control over layout and many advanced features.  Related content I previously posted tutorials describing two other ways of making a website using a similar set of tools.\nOne rather simple way to set up a website is using R Markdown and Github without the distill package, I described this approach in this blog post. That approach is rather similar to the one described here. The advantage of using distill is that it\u0026rsquo;s likely going to see more development and will allow more configuration down the road.\nIf you need the ability to do more customization, you can use the blogdown R package and Hugo. I wrote a 2-series blog post with instructions for creating your own website using blogdown, Hugo and Netlify. (Here are Part 1 and part 2). While I\u0026rsquo;m using Hugo/blogdown for my personal website I have been using a basic RMarkdown/Github setup for my online courses, such as this one.\nQuick tool overview The tools used here are fairly simple. GitHub is used for hosting the website and R and RStudio, together with some packages (most importantly the distill package) are used to create the site.\nPre-requisites First, you need to install R and Rstudio and set up a GitHub account. (That does not count toward the 30 minutes of getting the website up and running üòÅ.)\nInstall R and RStudio If you don\u0026rsquo;t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you install a new R version.\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it\u0026rsquo;s a recent version. If you have an older version, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach).\nGet GitHub up and running If you are new to GitHub, you need to create an account. At some point, it would also be useful to learn more about what Git/GitHub is and how to use it, but for this purpose you actually don\u0026rsquo;t need to know much. If you want to read a bit about Git/GitHub, see e.g. this document, which I wrote for one of my courses..\nInstall Gitkraken (optional but assumed) There are many ways you can interact with Git/GitHub. I mostly use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free. If you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don\u0026rsquo;t need it for our purpose.\nOnce you have your GitHub account set up and Gitkraken installed, make sure you connect Gitkraken to your Github account.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/GitHub client (e.g. the one that comes with RStudio), you can of course use that one too.\nStarting your website Ok, the 30 minute timer starts now üòÅ. With the above registration and installation bits out of the way, you can get started with your website. To do so, follow these steps:\n Open RStudio. Install the distill package by typing install.packages('distill') into the R console. Choose File -\u0026gt; New Project -\u0026gt; New Directory and find the Distill Website entry. If it\u0026rsquo;s not there, close and re-open RStudio and make sure the distill package installed ok (e.g., by calling library(distill) in the R console and making sure there is no error message). In the menu that opens, provide the name for your website project (this will also be the name of the URL on GitHub, so choose something informative). Decide where to locate it on your local drive. Don\u0026rsquo;t place it somewhere were it gets synced with e.g., Dropbox, OneDrive, Google Drive. This can mess with the GitHub work flow. Give it a title and check the configure GitHub Pages box, as shown in the figure.  Test the website Once you created the project following the steps outlined above, RStudio should restart itself and you should see 3 files open in the RStudio file pane (top left) namely index.Rmd, about.Rmd and _site.yml. In the top right pane, you should see a tab called Build. Click on it, then click on the Build Website hammer icon. Alternatively, you can type ``rmarkdown::render_site()` into the R console.\nIf you don\u0026rsquo;t find the Build Website button or the rmarkdown::render_site() command produced an error message, something went wrong with the setup. You can try to close RStudio, navigate to the folder for your website you just created and click on the .Rproj file, which should open RStudio and place you in the project. Maybe the Build tab and Build Website buttons are now there? If not, revisit the steps above and make sure you did them all, especially make sure the 3 starter files are in the same folder.\nIf things work, a preview window should open with the beginning of your new website. You\u0026rsquo;ll see a menu at the top, you can click on the links. Not much will happen just now, we\u0026rsquo;ll get to this.\nA brief explanation of your new website Your website is fairly simple and consists of these documents.\nThe file index.Rmd This is the main landing page of your website. It always needs to be there and you fill it with the content you want on your main page. It should have opened in RStudio, if not, click on it (in the lower right File section of RStudio) to open it. You will see some stuff at the top between the --- symbols, that\u0026rsquo;s called the YAML header and is used for formatting. In this case, we are using the distill format for output. This is a type of R Markdown. If you are new to R Markdown, you can learn more about it on the R Markdown website. I also have a discussion of R Markdown and reproducibility on one of my course pages. You\u0026rsquo;ll find additional links to potentially useful R Markdown (and Github) resources there. Details about the distill version of R Markdown are here.\nThe file about.Rmd This is another R Markdown file with so far little content.\nThe file _site.yml This short file (called a YAML file) contains the main settings for your site that control the overall look. It also defines the structure, i.e. the menu bar at the top of your site. We\u0026rsquo;ll edit this file shortly.\nThe docs folder This folder is created and updated when you build the website. It contains the html files that are created from your Rmd files. Those files will be used by Github to display your website (we\u0026rsquo;ll do that below). Note that you can change the folder name in _site.yml, but I recommend leaving it as is, since docs is also the default location for Github to look for these files.\nAdditional content There is an .Rproj file that is used by RStudio to store information about your project. Later, we\u0026rsquo;ll add a few more additional files.\nCreating and editing content Making new content for your website is very easy, all you need to do is edit existing or create new R Markdown (.Rmd) files. Note that if you don\u0026rsquo;t use R code in your file, you could just use plain Markdown/Md files, but I find it easier for consistency to always use Rmd files, even if they don\u0026rsquo;t contain R code.\nLet\u0026rsquo;s create some content. Open the index.Rmd file and write some introductory text at the bottom of the file, below the YAML header. Something like This is my new website, it\u0026rsquo;s going to be awesome! or whatever you like to write.\nThen open the about.Rmd file and also write some text, e.g. provide an introduction that describes you. if you want, add a picture, as e.g. described here.\nFinally, let\u0026rsquo;s create yet another file. Go to File, New File, R Markdown choose as output format HTML document and give it a title and name. Let\u0026rsquo;s call it Projects. A template will open. Replace the output: html_document line in the YAML with output: distill::distill_article\nAlso delete the existing content (or keep it at the bottom if you want to see how it looks when compiled). Then write some placeholder text, e.g. This is where I will list my cool projects. Save this new page as projects.Rmd into the main folder of your website.\nAnother option for creating new files, which I usually use, is to go into the website folder, make a copy of an existing file and rename. E.g. copy projects.Rmd to newpage.Rmd, then open and edit. Either approach works.\nNow we need to include the new page you created into the overall site by adding it to the navbar section of _site.yml. Open that file, and in the navbar section below the current About entry, add\n- text: \u0026quot;Projects\u0026quot;\rhref: projects.html\r Warning! YAML files or YAML headers are very picky about the exact formatting, and the number of leading empty spaces. Often, if you change something in those files and the site doesn\u0026rsquo;t compile, it means you forgot to add the required empty spaces or added more than you should. Always check that the spacing is correct.\nHere, make sure your new navbar entries is exactly positioned like the entries above it.\nOnce edited the _site.yml file, save all your files, then rebuild the website. If things work, the preview window should show up, and you should now see the text you wrote and be able to click on your new Projects entry. (You\u0026rsquo;ll find the newly created html files in the docs folder.)\nEditing the look As mentioned above _site.yml controls the look. You already edited it by adding something to the navigation bar.\nTo do major style changes, distill uses CSS, which is the standard of styling for any website. At some point, you\u0026rsquo;ll likely need to mess with CSS a bit (it\u0026rsquo;s easy) to get exactly the look you want. For now, we\u0026rsquo;ll take a shortcut and get and apply a theme someone else wrote.\nThe website of Emi Tanaka (who I do not know personally) is one of the sites featured on the distillery. Her website is here. As of the time of this writing, she has a theme with different fonts and colors. The theme is controlled by a CSS file for her site. You can get it by right-clicking here and doing a \u0026lsquo;save link as\u0026rsquo;. Save the file (which should be called theme.css into the main directory of your new website.\nAfter you have done that, go to the _site.yml file and add theme: theme.css as a new line under the title line. Save, rebuild your website, and you should see it looking differently.\nSince the promise was that we\u0026rsquo;ll get a website going in 30 minutes, we won\u0026rsquo;t do any more styling now üòÅ. As you start customizing your look more, you might want to start read the section Creating a Website section on the distill website.\nMaking your site public on GitHub. The final step involves getting your site onto GitHub so it can be turned into a public website. For the following, I assume you have GitKraken up and running. If you are familiar with Git/GitHub and use your own workflow, adjust accordingly.\nGitkraken  Open GitKraken, go to File -\u0026gt; Init Repo -\u0026gt; Local Only. Give it the name of your main website directory, e.g. mywebsite. The Initialize In folder should be the folder above where you created the website, such that the Full path entry is the actual location of your website on your computer. For .gitignore Template you can choose R. The rest you can leave as is.  Once done, click Create repository. You should see a bunch of files ready for staging on the left. Click Stage all changes enter a commit message, commit. Then Click the Push button.\nAt this point, if you didn\u0026rsquo;t properly connect GitKraken and GitHub previously, you\u0026rsquo;ll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou\u0026rsquo;ll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your website folder name. Then click the green button. If things worked, your local website folder has been sent to GitHub and is ready to be turned into a website.\nGithub website For the last step, go to your account on Github.com and find the repository for the website you just created. On the bar at the top, in the right corner there should be the Settings button. Click on it. Scroll down until you find the GitHub Pages section. Under Source, select Main and then choose /docs as the folder. Don\u0026rsquo;t choose a theme since we are using our own. Save those changes. If everything works (it could take a minute or so), your website is now live and public! Look right underneath the GitHub Pages section, there should be something like Your site is ready to be published at https://andreashandel.github.io/mywebsite/. Click on the link and your new site should show up.\nThat\u0026rsquo;s it. Now the hard part starts, creating good content. üòÑ\nNext steps   You\u0026rsquo;ll likely want to create content and change the look. See the sources listed below.\n  You might also want to update the Readme.md file which is the file people see when they land in your GitHub repository for this file. I suggest adding a link to the website.\n  Updating your site This process is fairly simple, you just need to remember to go through all the steps.\n Make any changes to files you want to make. Create new Rmd files in the main folder, edit them. If you include new files or rename them, don\u0026rsquo;t forget to change your _navbar.yml file. Rebuild the website by either clicking the Build Website button inside the Build tab in the top right R Studio pane, or by typing rmarkdown::render_site() into the console. Push your changes to GitHub. Wait a minute or so, then reload your website on Github and check that things look right.  More Information   Since this setup is based on R Markdown, the R Markdown book is very useful and contains lots of relevant information, especially this section. There is also the newer R Markdown cookbook which is more of a how-to guide, while the R Markdown book is more of a full reference book. Both are very useful sources of information.\n  The distill website is the obvious place for additional information on the distill package.\n  Look at the examples shown on the distillery website. If you see something you like, look at their GitHub repository and their code to see how they do it.\n  In my teaching, I use the Github/R Markdown workflow. Therefore, I have some information and links on that topic. You can browse through the website of this course and look for relevant content. The exercises teach parts of Github and making web pages (in fact, going through this tutorial is part of an exercise üòÉ.\n  Some tips, tricks and comments Here are some thoughts and suggestions that I\u0026rsquo;ve learned by using this general setup for several online courses (previously I didn\u0026rsquo;t use the distill setup, but these comments still apply).\n  Every time you build your website, everything gets recompiled. If you have simple content, and no/little R code, that\u0026rsquo;s ok. If you are having materials that take long to run (e.g. complex R code inside your website), I suggest to move the R code from the Rmd file to a separate R script and save results from the code into files (figures/tables/Rdata files). Then load those results into your Rmd file. This way you only need to run the time-consuming R code if those parts have changed, but on a standard website re-build the code won\u0026rsquo;t need to run. There is also a way to set up your distill website as a blog that doesn\u0026rsquo;t lead to everything being recompiled. See here for more.\n  Sometimes when I use this workflow to build websites on a Windows computer, things slow down markedly. It turns out that Windows Defender is at times not working right. If you encounter a very slow re-build process on a Windows machine, this could be the issue and you might want to check out this discussion and especially the provided link which explains how to potentially fix it (the fix worked for me).\n  You can have files other than Rmd/Md in your main folder, and you can have files in sub-folders. Those are ignored by rmarkdown when the site is built. Having those can be useful for storing additional materials. I generally have a media folder in which I place figures and other materials, and link to it from my Rmd files.\n  If you use this setup for teaching and want to slowly release content to students (while still making edits to later parts of the course), I recommend using a staging setup. A simple way to do that is to make 2 repositories, the main one for the course, and one where you do the development. Note that even if you set this to a private repository, if you turn on Github Pages, students could find it. That\u0026rsquo;s usually not a big deal, nobody is looking. But you might want to be aware of it. With 2 repositories, you can do the testing/development of the course in the staging repository. Once things work and whenever you want to release new content to the students, you copy it over to the main repository, build it there and push it to the main course repo. Alternatively, you can do development in the main repository, as long as you don\u0026rsquo;t make the files visible in the navbar section, students won\u0026rsquo;t easily see them. Of course the files can be found through a direct link or through browsing the Github repository, so don\u0026rsquo;t place anything in there (e.g. if you use this four a course, keep the solutions to your homework problems offline until after the deadline üòÑ).\n  It\u0026rsquo;s easy to have broken links when creating any website and it\u0026rsquo;s good to check that things are ok. There are simple free tools out there that let you check to make sure links are not broken. I like using Dr Link Check or the W3C Link Checker. I\u0026rsquo;m sure many others exist.\n  ","date":1616284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616284800,"objectID":"9e4353ffa83ebd210d78febf88556202","permalink":"https://www.andreashandel.com/posts/distill-github-website/","publishdate":"2021-03-21T00:00:00Z","relpermalink":"/posts/distill-github-website/","section":"posts","summary":"The following are step-by-step instructions for creating a website using [R Markdown](https://rmarkdown.rstudio.com/) and the [distill R package](https://rstudio.github.io/distill/) (through [R Studio](https://rstudio.com/products/rstudio/)) and [GitHub](https://github.com/).","tags":["R","markdown","github","website","distill"],"title":"Create a GitHub website with distill in less than 30 minutes","type":"posts"},{"authors":["Andreas Handel"],"categories":["presentation","research","influenza","vaccines","inoculum dose"],"content":"These are slides for a talk discussing some of my past and present work on the role of inoculum dose for infection and vaccination. The presentation slides are here.\n","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613952000,"objectID":"337d42bd9b3a2e853b9b208b5d5aca3a","permalink":"https://www.andreashandel.com/presentations/2021-02-u01/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/presentations/2021-02-u01/","section":"presentations","summary":"A (virtual) presentation of some work on understanding the impact of dose for infection and vaccination.","tags":null,"title":"The Role of Inoculum Dose Following Infection or Vaccination","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","career","CV","resume"],"content":"These are slides for a talk/discussion I led with our graduate students discussing CVs and resumes and how to create good ones. The presentation slides are here.\n","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"6f14b5db935e916e1967315b855534e7","permalink":"https://www.andreashandel.com/presentations/2021-02-cv-resume/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/presentations/2021-02-cv-resume/","section":"presentations","summary":"A presentation for our departmental PhD and MS students on how to create CVs and resumes","tags":null,"title":"Tips for writing a (hopefully good) CV or resume","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","career","teaching","online presence"],"content":"These are slides for a discussion I led with our graduate students on the topic of online brand/presence building. The presentation slides are here.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611187200,"objectID":"3bbc89f37579e7992bb8d3b2495793a6","permalink":"https://www.andreashandel.com/presentations/2021-01-building-your-brand/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/presentations/2021-01-building-your-brand/","section":"presentations","summary":"Slides for a discussion with our departmental PhD and MS students on how to build an online brand and presence","tags":null,"title":"Building and curating your brand (online presence)","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","teaching"],"content":"These are slides for a talk/discussion I led with our graduate students discussing Individual Development Plans, and specifically the AAAS myIDP plan. This is the kick-off presentation/discussion for a semester-long IDP development project done as part of the seminar class. The presentation slides are here.\n","date":1610582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610064000,"objectID":"24114a83a8b7e53abea5abeed120f296","permalink":"https://www.andreashandel.com/presentations/2021-01-idp/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/presentations/2021-01-idp/","section":"presentations","summary":"A presentation for our departmental PhD and MS students to kick off a semester-long IDP development","tags":["career"],"title":"Introduction to Individual Development Plans and the AAAS myIDP","type":"presentations"},{"authors":null,"categories":[],"content":"The following blog post provides step-by-step instructions for creating a simple (and free) website using (R)Markdown and Github.\nMotivation Previously, I wrote a 2-series blog post with instructions for creating your own website using blogdown, Hugo and Netlify. (Here are Part 1 and part 2).\nThe blogdown/Hugo setup is fairly flexible and powerful, but sometimes more complex than what is needed. At least I find this to be the case. While I\u0026rsquo;m using Hugo/blogdown for my personal website I am currently hosting my online courses using a simpler setup, the one I\u0026rsquo;ll be describing here.\nRequired skills I assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nWhat this document covers This document is meant to provide you with the minimum required instructions to get a simple own website up and running quickly. As such, instructions and background information are kept at a minimum. I used a recipe-like approach by giving hopefully detailed and specific enough instructions to get things to work. I\u0026rsquo;m not covering any why here or provide much further explanations. If you decide you like to use this setup for your website, you will likely want to go beyond this document and learn a bit more about the various tools involved in the process. To that end, links to further resources are provided.\nWho this is (not) for This way of making and hosting a website might be for you if:\n You are (or would like to be) an R, RMarkdown and GitHub user. This is a method of creating a website using those tools which very efficiently fits into such a workflow. You want a way to host a website where all the content is fully controlled by you, and the website can be hosted without much resources (and for free). You are curious about R/RMarkdown/GitHub, how to use it to build a website, and you\u0026rsquo;ve got a bit of time to spare and want to give it a try. You want something that\u0026rsquo;s fairly simple and easy to set up and maintain.  This way of making and hosting a website might not be for you if:\n Your main workflow is MS Word, Powerpoint, etc. and you are not interested in R/Markdown/GitHub. You want everything accessible through a graphical interface. You need a complex setup with lots of control over layout and many advanced features.  Motivating Examples Here are a few examples of websites written with R Markdown/Github:\n My modern applied data analysis course Jeff Leek\u0026rsquo;s advanced data science course  Quick tool overview The tools used here are fairly simple. Github is used for hosting the website and R Markdown and a few extra files contain all the content and formatting.\nPre-requisites First, you need to install R and Rstudio and set up a Github account. (That does not count toward the 30 minutes of getting the website up and running üòÅ.)\nInstall R and RStudio If you don\u0026rsquo;t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it\u0026rsquo;s a recent version. If you have an older verion of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach).\nGet Github up and running If you are new to Github, you need to create an account. At some point, it would also be useful to learn more about what Git/Github is and how to use it, but for this purpose you actually don\u0026rsquo;t need to know much. If you want to read a bit about Git/Github, see e.g. this document, which I wrote for one of my courses.. But for now, you don\u0026rsquo;t need to know much about Git/Github.\nInstall Gitkraken (optional but assumed) There are many ways you can interact with Git/Github. I mosty use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free, if you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don\u0026rsquo;t need it for our purpose.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/Github client (e.g. the one that comes with RStudio), you can of course use that one too. Make sure you connect Gitkraken to your Github account.\nStarting your website Ok, the 30 minute timer starts now üòÅ\nWith the above registration and installation bits out of the way, you can get started with your website. To do so, follow these steps:\nGithub  Go to Github, log in if needed. Somewhere (usually on the left), you should find a green button that says New, click it to create a new repository. Give it the name of the webpage you want to create (e.g. mywebsite). Provide a brief description (e.g. My first Github website.) Check the boxes add readme and add .gitignore and for the template chose R. You can keep the license box unchecked, or choose a license for your page. Once done, create the new repository.  Gitkraken I assume you\u0026rsquo;ll be using Gitkraken, but if you have another way of using Git/Github, feel free to use your own approach.\n Make sure Gitkraken is connected to your Github account. Open Gitkraken, under File choose Clone Repo go to Github.com, find the name of the repository you just created and clone it to some place on your local computer (i.e. copy it from Github to your local computer.) E.g. if your repository was called mywebsite, and you place it on your desktop under windows, you might choose C:\\Users\\yourname\\Desktop as the target.  Starter files  Download the files _navbar.yml, _site.yml and index.Rmd. You need those are 3 files to create your website and I\u0026rsquo;m providing you with simple templates. Place all 3 files into the folder of your new website (e.g. C:\\Users\\yourname\\Desktop\\mywebsite).  RStudio  Open RStudio. Choose File, New Project, Existing Directory and find the directory/folder you just created on Github and copied to your local computer (e.g. C:\\Users\\yourname\\Desktop\\mywebsite). Based on the files you copied into the folder earlier, R Studio should recognize that you are making a website and in the top right pane there should be a Build tab. Click on it, then Build Website. If that doesn\u0026rsquo;t work, you can also type rmarkdown::render_site() into the R console (bottom left pane in R Studio). If you don\u0026rsquo;t find the Build Website button or the rmarkdown::render_site() command produced an error message, something went wrong with the setup. You can try to close RStudio, navigate to the folder for your website and click on the .Rproj file, which should open RStudio and place you in the project. Maybe the Build tab and Build Website buttons are now there? If not, revisit the steps above and make sure you did them all, especially make sure the 3 starter files are in the same folder. If things work, a preview window should open with the beginning of your new website. You\u0026rsquo;ll see a menu at the top, but if you click on the links, they won\u0026rsquo;t work. We need to create those files first, which we\u0026rsquo;ll do below.  A brief explanation of your new website Your website is fairly simple and consists of these documents.\nThe file index.Rmd This is the main landing page of your website. It always needs to be there and you fill it with the content you want on your main page. It\u0026rsquo;s a regular R Markdown site. If you are new to R Markdown, you can learn more about it on the R Markdown website. I also have a discussion of R Markdown and reproducibility on one of my course pages. You\u0026rsquo;ll find additional links to potentially useful R Markdown (and Github) resources there.\nThe file _site.yml This short file contains the main settings for your site that control the styling. To change the layout, the theme setting is the most important. R Markdown supports the Bootswatch theme library. More information on themes and other configuration options for the _site.yml file can be found here. Once you start playing with your website, you\u0026rsquo;ll want to explore and probably adjust those options. You can even include your own style using CSS. This whole section of the R Markdown book is useful to read (at some point) to learn what you can do.\nFor now, just to give it a quick try, open the _site.yml file (in R Studio or some other text editor) and replace cerulean with spacelab. Save, then rebuild the website. You should see the layout has changed.\nThe file _navbar.yml This file allows you to build a menu for your website. If you open the file, you\u0026rsquo;ll see that I created a small menu. You see links to files aboutme.html, project1.html, project2.html and contact.html. Those files currently do not exist, thus if you click on those menus in the preview, you get an error message. You need to make sure that the files of your website match those listed in this menu file.\nYou can find some more information on the _navbar.yml file and what settings are available in this chapter of the R Markdown book. As you\u0026rsquo;ll read, it is possible to have only a single _site.yml file which contains the _navbar.yml content, but I prefer to keep them separate.\nIf a file is not listed in the _navbar.yml file, it is not accessible to users, unless they know the direct link to the file. This can be useful since it means you can have files in development in the Github repository, and they will only become visible once you link them. Of course since they can be found through a direct link or through browsing your Github repository, don\u0026rsquo;t place anything private/confidential in this folder (e.g. if you use this four a course, keep the solutions to your homework problems offline until after the deadline üòÑ).\nThe docs folder This folder is created and updated when you build the website. It contains the html files that are created from your Rmd files. Those files will be used by Github to display your website (we\u0026rsquo;ll do that below). Note that you can change the folder name in _site.yml, but I recommend leaving it as is, since docs is also the default location for Github to look for these files.\nAdditional content Additional files are either for Github (e.g. .gitignore and README.md) or for RStudio (e.g. the mywebsite.Rproj file). Those should be there, and you might want to edit those at some point, especially the README.md file. But since they are not directly part of the website, I\u0026rsquo;ll not further discuss them here.\nBeware! Both _site.yml and _navbar.yml are very picky about the exact formatting, and the number of leading empty spaces. Often, if you change something in those files and the site doesn\u0026rsquo;t compile, it means you forgot to add the required empty spaces or added more than you should. Always check first if the spacing is correct.\nCreating and editing content Making new content for your website is very easy, all you need to do is create new Rmd files. Note that if you don\u0026rsquo;t use R code in your file, you could just use plain Markdown/Md files, but I find it easier for consistency to always use Rmd files, even if they don\u0026rsquo;t contain R code.\nLet\u0026rsquo;s create the missing documents. Go to File, New File, R Markdown choose as output format HTML document and give it a title and name. A template will open, you can fill it and then save as say project1.Rmd into the main folder of your website.\nAnother option, which I usually use, is to go into the folder, make a copy of an existing file and rename. E.g. copy project1.Rmd to project2.Rmd, then open and edit. Either approach works.\nUse whichever approach you like to create Rmd versions of the 4 files that are listed in the _navbar.yml file, namely aboutme.Rmd, project1.Rmd, project2.Rmd and contact.Rmd. All files should be in the main folder. Edit the content in those files as much or as little as you want.\nOnce created, rebuild the website. In the preview that shows up, you should now be able to click on all menus in the preview and reach the pages you just created. (You\u0026rsquo;ll find the newly created html files in the docs folder.)\nMaking your site public. The final step involves getting your site public, which is easy.\n Once you are done editing your content (for now), go to Gitkraken (or your preferred Git client) and push your changes to Github. Go to Github.com and find the repository for this website. On the bar at the top, in the right corner there should be the Settings button. Click on it. Scroll down until you find the GitHub Pages section. Under Source, select Main and then choose /docs as the folder. Don\u0026rsquo;t choose a theme since we are using our own. Save those changes. Now if you look right underneath the GitHub Pages section, there should be something like Your site is ready to be published at https://andreashandel.github.io/mywebsite/. Click on the link. If everything works, your website is now live and public!  **That\u0026rsquo;s basically it. Now the hard part starts, creating good content. üòÑ **\nUpdating your site This process is fairly simple, you just need to remember to go through all the steps.\n Make any changes to files you want to make. Create new Rmd files in the main folder, edit them. If you include new files or rename them, don\u0026rsquo;t forget to change your _navbar.yml file. Rebuild the website by either clicking the Build Website button inside the Build tab in the top right R Studio pane, or by typing rmarkdown::render_site() into the console. Push your changes to GitHub. Wait a minute or so, then reload your website on Github and check that things look right.  Some tips, tricks and comments Here are some thoughts and suggestions that I\u0026rsquo;ve learned by using this setup for several online courses.\n  The setup described here works well for fairly simple sites. If you want your content structured in a more complex way and more features, you probably want to use the blogdown/hugo setup. Similarly, while you can customize the layout by trying different themes and by including your own CSS (see links above), if you want to start making it look exactly how you want, you\u0026rsquo;ll likely be wasting a lot of time and might be better off using a different setup (e.g. blogdown/hugo).\n  Every time you build your website, everything gets recompiled. If you have simple content, and no/little R code, that\u0026rsquo;s ok. If you are having things that take long to run (e.g. complex R code inside your website), I suggest to move the R code to a separate R script and save results from the code into files (figures/tables/Rdata files). You then just load thpse results into your Rmd file. This way you only need to run the time-consuming R code if those parts have changed, but on a standard website re-build the code won\u0026rsquo;t need to run.\n  Sometimes when I build websites like this one on a Windows computer, things slow down markedly. It turns out that Windows Defender is at times not working right and I have to make adjustments to prevent it from trying to scan all newly created files in real time while re-building my website. If you encounter a very slow re-build process on a Windows machine, this could be the issue and you might want to check out this discussion and especially the provided link which explains how to potentially fix it (the fix worked for me).\n  You can have files other than Rmd/Md in your main folder, and you can have files in sub-folders. Those are ignored by rmarkdown when the site is built. Having those can be useful for storing additional materials. I generally have a media folder in which I place figures and other materials, and link to it from my Rmd files.\n  If you use this setup for teaching and want to slowly release content to students (while still making edits to later parts of the course), I recommend using a staging setup. A simple way to do that is to make 2 repositories, the main one for the course, and one where you do the development. Note that even if you set this to a private repository, if you turn on Github Pages, students could find it. That\u0026rsquo;s usually not a big deal, nobody is looking. But you might want to be aware of it. With 2 repositories, you can do the testing/development of the course in the staging repository. Once things work and whenever you want to release new content to the students, you copy it over to the main repository, build it there and push it to the main course repo. Alternatively, you can do development in the main repository, as long as you don\u0026rsquo;t make the files visible in the _navbar.yml file, students won\u0026rsquo;t easily see them.\n  I like to be able to specify the date when a page was last changed. The problem is that all pages are always rebuild, so asking for \u0026lsquo;current date\u0026rsquo; during rebuilding doesn\u0026rsquo;t work. What I found out to work is to have this line in the YAML file header: file.mtime(knitr::current_input()). It gets the \u0026lsquo;last modified\u0026rsquo; time stamp of the file that is being processed and displays it. It\u0026rsquo;s not perfect, e.g. if you recently cloned the files from Github to a new computer, it will show that date as the modified date. But it\u0026rsquo;s good enough for me. If you want exact last edited date time stamps, you\u0026rsquo;ll probably have to do it manually.\n  It\u0026rsquo;s easy to have broken links when creating any website and it\u0026rsquo;s good to check that things are ok. There are simple free tools out there that let you check to make sure links are not broken. I like using Dr Link Check or the W3C Link Checker. I\u0026rsquo;m sure many others exist.\n  More Information The whole area of use R Markdown to make websites is still under rapid development. Here are a few sources that - as of this writing - might be useful and not too outdated.\n  Since this setup is fully based on R Markdown, the R Markdown book is very useful and contains lots of relevant information, especially this section. There is also the newer R Markdown cookbook which is more of a how-to guide, while the R Markdown book is more of a full reference book. Both are very useful sources of information.\n  The R Studio folks have a package called distill which is still under heavy development and which allows turning R Markdown into lots of outputs, including basic websites. This is still a work in progress, and while I tried distill in the past (and it couldn\u0026rsquo;t quite do what I want), I haven\u0026rsquo;t tried it lately. I find the setup I describe here works for my teaching needs, but if you need more/different features, I recommend checking out distill. There is aparently also a new package called postcards that works with distill to produce simple websites, very similar to the approach shown here. See e.g. this blog post by Allison Hill for an introduction. I expect these packages to develop and mature quickly.\n  In my teaching, I use the Github/R Markdown workflow, so students are exposed to it. Therefore, I have a good bit of information and links on that topic. You can browse through the website of this course and look for relevant content. A lot of the exercises also teach parts of Github and making webpages (even simpler ones than described here).\n  ","date":1610323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610323200,"objectID":"73683e7b3a6a2513d7490617f6eafc41","permalink":"https://www.andreashandel.com/posts/github-website/","publishdate":"2021-01-11T00:00:00Z","relpermalink":"/posts/github-website/","section":"posts","summary":"The following are step-by-step instructions for creating a fairly basic but still useful website using [R Markdown](https://rmarkdown.rstudio.com/) (through [R Studio](https://rstudio.com/products/rstudio/)) and [Github](https://github.com/).","tags":["R","markdown","github","website"],"title":"Create a simple Markdown/Github website in less than 30 minutes","type":"posts"},{"authors":null,"categories":["Course"],"content":"Infectious Disease Epidemiology - A model-based approach (IDEMA) is an online course I teach at the University of Georgia. All course materials (or links to the materials) are available in the form of a simple GitHub website and can be used by anyone. You can find the course on this site.\nThe course also makes heavy use of my IDEMA online book and the DSAIDE R package.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"263be81d60d5e55cbfc79235f3469c94","permalink":"https://www.andreashandel.com/my-projects/idemacourse/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/my-projects/idemacourse/","section":"my-projects","summary":"Complete materials of an online course on infectious disease epidemiology from a model-based perspective.","tags":["Modeling","Epidemiology","Infectious Disease","Teaching","Course"],"title":"Infectious Disease Epidemiology - A model-based approach","type":"my-projects"},{"authors":["Andreas Handel"],"categories":["presentation","research","influenza","vaccines"],"content":"These are slides for a talk discussing a recent project that investigated the impact of standard-dose versus high-dose influenza vaccine on immune responses. I gave the talk during the ISV Virtual Congress North America Influenza Vaccine Symposium. The presentation slides are here.\nThe organizers also recorded the event, the video recording of my presentation can be found here.\n","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"1cba9b6d1303909439382039508bf9f0","permalink":"https://www.andreashandel.com/presentations/2020-12-isv/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/presentations/2020-12-isv/","section":"presentations","summary":"A (virtual) presentation of some work on understanding the impact of dose for influenza vaccines.","tags":null,"title":"The Role of Influenza Vaccine Dose Towards Homologous and Heterologous Protection","type":"presentations"},{"authors":null,"categories":["Book"],"content":"An online book (perpetually under construction) convering infectious disease epidemiology using a model-based approach.\nSome parts of the book are fairly readable and complete enough that I use it when I teach a course on that topic. Other sections are currently only templates/outlines. While I try to ensure that what I write is correct, the whole book is not thoroughly fact-checked, error-corrected, properly referenced, etc. While I have been considering the idea of turning this into a full/real book, in my current thinking the cost-benefit analysis doesn\u0026rsquo;t pan out. I thus plan to leave it online for free as is, with occasional updates and fixes, but without an attempt to make it polished and complete enough for a printed book.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"ad6bafa416715888d5db7730d5ef90b6","permalink":"https://www.andreashandel.com/my-projects/idemabook/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/my-projects/idemabook/","section":"my-projects","summary":"An online book (perpetually under construction) covering infectious disease epidemiology from a model-based perspective.","tags":["Infectious Disease","Epidemiology","Modeling","Teaching","Book"],"title":"Infectious Disease Epidemiology - a Model-based Approach","type":"my-projects"},{"authors":["Andreas Handel"],"categories":["presentation","teaching","modeling","software"],"content":"These are slides for a talk I gave to a group of undergraduate students as part of a First Year Odyssey Seminar class. I provide a very brief introduction to infectious disease modeling and show the DSAIDE software and a worked example. The presentation slides are here.\n","date":1606089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606089600,"objectID":"ca5e3827e99beb92ce280562f3420398","permalink":"https://www.andreashandel.com/presentations/2020-11-fyos/","publishdate":"2020-11-11T00:00:00Z","relpermalink":"/presentations/2020-11-fyos/","section":"presentations","summary":"An introduction ot infectious disease modeling and software to learn it.","tags":null,"title":"User-friendly software for simulation modeling of infectious diseases","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","research","modeling","COVID-19"],"content":"These are slides for a talk discussing some recent projects on modeling COVID-19 spread and control. I gave a pre-recorded online talk as part of the International Symposium on the Prevention and Control of Infectious Diseases during the Pandemic of COVID-19 - The 4th Pudong Health Forum and the Yangtze River Delta Regional Public Health Symposium. The presentation slides are here.\n","date":1605657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605657600,"objectID":"3d448573e5c99102f47fbeec2dbbe357","permalink":"https://www.andreashandel.com/presentations/2020-11-pudong/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/presentations/2020-11-pudong/","section":"presentations","summary":"A (virtual) presentation of some recent COVID-19 modeling work.","tags":null,"title":"Simulation modeling to inform COVID-19 control and vaccination strategies","type":"presentations"},{"authors":null,"categories":["software","modeling"],"content":"We developed an R package that teaches a modern, model-based approach to infectious disease epidemiology, without the need to write computer code. Learn more about it on the package website.\n","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"902ac4ef3fc938d9e1ffdadf23901852","permalink":"https://www.andreashandel.com/my-projects/dsaide/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/my-projects/dsaide/","section":"my-projects","summary":"R package that teaches model-based infectious disease epidemiology in a user-friendly way.","tags":["Infectious Disease Models","R Package","Teaching Resource","software"],"title":"DSAIDE - Dynamical Systems Approach to Infectious Disease Epidemiology","type":"my-projects"},{"authors":["Andreas Handel"],"categories":["presentation","research","modeling","COVID-19","vaccines"],"content":"These are slides for a brief presentation on COVID-19 vaccines and how to model their deployment. The presentation slides are here.\n","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603324800,"objectID":"96f165867a9d57e56cebd72624638e6b","permalink":"https://www.andreashandel.com/presentations/2020-10-uga-pha/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/presentations/2020-10-uga-pha/","section":"presentations","summary":"A (virtual) presentation and discussion covering COVID-19 vaccines and modeling.","tags":null,"title":"An overview of COVID-19 vaccines and modeling","type":"presentations"},{"authors":null,"categories":[],"content":"Overview Generating personalized documents is often useful. Since this is a very common task, programs like Word or similar software can do this. But I like to use R if I can. And the whole R Markdown system is perfectly suited for repeat generation of customized documents.\nI\u0026rsquo;m certainly not the first one to have the idea of using R, and in fact my initial approach is based on this prior post. Here, I describe a few ways of using R and R Markdown to auto-generate custom documents, and provide example code and explanations for anyone who might want to use this (including my future self).\nRationale As a teacher, I occasionally need to generate personalized certificates for students to indicate they successfully completed a workshop, class, or similar event. Another use case is to generate personalized letters of acceptance (or rejection) to some program. Many other use cases exist. In fact, my first and original application was to automatically generate gift certificates for all my nieces and nephews üòÅ.\nFor this example, I assume that we are trying to generate certificates of completion for a group of students, with each certificate showing their name and score.\nOverall setup You need the following elements:\n A data frame containing the information you want to be personalized, e.g. the name and score of each student. A Rmd document that serves as the template for the certificate. I show two examples, for use with either LaTeX/PDF or Word as output. A short script that reads the data frame, reads the template, then personalizes the template and creates output (done via R Markdown/knitr/pandoc).  We\u0026rsquo;ll discuss each element next.\nData frame This part is fairly straightforward. You need a file (e.g. and Excel or CSV file) that you can load as a data frame and that contains the information you want to use for each personalized document. Here is an example of a very simple data frame containing names and scores for 2 students, which is stored in this csv file.\n   LastName FirstName Score     Smith Jennifer 93   Jones Paul 82    How you collected that data is up to you. If the file containing your data is a bit more messy, you might have to do a few cleaning steps before you can use it.\nTemplates There are different ways you can create the templates and thus the output, with different advantages and disadvantages. I\u0026rsquo;ll describe and show examples for LaTeX/PDF and Word, and discuss other options briefly.\nLaTeX/PDF For this approach, you start with a template file that contains some LaTeX commands, including the placeholders that will get personalized. Here is code for an example file, you can get the file here. We\u0026rsquo;ll look at the resulting output below.\n---\rtitle: \u0026quot;\u0026quot;\routput: pdf_document\rclassoption: landscape\r---\r\\begin{center}\r\\includegraphics[height=4cm]{fig1.png}\r\\hfill\r\\includegraphics[height=4cm]{fig2.jpg} \\\\\r\\bigskip\r{\\Huge\\bf Certificate of Accomplishment } \\\\\r\\bigskip\r{\\Huge \u0026lt;\u0026lt;FIRSTNAME\u0026gt;\u0026gt; \u0026lt;\u0026lt;LASTNAME\u0026gt;\u0026gt; } \\\\\r\\bigskip\r{\\Large has successfully completed the course {\\it Generating Certificates with R} with a score of \u0026lt;\u0026lt;SCORE\u0026gt;\u0026gt; } \\\\\r\\bigskip\r{\\Huge Congratulations!}\r\\end{center}\r This template places 2 images at the top, writes some text, and most importantly, adds some placeholder text that will be customized for each student with the script shown below. It doesn\u0026rsquo;t matter what placeholder text you write, as long as it\u0026rsquo;s unique such that when you do the replacement, only the instance you want replaced is actually changed. Enclosing with special characters such as \u0026lt;\u0026lt; \u0026gt;\u0026gt; is a good option for this, but it\u0026rsquo;s not required.\nThe advantages of the LaTeX/PDF approach are that 1) LaTeX allows you to do a lot of formatting and customization of the template so it looks exactly the way you want it, 2) the end product is a PDF file, which is easy to print or share with those for whom they are meant. The disadvantage is that you need to know a bit of LaTeX to set up your template, or at least be willing to spend some time with Google until you found all the snippets of commands you need for the layout you want to have.\nWord For this approach, you start with a template file that contains commands that lead to a decent looking Word document. Again, it needs to include the placeholders that will get personalized. Here is code for an example file, you can get the file here. We\u0026rsquo;ll look at the end result below.\n---\rtitle: \u0026quot;\u0026quot;\routput: word_document:\rreference_docx: wordstyletemplate.docx\r---\r![Image](fig1.png)\r# Certificate of Accomplishment\r:::{custom-style=\u0026quot;mystyle1\u0026quot;}\r\u0026lt;\u0026lt;FIRSTNAME\u0026gt;\u0026gt; \u0026lt;\u0026lt;LASTNAME\u0026gt;\u0026gt;\r:::\rhas successfully completed the course \u0026quot;Generating Certificates with R\u0026quot; with a score of \u0026lt;\u0026lt;SCORE\u0026gt;\u0026gt;\r:::{custom-style=\u0026quot;mystyle2\u0026quot;}\rCongratulations!\r:::\r```{r fig2, echo=FALSE, out.width=\u0026quot;50%\u0026quot;}\rknitr::include_graphics(\u0026quot;fig2.jpg\u0026quot;)\r```\r Note that by default, going from R Markdown to Word doesn\u0026rsquo;t give you much ability to apply formatting. However, it is possible to do a decent amount of formatting using a word style template. I have another blog post which describes this approach, and I\u0026rsquo;m using it here.\nEven with the word style formatting, some things can\u0026rsquo;t be controlled well. Placement and sizing of figures is the main problem, no matter if you include the figures with basic Markdown commands or use the include_graphics() function. You\u0026rsquo;ll see the problem if you try to run this example (code below). As such, for something that includes figures, using the LaTeX/PDF workflow seems almost always better. A scenario where the Word setup might be useful is if you want to produce customized letters. The one main advantage of a Word output (in addition to not having to figuring out LaTeX commands) is that the output can be further edited if needed.\nOther options I believe the PDF or Word outputs are best for most instances, but occasionally another format might be needed. You can use this overall approach to generate other outputs, for instance the standard R Markdown html output, or different versions of presentation slides (e.g. ioslides, Beamer, Powerpoint), etc. In principle, any R Markdown output format should work. You just need to alter your template file accordingly.\nProcessing script Once you generated your data and template, you only need a few lines of code to read the template, personalize it, and turn it into the wanted output. Here is example code that shows how to process the data and template files above, you can get the file here.\n#####################################################\r# automatically create customized certificates\r#####################################################\r# load needed packages\rlibrary('readr')\rlibrary('dplyr')\rlibrary('stringr')\r# load data, read everything in as a string/character\rdf \u0026lt;- read.csv(\u0026quot;student_data.csv\u0026quot;,colClasses = 'character')\r#set this to TRUE if you want to generate word output, FALSE for pdf\rword_out = TRUE\r# load either pdf or word certificate template\rtemplate \u0026lt;- ifelse(word_out, readr::read_file(\u0026quot;certificate_template_word.Rmd\u0026quot;), readr::read_file(\u0026quot;certificate_template_pdf.Rmd\u0026quot;))\r#run through all students, generate personalized certificate for each\rfor (i in 1:nrow(df))\r{\r#replace the placeholder words in the template with the student information\rcurrent_cert \u0026lt;- template %\u0026gt;%\rstr_replace(\u0026quot;\u0026lt;\u0026lt;FIRSTNAME\u0026gt;\u0026gt;\u0026quot;, df[i,'FirstName']) %\u0026gt;%\rstr_replace(\u0026quot;\u0026lt;\u0026lt;LASTNAME\u0026gt;\u0026gt;\u0026quot;, df[i,'LastName']) %\u0026gt;%\rstr_replace(\u0026quot;\u0026lt;\u0026lt;SCORE\u0026gt;\u0026gt;\u0026quot;, df[i,'Score'])\r#generate an output file name based on student name\rout_filename = paste(df[i,'LastName'],df[i,'FirstName'],'Certificate',sep=\u0026quot;_\u0026quot;)\rout_filename = paste0(out_filename, ifelse(word_out, '.docx','.pdf'))\r#save customized Rmd to a temporary file\rwrite_file(current_cert, \u0026quot;tmp.Rmd\u0026quot;)\r#create the certificates using R markdown.\r#it will detect the ending of the output file and use the right format\rrmarkdown::render(\u0026quot;tmp.Rmd\u0026quot;, output_file = out_filename)\r#temporary Rmd file can be deleted.\rfile.remove(\u0026quot;tmp.Rmd\u0026quot;)\r}\r Results If you run the script for the PDF output, you\u0026rsquo;ll get two pdf certificates, here is one of them. Similarly, if you run the Word template, you get two Word documents, here is one of them. As mentioned above, the Word output is not that nicely formatted, the figure placement and sizing can\u0026rsquo;t really be controlled.\nFurther Resources To reproduce the example here, you can download this zip file which contains the data file, the R Markdown template for PDF output and the R Markdown template for Word output, the certificate generator script, the files for figure 1 and figure 2 as well as the word style file. Place all files into a folder and run the certificate_generator.R function to produce either PDF or Word output.\n","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"86e4c7a26d3bc84be6e928b84a4a4a1d","permalink":"https://www.andreashandel.com/posts/certificate-generation/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/posts/certificate-generation/","section":"posts","summary":"Overview Generating personalized documents is often useful. Since this is a very common task, programs like Word or similar software can do this. But I like to use R if I can.","tags":[],"title":"Automatically generate personalized certificates and other documents with R","type":"posts"},{"authors":["Andreas Handel"],"categories":["presentation","research","modeling","COVID-19"],"content":"These are slides for a talk discussing some COVID-19 projects. I gave the talk (online) at Virginia Tech. The presentation slides are here.\n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"d74c6cd2029d3a74d9b9d45929a903a4","permalink":"https://www.andreashandel.com/presentations/2020-10-vt/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/presentations/2020-10-vt/","section":"presentations","summary":"A (virtual) presentation at Virginia Tech discussing some recent projects related to COVID-19 modeling and analysis.","tags":null,"title":"COVID-19: Modeling, Visualization and Data Analysis","type":"presentations"},{"authors":null,"categories":[],"content":"Background A good while back (around 2 years as of this writing), I needed the feature to turn an R Markdown document into a Word document (that\u0026rsquo;s easy) and to apply custom styles to specific parts of the Word document (that was trickier). I found some good information in this RStudio article, but it didn\u0026rsquo;t quite address everything I was looking for.\nSpecifically, I wanted to find a way to format certain parts of the R Markdown document in a specific style. I asked online and got some help from JJ Allaire (yes, the guy who started RStudio ü§©). Then recently, Dean Attali (yes, the guy who does a lot of cool Shiny stuff ‚ú®) posted a reply asking for an example. I tried to dig out the 2 year old project (it turns out I ended up not needing it for that project and haven\u0026rsquo;t used it since).\nAs I was trying to contemplate how to best share the example with Dean, I figured I\u0026rsquo;ll write a brief blog post, which might benefit others, too. So here it goes.\nWord template preparation  Create a new word document (either through RMarkdown -\u0026gt; Word, or just open Word and create a new empty one). Open the word document. Write a bit of text. It doesn\u0026rsquo;t matter what you write, it\u0026rsquo;s just meant so you can create and apply new styles to it. For instance you can write Text for mystyle1. Mark the text you wrote, click on the arrow to the left of the Styles box (see the red \u0026ldquo;circle\u0026rdquo; in the figure) and choose Create a style. Depending on your version of Word, this might be somewhere else. Create the formatting the way you want. Repeat to create as many custom styles as you want, save the word document into the folder of your RMarkdown file.  RMarkdown setup Start a new Rmarkdown document. Your YAML header should look something like this:\n---\rtitle: \u0026quot;An example of formatting text blocks in Word\u0026quot;\rauthor: \u0026quot;Andreas Handel\u0026quot;\rdocumentclass: article\rsite: bookdown::bookdown_site\routput:\rbookdown::word_document2: toc: false\rreference_docx: wordstyletemplate.docx\r---\r Note that I\u0026rsquo;m using bookdown as the output format here, but any others that can produce word output, e.g. the standard R Markdown format, should work equally well. The important part is the last line, which specifies the word document with the custom styles you created in the previous step.\nRMarkdown input content You can now assign text blocks in your R markdown file specific styles. Here I created 3 styles called mystyle1/mystyle2/mystyle3 in the Word doc, and assign them to specific parts of the text. This example markdown text shows how to use the styles.\n# A regular section\rThis text is not getting a special format.\r# A formatted Section\r:::{custom-style=\u0026quot;mystyle1\u0026quot;}\rThis is formatted according to the _mystyle1_ format.\r:::\r# Another formatted block of text\rSome more regular text.\r:::{custom-style=\u0026quot;mystyle2\u0026quot;}\rNow text formatted based on _mystyle2_.\r:::\rMore regular text.\r:::{custom-style=\u0026quot;mystyle3\u0026quot;}\rThis format includes a border and it also works with an equation.\r$$Y = bX + c$$\r:::\rRegular text again.\r::: {custom-style=\u0026quot;mystyle1\u0026quot;}\r# With a header\rNote that the header formatting is overwritten.\r:::\r Word output The resulting word document looks like this:\nSome notes One thing you see in this example is that your own styles overwrite all others, so headers inside your custom style will just be formatted like your custom style. Some other quirks I noticed is that you seem to need empty lines before and after your custom style block. I seem to remember that formatting of R chunks works ok, but I also seem to recall that sometimes manual intervention is required.\nOverall, this approach gives you a good deal of flexibility for applying styling to your Word documents when writing in R Markdown, but there might still be some kinks. As I mentioned in the beginning, I ended up not using it for the project I had intended to use it (a review paper I wrote), so I don\u0026rsquo;t have a lot of real world experience beyond what I\u0026rsquo;m describing here.\nFurther Resources You can get the Word template and the example R Markdown file I used to illustrate this here and here.\nI recently saw that the new R Markdown cookbook has a section describing word styling. I expect that more content will be added to the cookbook, so it\u0026rsquo;s worth checking regularly.\n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"dab175c10b38118fa474aacdbf20b5f6","permalink":"https://www.andreashandel.com/posts/word-formatting-rmarkdown/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/posts/word-formatting-rmarkdown/","section":"posts","summary":"Background A good while back (around 2 years as of this writing), I needed the feature to turn an R Markdown document into a Word document (that\u0026rsquo;s easy) and to apply custom styles to specific parts of the Word document (that was trickier).","tags":["R","RMarkdown","Word"],"title":"Custom Word formatting using R Markdown","type":"posts"},{"authors":null,"categories":["software"],"content":"We developed an R package that provides immunologists and other bench scientists a user-friendly introduction to simulation modeling of within-host infection, without the need to write computer code. Learn more about it on the package website.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"f8b9aff40a7c8fb28d289ec5d12225c7","permalink":"https://www.andreashandel.com/my-projects/dsairm/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/my-projects/dsairm/","section":"my-projects","summary":"R package that teaches modeling for within-host infection and immunology.","tags":["Within-host Models","R Package","Teaching Resource","software"],"title":"DSAIRM - Dynamical Systems Approach to Immune Response Modeling","type":"my-projects"},{"authors":null,"categories":["Course"],"content":"Simulation Modeling in Immunology (SMI) is an annual workshop that I co-teach with my colleague Paul Thomas. It usually occurs each year in July in Seattle. In 2020 we taught it online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site.\nThe course makes heavy use of my DSAIRM R package.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"2629e084aac7d057579e7bca21754af4","permalink":"https://www.andreashandel.com/my-projects/smicourse/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/my-projects/smicourse/","section":"my-projects","summary":"Materials of an online course on simulation modeling in immunology.","tags":["Modeling","Immunology","Teaching","Course"],"title":"Simulation Modeling in Immunology","type":"my-projects"},{"authors":["Andreas Handel"],"categories":["presentation","research","modeling","COVID-19"],"content":"These are slides for a talk discussing some COVID-19 modeling projects. I gave the talk (online) at UBC. The presentation slides are here.\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"4d1e146c6f9e3f744f3ad4747694038a","permalink":"https://www.andreashandel.com/presentations/2020-09-ubc/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/presentations/2020-09-ubc/","section":"presentations","summary":"A (virtual) presentation at the University of British Columbia discussing some recent COVID-19 modeling projects.","tags":null,"title":"Modeling COVID-19","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","research","COVID-19"],"content":"Short talk for an online webinar hosted by MJH life sciences on the topic Battling Dual Threats: Flu and COVID-19 Converge. My talk was titled Population-level patterns of COVID-19 and Flu: What should we expect? The presentation slides are here.\n","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"b1a791667d56fc42a6dbd0530da4ec4d","permalink":"https://www.andreashandel.com/presentations/2020-09-mjh/","publishdate":"2020-09-12T00:00:00Z","relpermalink":"/presentations/2020-09-mjh/","section":"presentations","summary":"A short talk given as part of an webinar on Battling Dual Threats: Flu and COVID-19 Converge hosted by MJH life sciences.","tags":null,"title":"Population-level patterns of COVID-19 and Flu: What should we expect?","type":"presentations"},{"authors":null,"categories":["Course"],"content":"Modern Applied Data Analysis (MADA) is a course I regularly teach online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"3d4cafeb9616f71f038a6ee151354483","permalink":"https://www.andreashandel.com/my-projects/madacourse/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/my-projects/madacourse/","section":"my-projects","summary":"Complete materials of an online course on Modern Applied Data Analysis I teach.","tags":["Data Analysis","Teaching","Course"],"title":"Modern Applied Data Analysis","type":"my-projects"},{"authors":["Andreas Handel"],"categories":["presentation","research","COVID-19"],"content":"These are slides for a talk discussing some recent COVID-19 projects. I gave the talk (online) as part of UGA\u0026rsquo;s Global Health Institute seminar series. The presentation slides are here.\n","date":1598486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598486400,"objectID":"26905c7c98cc9a254f915839d2e4b51d","permalink":"https://www.andreashandel.com/presentations/2020-08-uga-ghi/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/presentations/2020-08-uga-ghi/","section":"presentations","summary":"A (virtual) presentation at UGA's Global Health Institute discussing some recent COVID-19 work.","tags":null,"title":"Studying COVID-19 Spread and Control","type":"presentations"},{"authors":["Andreas Handel"],"categories":["workshop","teaching","modeling","immunology"],"content":"We placed all materials for this workshop online, you can find them here.\n","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"08ec05099d684da4409fb0fd7979deb7","permalink":"https://www.andreashandel.com/presentations/2020-07-sismid/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/presentations/2020-07-sismid/","section":"presentations","summary":"For the 12th time, and the first time online, my colleague Paul Thomas and I taught our annual SISMID workshop.","tags":null,"title":"Infectious Diseases, Immunology, and Within-Host Models","type":"presentations"},{"authors":null,"categories":["R","Grant Writing"],"content":"Overview While most of my grant proposals go to the NIH, which thankfully does not require an arcane conflict of interest (COI) document, I am sometimes part of a grant proposal that goes to the NSF or another agency that require a COI. Instead of just asking for any actual conflicts of interest, these documents ask one to list every co-author in the last N years, which is fairly stupid these days when most papers in the biomedical sciences have many co-authors. I hope the agencies get rid of this in my opinion pointless document soon. Until then, I have to do it.\nI don‚Äôt want to retrieve all my co-authors and fill the form by hand. In a previous post, I showed how one can use the bibliometrix R package to do an analysis of a set of publications. Among other things, this approach returns all co-authors, which I will use here to make the COI table almost completely automated.\nThe RMarkdown file to run this analysis is here.\nRequired packages library(dplyr)\rlibrary(knitr)\rlibrary(flextable)\r#remotes::install_github('massimoaria/bibliometrix')\rlibrary(bibliometrix)\r Loading data As explained in a previous post, the currently best way to get all my papers is to download them from NIH‚Äôs ‚ÄúMy Bibliography‚Äù and export it in MEDLINE format. Then read in the file with the code below.\n#read bib file, turn file of references into data frame\rpubs \u0026lt;- bibliometrix::convert2df(\u0026quot;medline.txt\u0026quot;, dbsource=\u0026quot;pubmed\u0026quot;,format=\u0026quot;pubmed\u0026quot;)  ## ## Converting your pubmed collection into a bibliographic dataframe\r## ## Done!\r## ## ## Generating affiliation field tag AU_UN from C1: Done!\r Each row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix website.\nGetting the right time period This specific funding agency I‚Äôm currently writing a COI for (NIFA) requires co-authors of the last 3 years, so let‚Äôs get them. I don‚Äôt know if they mean 3 full years. I‚Äôm doing this mid-2020, so to be on safe side, I go back to 2017.\nperiod_start = 2017\rpubs_new = pubs[pubs$PY\u0026gt;=period_start,]\r I need the full names of the authors. They are stored for each publication in the AF field. This is the only information I need for the COI form. I pull it out, then do a bit of processing to get it in the right shape, then remove duplicates and sort.\nallauthors = paste0(pubs_new$AF,collapse = \u0026quot;;\u0026quot;) #merge all authors into one vector\rallauthors2 = unlist(strsplit(allauthors, split =\u0026quot;;\u0026quot;))\rauthors = sort(unique(allauthors2)) #split vector of authors, get unique authors\r Note that I originally did the above steps using biblioAnalysis(pubs_new). However, this function/approach broke in a recent version of the package, and I realized that I can just use a few base R commands to get what I need, which is the approach shown above. If you use the biblioAnalysis() function, the Authors are in the Authors field of the returned object.\nGetting a table of co-authors Here is the full table of my co-authors in the specified time period. I made a tibble that looks similar to what the COI document requires.\n#removing the 1st one since that's me\rauthortable = dplyr::tibble(Name = authors, \u0026quot;Co-Author\u0026quot; = 'x', Collaborator = '', 'Advisees/Advisors' = '', 'Other ‚Äì Specify Nature' = '')\r Finally, I‚Äôm using the flextable package to make a decent looking table and save it to a word document.\nft \u0026lt;- flextable::flextable(authortable)\rflextable::autofit(ft)\r  .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-7f8ff422{border-collapse:collapse;}.cl-7f839614{font-family:\u0026lsquo;Arial\u0026rsquo;;font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7f839615{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7f8491b8{width:169pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491b9{width:110.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491ba{width:80.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491bb{width:70.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491bc{width:132.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491bd{width:110.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491be{width:132.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491bf{width:80.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491c0{width:169pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491c1{width:70.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8491c2{width:110.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dc6{width:169pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dc7{width:80.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dc8{width:70.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dc9{width:132.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dca{width:110.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dcb{width:132.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dcc{width:80.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dcd{width:70.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dce{width:169pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dcf{width:70.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f852dd0{width:80.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8575ce{width:110.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8575cf{width:132.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7f8575d0{width:169pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}Name\nCo-Author\nCollaborator\nAdvisees/Advisors\nOther ‚Äì Specify Nature\nAHMED, HASAN\nx\n\n\n\nALIKHAN, MALIHA A\nx\n\n\n\nAMANNA, IAN J\nx\n\n\n\nANTIA, ALICE\nx\n\n\n\nANTIA, RUSTOM\nx\n\n\n\nBOOM, W HENRY\nx\n\n\n\nBULUSHEVA, IRINA\nx\n\n\n\nCARLSON, NICHOLE E\nx\n\n\n\nCASTELLANOS, M E\nx\n\n\n\nCASTELLANOS, MARIA\nx\n\n\n\nCHAKRABURTY, SRIJITA\nx\n\n\n\nCHEN, ENFU\nx\n\n\n\nCHENG, WEI\nx\n\n\n\nCOATES, P TOBY\nx\n\n\n\nCROFT, NATHAN P\nx\n\n\n\nDALE, ARIELLA PERRY\nx\n\n\n\nDENHOLM, J T\nx\n\n\n\nDOBBIN, KEVIN\nx\n\n\n\nDUDEK, NADINE L\nx\n\n\n\nEBELL, MARK\nx\n\n\n\nEBELL, MARK H\nx\n\n\n\nEGGENHUIZEN, PETER J\nx\n\n\n\nFOREHAND, RONALD\nx\n\n\n\nFUGGER, LARS\nx\n\n\n\nGAN, POH Y\nx\n\n\n\nGARCIA-SASTRE, ADOLFO\nx\n\n\n\nGREGERSEN, JON W\nx\n\n\n\nGUAN, JING\nx\n\n\n\nHALLORAN, M ELIZABETH\nx\n\n\n\nHANDEL, A\nx\n\n\n\nHANDEL, ANDREAS\nx\n\n\n\nHECKMAN, TIMOTHY G\nx\n\n\n\nHOLDSWORTH, STEPHEN R\nx\n\n\n\nHOLT, STEPHEN G\nx\n\n\n\nHOUBEN, R M G J\nx\n\n\n\nHUANG, HAODI\nx\n\n\n\nHUDSON, BILLY G\nx\n\n\n\nHUO, XIANG\nx\n\n\n\nHUYNH, MEGAN\nx\n\n\n\nJOLOBA, MOSES L\nx\n\n\n\nKAKAIRE, R\nx\n\n\n\nKIRIMUNDA, S\nx\n\n\n\nKITCHING, A RICHARD\nx\n\n\n\nKIWANUKA, N\nx\n\n\n\nLA GRUTA, NICOLE L\nx\n\n\n\nLI, CHANGWEI\nx\n\n\n\nLI, CHAO\nx\n\n\n\nLI, YAN\nx\n\n\n\nLING, FENG\nx\n\n\n\nLOH, KHAI L\nx\n\n\n\nLONGINI, IRA M\nx\n\n\n\nMALONE, LASHAUNDA L\nx\n\n\n\nMANICASSAMY, BALAJI\nx\n\n\n\nMARTINEZ, L\nx\n\n\n\nMARTINEZ, LEONARDO\nx\n\n\n\nMCBRYDE, E S\nx\n\n\n\nMCKAY, BRIAN\nx\n\n\n\nMOORE, JAMES R\nx\n\n\n\nMU, LAN\nx\n\n\n\nOOI, JOSHUA D\nx\n\n\n\nPAWELEK, KASIA A\nx\n\n\n\nPETERSEN, JAN\nx\n\n\n\nPOWER, DAVID A\nx\n\n\n\nPURCELL, ANTHONY W\nx\n\n\n\nQUACH, T\nx\n\n\n\nQUINN, FREDERICK D\nx\n\n\n\nRAGONNET, R\nx\n\n\n\nRAMARATHINAM, SRI H\nx\n\n\n\nREID, HUGH H\nx\n\n\n\nROSSJOHN, JAMIE\nx\n\n\n\nSETTE, ALESSANDRO\nx\n\n\n\nSHEN, YE\nx\n\n\n\nSIDNEY, JOHN\nx\n\n\n\nSLIFKA, MARK\nx\n\n\n\nSNG, XAVIER Y X\nx\n\n\n\nSTEIN, CATHERINE M\nx\n\n\n\nSUMNER, T\nx\n\n\n\nTAN, YU H\nx\n\n\n\nTHOMAS, PAUL G\nx\n\n\n\nTRAUER, J M\nx\n\n\n\nTSCHARKE, DAVID C\nx\n\n\n\nWAKIM, LINDA M\nx\n\n\n\nWANG, XIAOXIAO\nx\n\n\n\nWATSON, KATHERINE A\nx\n\n\n\nWHALEN, C C\nx\n\n\n\nWHALEN, CHRISTOPHER C\nx\n\n\n\nWILLETT, ZOE J\nx\n\n\n\nWOLDU, H\nx\n\n\n\nWOLDU, HENOK\nx\n\n\n\nWU, TING\nx\n\n\n\nZALWANGO, S\nx\n\n\n\nZALWANGO, SARAH\nx\n\n\n\nZARNITSYNA, VERONIKA\nx\n\n\n\nZARNITSYNA, VERONIKA I\nx\n\n\n\nZHU, LIMEI\nx\n\n\n\n\r\r\rvar dest = document.getElementById(\"ad329918-8ff2-4ae6-ae76-9ba5e1f49f30\");\rvar template = document.getElementById(\"346a0d98-4309-4a26-9a0b-f73298d4a474\");\rvar caption = template.content.querySelector(\"caption\");\rif(caption) {\rcaption.style.cssText = \"display:block;text-align:center;\";\rvar newcapt = document.createElement(\"p\");\rnewcapt.appendChild(caption)\rdest.parentNode.insertBefore(newcapt, dest.previousSibling);\r}\rvar fantome = dest.attachShadow({mode: 'open'});\rvar templateContent = template.content;\rfantome.appendChild(templateContent);\r\rflextable::save_as_docx(\u0026quot;my table\u0026quot; = ft, path = \u0026quot;COItable.docx\u0026quot;)\r I notice a few duplicates in the table that need to be removed. Of course I also need to remove myself. And for some, the full name doesn‚Äôt show. I need to fill in a few of the other columns and potentially add a few individuals who were not captured. So it‚Äôs not fully automated, but I can copy this table into the COI statement and the remaining edits are still annoying but not that terrible.\nDiscussion These kinds of COI documents that ask for all co-authors are in my opinion antiquated and should go away. In the meantime using a somewhat automated approach makes the problem not too bad. I will have to make a few manual adjustments to the table, but overall it‚Äôs not too bad. I‚Äôm still glad that NIH does not require this.\n","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"196df4457ac53280e4112f53b968b7e7","permalink":"https://www.andreashandel.com/posts/conflict-of-interest-form/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/posts/conflict-of-interest-form/","section":"posts","summary":"Some code and examples showing how to generate a conflict of interest statement required by some funding agencies in an almost completely automated manner.","tags":["R","Grant Writing"],"title":"Generating a conflict of interest form automatically","type":"posts"},{"authors":["Andreas Handel"],"categories":["presentation","research","COVID-19"],"content":"These are slides for a talk discussing some recent COVID-19 projects. I gave the talk (online) at GA Southern. The presentation slides are here.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"54b6341768399de52492cf978d21b635","permalink":"https://www.andreashandel.com/presentations/2020-04-gasouthern-covid/","publishdate":"2020-04-12T00:00:00Z","relpermalink":"/presentations/2020-04-gasouthern-covid/","section":"presentations","summary":"A (virtual) presentation at GA Southern discussing some recent COVID-19 projects.","tags":null,"title":"Some recent analysis and modeling applied to COVID-19","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","research","tuberculosis"],"content":"These are slides for a talk discussing a recent TB superspreader project I did, as well as some general background on the topic. I gave the talk to the Epidemiology in Action group (Dr. Whalen\u0026rsquo;s research group) at UGA. The presentation slides are here.\n","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"532135de476ca4eeecdcdc7e9a1bdcfc","permalink":"https://www.andreashandel.com/presentations/2020-03-tb-superspreaders/","publishdate":"2020-03-02T00:00:00Z","relpermalink":"/presentations/2020-03-tb-superspreaders/","section":"presentations","summary":"A presentation on some TB superspreading work I did for the EIA research group at UGA.","tags":null,"title":"Tuberculosis Superspreading","type":"presentations"},{"authors":["Andreas Handel"],"categories":["presentation","academia"],"content":"These are slides for a talk/discussion I led with our graduate students on how to efficiently read the (peer reviewed) literature, how to manage your references, and how to write and publish papers. The presentation slides are here.\n","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"46593dccc0ef65a6ee5b6e60495ab4dc","permalink":"https://www.andreashandel.com/presentations/2020-02-reading-managing-publishing-papers/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/presentations/2020-02-reading-managing-publishing-papers/","section":"presentations","summary":"A presentation for our departmental PhD students on various topics related to (peer reviewed) papers.","tags":null,"title":"Reading, managing and publishing papers","type":"presentations"},{"authors":null,"categories":["R","Data Analysis","bibliometrics"],"content":"Overview I needed some information on all my publications for \u0026ldquo;bean counting\u0026rdquo; purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications.\nInstead of doing things by hand, there are nicer/faster ways using R. in part 1, I did a few things using the scholar package. While some parts worked nicely, I encountered 2 problems. First, since my Google Scholar record lists items other than peer-reviewed journal articles, they show up in the analysis and need to be cleaned out. Second, Google Scholar doesn\u0026rsquo;t like automated queries through the API and is quick to block, at which point things don\u0026rsquo;t work anymore.\nTo get around these issues, I decided to give a different R package a try, namely bibliometrix. The workflow is somewhat different.\nThe RMarkdown file to run this analysis is here.\nRequired packages library(dplyr)\rlibrary(knitr)\rlibrary(bibliometrix)\r Loading data Old: I keep all references to my published papers in a BibTeX file, managed through Zotero/Jabref. I know this file is clean and correct. I\u0026rsquo;m loading it here for processing. If you don\u0026rsquo;t have such a file, make one using your favorite reference manager. Or create it through a saved search on a bibliographic database, as explained on the bibliometrix website.\nNew: In the current version of bibliometrix, reading in my bibtex file failed. A fairly good alternative is to go to your NIH \u0026ldquo;My Bibliography\u0026rdquo; (which anyone with NIH funding needs to have anyway) and export it in MEDLINE format. Then read in the file with the code below. As of the time of writing this, it requires the Github version of bibliometrix.\n#read bib file, turn file of references into data frame\rpubs \u0026lt;- bibliometrix::convert2df(\u0026quot;medline.txt\u0026quot;, dbsource=\u0026quot;pubmed\u0026quot;,format=\u0026quot;pubmed\u0026quot;)  ## ## Converting your pubmed collection into a bibliographic dataframe\r## ## Done!\r## ## ## Generating affiliation field tag AU_UN from C1: Done!\r Each row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix documentation.\nAnalyzing 2 time periods For my purpose, I want to analyze 2 different time periods and compare them. Therefore, I split the data frame containing publications, then run the analysis on each.\n#get all pubs for an author (or multiple)\rperiod_1_start = 2009\rperiod_2_start = 2015\r#here I want to separately look at publications in the 2 time periods I defined above\rpubs_old \u0026lt;- data.frame(pubs) %\u0026gt;% dplyr::filter((PY\u0026gt;=period_1_start \u0026amp; PY\u0026lt;period_2_start ))\rpubs_new \u0026lt;- data.frame(pubs) %\u0026gt;% dplyr::filter(PY\u0026gt;=period_2_start)\rres_old \u0026lt;- bibliometrix::biblioAnalysis(pubs_old, sep = \u0026quot;;\u0026quot;) #perform analysis\rres_new \u0026lt;- bibliometrix::biblioAnalysis(pubs_new, sep = \u0026quot;;\u0026quot;) #perform analysis\r General information The summary functions provide a lot of information in a fairly readable format. I apply them here to both time periods so I can compare.\nTime period 1\nsummary(res_old, k = 10)\r ## ## ## MAIN INFORMATION ABOUT DATA\r## ## Timespan 2009 : 2014 ## Sources (Journals, Books, etc) 12 ## Documents 19 ## Average years from publication 9.32 ## Average citations per documents 0 ## Average citations per year per doc 0 ## References 1 ## ## DOCUMENT TYPES ## clinical trial;journal article;research support, non-u.s. gov't 1 ## comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't 1 ## journal article 2 ## journal article;research support, n.i.h., extramural 5 ## journal article;research support, n.i.h., extramural;research support, non-u.s. gov't 3 ## journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s. 1 ## journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;review;systematic review 1 ## journal article;research support, non-u.s. gov't 3 ## journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, u.s. gov't, p.h.s. 1 ## journal article;review 1 ## ## DOCUMENT CONTENTS\r## Keywords Plus (ID) 148 ## Author's Keywords (DE) 148 ## ## AUTHORS\r## Authors 45 ## Author Appearances 80 ## Authors of single-authored documents 0 ## Authors of multi-authored documents 45 ## ## AUTHORS COLLABORATION\r## Single-authored documents 0 ## Documents per Author 0.422 ## Authors per Document 2.37 ## Co-Authors per Documents 4.21 ## Collaboration Index 2.37 ## ## ## Annual Scientific Production\r## ## Year Articles\r## 2009 5\r## 2010 2\r## 2011 1\r## 2012 3\r## 2013 2\r## 2014 6\r## ## Annual Percentage Growth Rate 3.713729 ## ## ## Most Productive Authors\r## ## Authors Articles Authors Articles Fractionalized\r## 1 HANDEL A 19 HANDEL A 5.55\r## 2 ANTIA R 6 ANTIA R 1.78\r## 3 DOHERTY PC 3 LONGINI IM JR 1.00\r## 4 LA GRUTA NL 3 DOHERTY PC 0.56\r## 5 LONGINI IM JR 3 LA GRUTA NL 0.56\r## 6 THOMAS PG 3 THOMAS PG 0.56\r## 7 PILYUGIN SS 2 BEAUCHEMIN CA 0.50\r## 8 ROHANI P 2 LI Y 0.50\r## 9 STALLKNECHT D 2 ROHANI P 0.50\r## 10 TURNER SJ 2 ROZEN DE 0.50\r## ## ## Top manuscripts per citations\r## ## Paper DOI TC TCperYear NTC\r## 1 ZHENG N, 2014, PLOS ONE 10.1371/JOURNAL.PONE.0105721 0 0 NaN\r## 2 HANDEL A, 2014, PROC BIOL SCI 10.1098/RSPB.2013.3051 0 0 NaN\r## 3 NGUYEN TH, 2014, J IMMUNOL 10.4049/JIMMUNOL.1303147 0 0 NaN\r## 4 LI Y, 2014, J THEOR BIOL 10.1016/J.JTBI.2014.01.008 0 0 NaN\r## 5 HANDEL A, 2014, J R SOC INTERFACE 10.1098/RSIF.2013.1083 0 0 NaN\r## 6 CUKALAC T, 2014, PROC NATL ACAD SCI U S A 10.1073/PNAS.1323736111 0 0 NaN\r## 7 HANDEL A, 2013, PLOS COMPUT BIOL 10.1371/JOURNAL.PCBI.1002989 0 0 NaN\r## 8 THOMAS PG, 2013, PROC NATL ACAD SCI U S A 10.1073/PNAS.1222149110 0 0 NaN\r## 9 JACKWOOD MW, 2012, INFECT GENET EVOL 10.1016/J.MEEGID.2012.05.003 0 0 NaN\r## 10 DESAI R, 2012, CLIN INFECT DIS 10.1093/CID/CIS372 0 0 NaN\r## ## ## Corresponding Author's Countries\r## ## Country Articles Freq SCP MCP MCP_Ratio\r## 1 USA 14 0.7778 11 3 0.214\r## 2 AUSTRALIA 3 0.1667 2 1 0.333\r## 3 CANADA 1 0.0556 1 0 0.000\r## ## ## SCP: Single Country Publications\r## ## MCP: Multiple Country Publications\r## ## ## Total Citations per Country\r## ## Country Total Citations Average Article Citations\r## 1 AUSTRALIA 0 0\r## 2 CANADA 0 0\r## 3 USA 0 0\r## ## ## Most Relevant Sources\r## ## Sources ## 1 JOURNAL OF THE ROYAL SOCIETY INTERFACE ## 2 JOURNAL OF THEORETICAL BIOLOGY ## 3 JOURNAL OF IMMUNOLOGY (BALTIMORE MD. : 1950) ## 4 PLOS ONE ## 5 PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA ## 6 BMC EVOLUTIONARY BIOLOGY ## 7 BMC PUBLIC HEALTH ## 8 CLINICAL INFECTIOUS DISEASES : AN OFFICIAL PUBLICATION OF THE INFECTIOUS DISEASES SOCIETY OF AMERICA ## 9 EPIDEMICS ## 10 INFECTION GENETICS AND EVOLUTION : JOURNAL OF MOLECULAR EPIDEMIOLOGY AND EVOLUTIONARY GENETICS IN INFECTIOUS DISEASES\r## Articles\r## 1 3\r## 2 3\r## 3 2\r## 4 2\r## 5 2\r## 6 1\r## 7 1\r## 8 1\r## 9 1\r## 10 1\r## ## ## Most Relevant Keywords\r## ## Author Keywords (DE) Articles Keywords-Plus (ID) Articles\r## 1 HUMANS 13 HUMANS 13\r## 2 MODELS BIOLOGICAL 8 MODELS BIOLOGICAL 8\r## 3 ANIMALS 7 ANIMALS 7\r## 4 COMPUTER SIMULATION 5 COMPUTER SIMULATION 5\r## 5 BIOLOGICAL EVOLUTION 4 BIOLOGICAL EVOLUTION 4\r## 6 MODELS IMMUNOLOGICAL 4 MODELS IMMUNOLOGICAL 4\r## 7 FEMALE 3 FEMALE 3\r## 8 MICE 3 MICE 3\r## 9 MUTATION 3 MUTATION 3\r## 10 AMINO ACID SEQUENCE 2 AMINO ACID SEQUENCE 2\r Time period 2\nsummary(res_new, k = 10)\r ## ## ## MAIN INFORMATION ABOUT DATA\r## ## Timespan 2015 : 2020 ## Sources (Journals, Books, etc) 22 ## Documents 29 ## Average years from publication 3.72 ## Average citations per documents 0 ## Average citations per year per doc 0 ## References 1 ## ## DOCUMENT TYPES ## comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't 1 ## journal article 7 ## journal article;multicenter study;research support, n.i.h., extramural 1 ## journal article;research support, n.i.h., extramural 5 ## journal article;research support, n.i.h., extramural;research support, non-u.s. gov't 5 ## journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;review 1 ## journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s. 1 ## journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.;review 1 ## journal article;research support, non-u.s. gov't 4 ## journal article;research support, non-u.s. gov't;research support, n.i.h., extramural 1 ## journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, n.i.h., extramural 1 ## letter 1 ## ## DOCUMENT CONTENTS\r## Keywords Plus (ID) 198 ## Author's Keywords (DE) 198 ## ## AUTHORS\r## Authors 209 ## Author Appearances 332 ## Authors of single-authored documents 1 ## Authors of multi-authored documents 208 ## ## AUTHORS COLLABORATION\r## Single-authored documents 1 ## Documents per Author 0.139 ## Authors per Document 7.21 ## Co-Authors per Documents 11.4 ## Collaboration Index 7.43 ## ## ## Annual Scientific Production\r## ## Year Articles\r## 2015 5\r## 2016 7\r## 2017 3\r## 2018 6\r## 2019 5\r## 2020 3\r## ## Annual Percentage Growth Rate -9.711955 ## ## ## Most Productive Authors\r## ## Authors Articles Authors Articles Fractionalized\r## 1 HANDEL A 29 HANDEL A 5.494\r## 2 WHALEN CC 7 ANTIA R 0.810\r## 3 ANTIA R 5 SHEN Y 0.723\r## 4 MARTINEZ L 5 WHALEN CC 0.651\r## 5 SHEN Y 5 MCKAY B 0.629\r## 6 LA GRUTA NL 4 EBELL MH 0.571\r## 7 MCKAY B 4 THOMAS PG 0.571\r## 8 THOMAS PG 4 LA GRUTA NL 0.540\r## 9 ZALWANGO S 4 ROHANI P 0.500\r## 10 DENHOLM JT 3 MARTINEZ L 0.485\r## ## ## Top manuscripts per citations\r## ## Paper DOI TC TCperYear NTC\r## 1 MCKAY B, 2020, PROC BIOL SCI 10.1098/RSPB.2020.0496 0 0 NaN\r## 2 MOORE JR, 2020, BULL MATH BIOL 10.1007/S11538-020-00711-4 0 0 NaN\r## 3 HANDEL A, 2020, NAT REV IMMUNOL 10.1038/S41577-019-0235-3 0 0 NaN\r## 4 MARTINEZ L, 2019, J INFECT DIS 10.1093/INFDIS/JIZ328 0 0 NaN\r## 5 WU T, 2019, NAT COMMUN 10.1038/S41467-019-10661-8 0 0 NaN\r## 6 MCKAY B, 2019, PLOS ONE 10.1371/JOURNAL.PONE.0217219 0 0 NaN\r## 7 DALE AP, 2019, J AM BOARD FAM MED SOCIOLOGICAL METHODS \u0026amp; RESEARCH 10.3122/JABFM.2019.02.180183 0 0 NaN\r## 8 WOLDU H, 2019, J APPL STAT 10.1080/02664763.2018.1470231 0 0 NaN\r## 9 HANDEL A, 2018, PLOS COMPUT BIOL 10.1371/JOURNAL.PCBI.1006505 0 0 NaN\r## 10 CASTELLANOS ME, 2018, INT J TUBERC LUNG DIS 10.5588/IJTLD.18.0073 0 0 NaN\r## ## ## Corresponding Author's Countries\r## ## Country Articles Freq SCP MCP MCP_Ratio\r## 1 USA 16 0.696 7 9 0.562\r## 2 AUSTRALIA 5 0.217 1 4 0.800\r## 3 GEORGIA 2 0.087 0 2 1.000\r## ## ## SCP: Single Country Publications\r## ## MCP: Multiple Country Publications\r## ## ## Total Citations per Country\r## ## Country Total Citations Average Article Citations\r## 1 AUSTRALIA 0 0\r## 2 GEORGIA 0 0\r## 3 USA 0 0\r## ## ## Most Relevant Sources\r## ## Sources ## 1 PLOS ONE ## 2 PLOS COMPUTATIONAL BIOLOGY ## 3 THE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\r## 4 THE LANCET. GLOBAL HEALTH ## 5 THE LANCET. RESPIRATORY MEDICINE ## 6 AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE ## 7 BMC INFECTIOUS DISEASES ## 8 BULLETIN OF MATHEMATICAL BIOLOGY ## 9 ELIFE ## 10 EPIDEMICS ## Articles\r## 1 4\r## 2 2\r## 3 2\r## 4 2\r## 5 2\r## 6 1\r## 7 1\r## 8 1\r## 9 1\r## 10 1\r## ## ## Most Relevant Keywords\r## ## Author Keywords (DE) Articles Keywords-Plus (ID) Articles\r## 1 HUMANS 23 HUMANS 23\r## 2 ANIMALS 8 ANIMALS 8\r## 3 FEMALE 7 FEMALE 7\r## 4 MALE 7 MALE 7\r## 5 MICE 6 MICE 6\r## 6 ADULT 5 ADULT 5\r## 7 CHILD 5 CHILD 5\r## 8 ADOLESCENT 4 ADOLESCENT 4\r## 9 ANTIVIRAL AGENTS/THERAPEUTIC USE 4 ANTIVIRAL AGENTS/THERAPEUTIC USE 4\r## 10 CHILD PRESCHOOL 4 CHILD PRESCHOOL 4\r Note that some values are reported as NA, e.g. the citations. Depending on which source you got the original data from, that information might be included or not. In my case, it is not.\nGetting a table of co-authors This can be useful for NSF applications. For reasons nobody understands, that agency still asks for a list of all co-authors. An insane request in the age of modern science. If one wanted to do that, the following gives a table.\nUpdate: I have since created a short blog post describing how to do just that part in a bit more detail. It has a few additional components that might be useful, if interested check it out here.\nHere is the full table of my co-authors in the first period dataset.\n#removing the 1st one since that's me\rauthortable = data.frame(res_old$Authors[-1])\rcolnames(authortable) = c('Co-author name', 'Number of publications')\rknitr::kable(authortable)\r    Co-author name Number of publications     ANTIA R 6   DOHERTY PC 3   LA GRUTA NL 3   LONGINI IM JR 3   THOMAS PG 3   PILYUGIN SS 2   ROHANI P 2   STALLKNECHT D 2   TURNER SJ 2   AKIN V 1   BEAUCHEMIN CA 1   BIRD NL 1   BROWN J 1   CHADDERTON J 1   CUKALAC T 1   DESAI R 1   DICKEY BW 1   FUNG IC 1   HALL AJ 1   HALL D 1   HEMBREE CD 1   JACKWOOD MW 1   KEDZIERSKA K 1   KJER-NIELSEN L 1   KOTSIMBOS TC 1   LEBARBENCHON C 1   LEON JS 1   LEVIN BR 1   LI Y 1   LOPMAN B 1   MARGOLIS E 1   MATTHEWS JE 1   MCDONALD S 1   MIFSUD NA 1   MOFFAT JM 1   NGUYEN TH 1   PARASHAR UD 1   PELLICCI DG 1   ROWNTREE LC 1   ROZEN DE 1   WHALEN CC 1   YATES A 1   ZARNITSYNA V 1   ZHENG N 1    Since I have many more co-authors in the second period, I\u0026rsquo;m not printing a table with all, instead I\u0026rsquo;m just doing those with whom I have more than 2 joint publications.\n#removing the 1st one since that's me\rauthortable = data.frame(res_new$Authors[-1])\rauthortable \u0026lt;- authortable %\u0026gt;% dplyr::filter(Freq\u0026gt;2)\rcolnames(authortable) = c('Co-author name', 'Number of publications')\rknitr::kable(authortable)\r    Co-author name Number of publications     WHALEN CC 7   ANTIA R 5   MARTINEZ L 5   SHEN Y 5   LA GRUTA NL 4   MCKAY B 4   THOMAS PG 4   ZALWANGO S 4   DENHOLM JT 3   EBELL M 3   MCBRYDE ES 3   SUMNER T 3   TRAUER JM 3    Making a table of journals It can be useful to get a list of all journals in which you published. I\u0026rsquo;m doing this here for the second time period. With just the bibliometrix package, I can get a list of publications and how often I have published in each.\njournaltable = data.frame(res_new$Sources)\r#knitr::kable(journaltable) #uncomment this to print the table\r It might also be nice to get some journal metrics, such as impact factors. While this is possible with the scholar package, the bibliometrix package doesn\u0026rsquo;t have it.\nHowever, the scholar package doesn\u0026rsquo;t really get that data from Google Scholar, instead it has an internal spreadsheet/table with impact factors (according to the documentation, taken - probably not fully legally - from some spreadsheet posted on ResearchGate). We can thus access those impact factors stored in the scholar package without having to connect to Google Scholar. As long as the journal names stored in the scholar package are close to the ones we have here, we might get matches.\nlibrary(scholar)\rifvalues = scholar::get_impactfactor(journaltable[,1], max.distance = 0.1)\r ## The impact factor data is out-of-date and we may remove this function in future release.\r journaltable = cbind(journaltable, ifvalues$ImpactFactor)\rcolnames(journaltable) = c('Journal','Number of Pubs','Impact Factor')\rknitr::kable(journaltable)\r    Journal Number of Pubs Impact Factor     PLOS ONE 4 2.766   PLOS COMPUTATIONAL BIOLOGY 2 3.955   THE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE 2 NA   THE LANCET. GLOBAL HEALTH 2 NA   THE LANCET. RESPIRATORY MEDICINE 2 NA   AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE 1 15.239   BMC INFECTIOUS DISEASES 1 2.620   BULLETIN OF MATHEMATICAL BIOLOGY 1 1.484   ELIFE 1 7.616   EPIDEMICS 1 3.364   EPIDEMIOLOGY AND INFECTION 1 2.044   FRONTIERS IN IMMUNOLOGY 1 5.511   JOURNAL OF APPLIED STATISTICS 1 0.699   JOURNAL OF THE AMERICAN BOARD OF FAMILY MEDICINE : JABFM 1 NA   NATURE 1 41.577   NATURE COMMUNICATIONS 1 12.353   NATURE REVIEWS. IMMUNOLOGY 1 41.982   PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY OF LONDON. SERIES B BIOLOGICAL SCIENCES 1 NA   PLOS BIOLOGY 1 9.163   PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA 1 9.504   PROCEEDINGS. BIOLOGICAL SCIENCES 1 NA   THE JOURNAL OF INFECTIOUS DISEASES 1 5.186    Ok that worked somewhat. It couldn\u0026rsquo;t find several journals. The reported IF seem reasonable. But since I don\u0026rsquo;t know what year those IF are from, and if the rest is fully reliable, I would take this with a grain of salt.\nDiscussion The bibliometrix package doesn\u0026rsquo;t suffer from the problems that I encountered in part 1 of this post when I tried the scholar package (and Google Scholar). The downside is that I can\u0026rsquo;t get some of the information, e.g. my annual citations. So it seems there is not (yet) a comprehensive solution, and using both packages seems best.\nA larger overall problem is that a lot of this information is controlled by corporations (Google, Elsevier, Clarivate Analytics, etc.), which might or might not allow R packages and individual users (who don\u0026rsquo;t subscribe to their offerings) to access certain information. As such, R packages accessing this information will need to adjust to whatever the companies allow.\n","date":1580601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621036800,"objectID":"6db3617530d60fde3007e3126eb7b62a","permalink":"https://www.andreashandel.com/posts/publications-analysis-2/","publishdate":"2020-02-02T00:00:00Z","relpermalink":"/posts/publications-analysis-2/","section":"posts","summary":"Some code and examples showing how to process and analyze meta-data for a set of publications using the `bibilometrix` R package.","tags":["R","Data Analysis","bibliometrics"],"title":"Using R to analyze publications - part 2","type":"posts"},{"authors":null,"categories":["R","Data Analysis","bibliometrics"],"content":"Overview I needed some information on all my publications for \u0026ldquo;bean counting\u0026rdquo; purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications.\nInstead of doing things by hand, there are nicer/faster ways using R. The following shows a few things one can do with the scholar package. I describe an alternative approach using the bibliometrix package in part 2 of this post.\nThe RMarkdown file to run this analysis is here.\nNotes   As of this writing, the scholar R package seems semi-dormant and not under active development. If Google changes their API for Scholar and the package isn\u0026rsquo;t updated, the below code might stop working.\n  A problem I keep encountering with Google Scholar is that it starts blocking requests, even after what I consider are not that many attempts to retrieve data. I notice that when I try to pull references from Google Scholar using JabRef and also with the code below. If that happens to you, try a different computer, or clear cookies. This is a well known problem and if you search online, you find others complaining about it. I haven\u0026rsquo;t found a great solution yet, other than not using the Google Scholar data. I describe such an approach in part 2 of this post. However, some analyses are only able with Google Scholar information.\n  To minimize chances of getting locked out by Google, I wrote the code below such that it only sends a request if there isn\u0026rsquo;t a local file already containing that data. To refresh data, delete the local files.\n  Required packages library(scholar)\rlibrary(dplyr)\rlibrary(tidyr)\rlibrary(knitr)\rlibrary(ggplot2)\r Get all citations for an individual First, I\u0026rsquo;m using Google Scholar to get all citations for a specific author (in this case, myself).\n#Define the person to analyze\rscholar_id=\u0026quot;bruHK0YAAAAJ\u0026quot; #either load existing file of publications\r#or get a new one from Google Scholar\r#delete the file to force an update\rif (file.exists('citations.Rds'))\r{\rcites \u0026lt;- readRDS('citations.Rds')\r} else {\r#get citations\rcites \u0026lt;- scholar::get_citation_history(scholar_id) saveRDS(cites,'citations.Rds')\r}\r Compare citations for different time periods For my purpose, I want to compare citations between 2 time periods (my Assistant Professor time and my Associate Professor time). I\u0026rsquo;m splitting them into 2. I\u0026rsquo;m doing this analysis at the beginning of 2020 and want only full years. The code snippets below give me what I need, two time periods 2009-2014 and 2014-2019.\nperiod_1_start = 2009\rperiod_2_start = 2015\rcites_1 \u0026lt;- cites %\u0026gt;% dplyr::filter((year\u0026gt;=period_1_start \u0026amp; year\u0026lt;period_2_start ))\r#remove last year since it's not a full year\rcites_2 \u0026lt;- cites %\u0026gt;% dplyr::filter((year\u0026gt;=period_2_start \u0026amp; year\u0026lt;2020 ))  Fitting a linear model to both time segments to look at increase in citations over both periods.\nfit1=lm(cites ~ year, data = cites_1)\rfit2=lm(cites ~ year, data = cites_2)\rinc1 = fit1$coefficients[\u0026quot;year\u0026quot;]\rinc2 = fit2$coefficients[\u0026quot;year\u0026quot;] print(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))\r ## [1] \u0026quot;Annual increase for periods 1 and 2 are 22.257143, 43.100000\u0026quot;\r Making a figure to show citation count increases\n# combine data above into single data frame\r#add a variable to indicate period 1 and period 2\rcites_1$group = \u0026quot;1\u0026quot;\rcites_2$group = \u0026quot;2\u0026quot;\rcites_df = rbind(cites_1,cites_2)\rxlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]\r#make the plot and show linear fit lines\rp1 \u0026lt;- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) + geom_point(size = I(4)) + geom_smooth(method=\u0026quot;lm\u0026quot;,aes(group = group), se = F, size=1.5) + scale_x_continuous(name = \u0026quot;Year\u0026quot;, breaks = xlabel, labels = xlabel) + scale_y_continuous(\u0026quot;Citations according to Google Scholar\u0026quot;) +\rtheme_bw(base_size=14) + theme(legend.position=\u0026quot;none\u0026quot;) + geom_text(aes(NULL,NULL),x=2010.8,y=150,label=\u0026quot;Average annual \\n increase 22%\u0026quot;,color=\u0026quot;black\u0026quot;,size=5.5) +\rgeom_text(aes(NULL,NULL),x=2017,y=150,label=\u0026quot;Average annual \\n increase 43%\u0026quot;,color=\u0026quot;black\u0026quot;,size=5.5) #open a new graphics window\r#note that this is Windows specific. Use quartz() for MacOS\rww=5; wh=5; windows(width=ww, height=wh)\tprint(p1)\r ## `geom_smooth()` using formula 'y ~ x'\r dev.print(device=png,width=ww,height=wh,units=\u0026quot;in\u0026quot;,res=600,file=\u0026quot;citations.png\u0026quot;)\r ## png ## 2\r Getting list of publications Above I got citations, but not publications. This function retrieves all publications for a specific author and returns it as a data frame.\n#get all pubs for an author (or multiple)\rif (file.exists('publications.Rds'))\r{\rpublications \u0026lt;- readRDS('publications.Rds')\r} else {\r#get citations\rpublications \u0026lt;- scholar::get_publications(scholar_id) saveRDS(publications,'publications.Rds')\r}\r Quick peek at publications glimpse(publications)\r ## Rows: 90\r## Columns: 8\r## $ title \u0026lt;fct\u0026gt; \u0026quot;Severe outcomes are associated with genogroup 2 genotype 4 no~\r## $ author \u0026lt;fct\u0026gt; \u0026quot;R Desai, CD Hembree, A Handel, JE Matthews, BW Dickey, S McDo~\r## $ journal \u0026lt;fct\u0026gt; \u0026quot;Clinical infectious diseases\u0026quot;, \u0026quot;BMC public health\u0026quot;, \u0026quot;Journal ~\r## $ number \u0026lt;fct\u0026gt; \u0026quot;55 (2), 189-193\u0026quot;, \u0026quot;11 (S1), S7\u0026quot;, \u0026quot;7 (42), 35-47\u0026quot;, \u0026quot;3 (12)\u0026quot;, \u0026quot;~\r## $ cites \u0026lt;dbl\u0026gt; 163, 158, 129, 124, 123, 115, 105, 89, 71, 71, 55, 53, 52, 49,~\r## $ year \u0026lt;dbl\u0026gt; 2012, 2011, 2010, 2007, 2006, 2012, 2006, 2017, 2016, 2008, 20~\r## $ cid \u0026lt;fct\u0026gt; 1979732925283755485, 10982184786304722425, 1038596204985444772~\r## $ pubid \u0026lt;fct\u0026gt; 5nxA0vEk-isC, _FxGoFyzp5QC, 9yKSN-GCB0IC, d1gkVwhDpl0C, u5HHmV~\r This shows the variables obtained in the data frame. One thing I notice is that this contains more entries than I have peer-reviewed publications. Since most people\u0026rsquo;s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis. That is not ideal. I\u0026rsquo;ll do/show a few more possible analyses, but decided to do the analyses below using the approach in part 2.\nMaking a table of journals and impact factors The scholar package has a function that allows one to get impact factors for journals. This data doesn\u0026rsquo;t actually come from Google Scholar, instead the package comes with an internal spreadsheet/table with impact factors. Looking a bit into the scholar package indicates that the data was taken from some spreadsheet posted on ResearchGate (probably not fully legal). Either way, let\u0026rsquo;s give it a try.\n#here I only want publications since 2015\rpub_reduced \u0026lt;- publications %\u0026gt;% dplyr::filter(year\u0026gt;2014)\rifdata \u0026lt;- scholar::get_impactfactor(pub_reduced$journal)  ## The impact factor data is out-of-date and we may remove this function in future release.\r #Google SCholar collects all kinds of 'publications'\r#including items other than standard peer-reviewed papers\r#this sorts and removes some non-journal entries iftable \u0026lt;- ifdata %\u0026gt;% dplyr::arrange(desc(ImpactFactor) ) %\u0026gt;% tidyr::drop_na()\rknitr::kable(iftable)\r    Journal Cites ImpactFactor Eigenfactor     CA-A CANCER JOURNAL FOR CLINICIANS 28839 244.585 0.06603   NATURE REVIEWS IMMUNOLOGY 39215 41.982 0.08536   NATURE 710766 41.577 1.35581   AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE 61024 15.239 0.08683   Nature Communications 178348 12.353 0.92656   PLOS BIOLOGY 28750 9.163 0.05868   eLife 25592 7.616 0.19592   eLife 25592 7.616 0.19592   PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES 41872 5.666 0.07129   Frontiers in Immunology 16999 5.511 0.06747   RHEUMATOLOGY 18744 5.245 0.03381   RHEUMATOLOGY 18744 5.245 0.03381   PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES 51704 4.847 0.08756   PROCEEDINGS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES 51704 4.847 0.08756   PLoS Computational Biology 23758 3.955 0.08279   PLoS Computational Biology 23758 3.955 0.08279   SOCIOLOGICAL METHODS \u0026amp; RESEARCH 4177 3.625 0.00462   Epidemics 576 3.364 0.00241   PATHOLOGY 2638 3.068 0.00446   PLoS One 582877 2.766 1.86235   PLoS One 582877 2.766 1.86235   PLoS One 582877 2.766 1.86235   PLoS One 582877 2.766 1.86235   PLoS One 582877 2.766 1.86235   PLoS One 582877 2.766 1.86235   BMC INFECTIOUS DISEASES 13612 2.620 0.04109   BMC IMMUNOLOGY 1784 2.615 0.00321   NEPHROLOGY 3115 2.178 0.00605   BULLETIN OF MATHEMATICAL BIOLOGY 3736 1.484 0.00447   JAPANESE JOURNAL OF INFECTIOUS DISEASES 1722 1.014 0.00242   COMPUTATIONAL STATISTICS 958 0.828 0.00337   JOURNAL OF APPLIED STATISTICS 2352 0.699 0.00379    OK so this doesn\u0026rsquo;t quite work. I know for instance that I didn\u0026rsquo;t publish anything in Cancer Journal for Clinicians and the 2 Rheumatology entries are workshop presentations. Oddly, when I look at publications$journal there is no Cancer Journal listed. Somehow this is a bug created by the get_impactfactor() function. I could fix that by hand. The bigger problem is what to do with all those publications that are not peer-reviewed papers. I could remove them from my Google scholar profile. But I kind of want to keep them there since some of them link to useful stuff. I could alternatively manually clean things at this step. This somewhat defeats the purpose of automation.\nGetting list of co-authors Another useful piece of information to have, e.g. for NSF grants, is a table with all co-authors. Unfortunately, get_publications() only pulls from the main Google Scholar page, which cuts off the author list. To get all authors, one needs to run through each paper using get_complete_authors(). The problem is that Google cuts off access if one sends too many queries. If you get error messages, it might be that Google blocked you. See the Notes section.\nallauthors = list()\rif (file.exists('allauthors.Rds'))\r{\rallauthors \u0026lt;- readRDS('allauthors.Rds')\r} else {\rfor (n in 1:nrow(publications)) {\rallauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)\r}\rsaveRDS(allauthors,'allauthors.Rds')\r}\r Theoretically, if the above code runs without Google blocking things, I should end up with a list of all co-authors which I could then turn into a table. The problem is still that it pulls all entries on my Google Scholar profile, and not just peer-reviewed papers. With a bit of cleaning I could get what I need. But overall I don\u0026rsquo;t like this approach too much.\nDiscussion While the scholar package has some nice features, it has 2 major problems:\n Google blocking the script if it decides too many requests are made (that can happen quickly). Since most people\u0026rsquo;s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis.  I do keep all my published, peer-reviewed papers in a BibTeX bibliography file in my reference manager (I\u0026rsquo;m using Zotero and/or Jabref). I know that file is clean and only contains peer-reviewed papers. Unfortunately, the scholar package can\u0026rsquo;t read in such data. In part 2 of this post series, I\u0026rsquo;ll use a different R package to produce the journal and author tables I tried making above.\nThe one feature only available through Google Scholar is the citation record and the analysis I did at the beginning if this post.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"c2fd7c2323825666cf66b874ce16c205","permalink":"https://www.andreashandel.com/posts/publications-analysis-1/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/posts/publications-analysis-1/","section":"posts","summary":"Some code and examples showing how to process and analyze meta-data for a set of publications using the `scholar` R package.","tags":["R","Data Analysis","bibliometrics"],"title":"Using R to analyze publications - part 1","type":"posts"},{"authors":["Andreas Handel"],"categories":["presentation","career"],"content":"These are slides for a talk/discussion I led with our graduate students on ways to build and curate an online presence and overall personal brand. The presentation slides are here.\n","date":1579737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579737600,"objectID":"56b8d5990f56ea05f08b896b03ac63bc","permalink":"https://www.andreashandel.com/presentations/2020-01-your-brand/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/presentations/2020-01-your-brand/","section":"presentations","summary":"A presentation for our departmental PhD students on how to build and manage their brand (aka online presence).","tags":null,"title":"Building and curating your brand","type":"presentations"},{"authors":null,"categories":[],"content":"This is part 2/2 of the website development posts, where we\u0026rsquo;ll move the website to GitHub. This provides a nice and more automated workflow for editing and deploying your site.\nIf you haven\u0026rsquo;t seen part 1 yet, I suggest you read through that first. In that part, I covered how to create a website using blogdown, Hugo, and Netlify.\nRequired skills I assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nI assume your website is at a stage as described at the end of part 1.\nQuick tool overview The only new tool for this part is GitHub. GitHub is a very powerful and common way of developing projects like code, research projects or a website. Github is a great tool to be familiar with in general, and most importantly, it very nicely integrates with the other tools to make things seamless and automated. As you learned in part 1, it is not strictly needed, but it makes updating automatic and is such an overall useful tool to be exposed to that I\u0026rsquo;m including it in the setup.\nGet a GitHub account If you do not already have a GitHub account, create one. Note that GitHub is widely used professionally, and you might want to allow other people to see your GitHub presence. I, therefore, recommend using a future-proof, professional user name.\nNote that Git and GitHub (which are technically different, here I\u0026rsquo;m using GitHub to refer to both) can be initially confusing, mainly because they use a lot of specialized terminology. I will try and walk you through all steps for getting a website up and running in detail, but you might have to look up a few things on the way. If you are completely new to GitHub, I recommend you take a quick look at this page (and links provided there) so you can get some idea of what it\u0026rsquo;s all about.\nGet Gitkraken Download and install Gitkraken, link it with your GitHub account. Gitkraken is a graphical Git/GitHub client that makes a lot of tasks related to GitHub easier. It\u0026rsquo;s not strictly needed, and if you already have your own way of using Git/GitHub (e.g. with another client or the command line) you can stick with that. There is also an option to use Git/GitHub through RStudio, which is fine for most things, but overall Gitkraken is more powerful. So if you plan to use GitHub more in the future, I recommend using it. In the following, I assume you are using Gitkraken. If you interface with GitHub some other way, you will have to adjust those specific instructions accordingly.\nRecommended, but optional: Upgrade GitHub (\u0026amp; Gitkraken) On GitHub, by default, all repositories are public (a repository is the collection of files and folders that make up a specific GitHub project, such as your website.) If you have public repositories, you need to be careful about files that shouldn\u0026rsquo;t be shared publicly (e.g. because of copyright restrictions or because it might violate data privacy). Normally, if you want private repositories, you have to pay. As student or educator, you can get private repositories for free.\nIf you are a student, I strongly recommend you get the GitHub student developer pack. This gives you access to private repositories. You also get 1 year of free Gitkraken Pro access. While the free version of Gitkraken works well, you can\u0026rsquo;t access private repositories with it. Often, being able to use private repositories is useful.\nEducators can also get a free GitHub Pro account here. As far as I\u0026rsquo;m aware, there is no free Gitkraken Pro for educators, but it\u0026rsquo;s fairly cheap. So if you want to use a private repository for your website (I don\u0026rsquo;t know why you would), you need to pay for Gitkraken Pro or use a different way to manage your GitHub repositories.\nCreate a GitHub repository  Log into GitHub, click on \u0026lsquo;Repositories\u0026rsquo; and create a new repository (green button). Choose a repository (repo) name that tells you what\u0026rsquo;s in the repo (e.g. YOURNAME-website). You can give it the same name as you named your main website directory/project in part 1, but that\u0026rsquo;s not required. Check the box Initialize this repository with a README. Set the .gitignore option to R, you can leave the license at none. Click create repository. Clone the repository from GitHub to your local computer (using Gitkraken or whatever software/method you decided to use). Place the local folder in a location on your computer where it is not synced with some other software (e.g., Dropbox, OneDrive).  Move your website folder to the GitHub repo  Find the main folder of your website and move all of it into your newly created GitHub repository. To make sure everything is up-to-date, close RStudio before doing so. Open the repository you just created in Gitkraken. In the top right of Gitkraken, there should be a notification about changed files. Click View changes, then Stage. Write a commit message, commit the changes. You should see the master with the computer symbol above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line. If your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e. someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this project, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of this action. Go back to GitHub.com and to your repository. You should see all your files in there.  Connecting GitHub to Netlify  The last step is to set up Netlify so it can automatically monitor your GitHub repository and process any changes into an updated website. To do so, log into your Netlify account. Select your website. Under Site Settings find the Build and deploy menu. Under Continuous deployment click on Link site to Git. Choose GitHub and the follow the steps to link your webpage repository. Once finished, Netlify will monitor that repository and automatically pull any updates and rebuild your website.  The new workflow The workflow for your website using GitHub now works as follows:\n Open your website in Rstudio by clicking on the .Rproj file. Load blogdown with library(blogdown). Make any edits you want. While you make your edits, you can run serve_site() to see your updates. Once you are done with your updates, restart R (to shut down serve_site()) and run build_site(). While Netlify will automatically run Hugo on your files, it won\u0026rsquo;t do any processing of Rmarkdown files, therefore you need to run the blogdown build command. Use Gitkraken to push/pull and therefore sync changes between your local computer (or multiple computers if you work on more than one machine) and Github.com.  Netlify will monitor your Github repository and when it sees changes, automatically rebuild your website. Note that this means that if you start working on a document and don\u0026rsquo;t finish it, and then push to GitHub, your half-finished document shows up. To avoid that, you can set draft: true in your TOML/YAML heading for the document in progress.\nMore Information For the non-GitHub aspects of this, see the resources mentioned at the end of part 1. For some more GitHub related information, see e.g. here.\n","date":1579478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"a18087286dbb265cdaff3cf77929761a","permalink":"https://www.andreashandel.com/posts/blogdown-website-2/","publishdate":"2020-01-20T00:00:00Z","relpermalink":"/posts/blogdown-website-2/","section":"posts","summary":"This is part 2/2 of the website development posts, where we'll move the website to [GitHub](https://GitHub.com).","tags":[],"title":"Creating a website in less than 1 hour - part 2","type":"posts"},{"authors":null,"categories":["R","website"],"content":"Update 2021-03-01: Hugo and the Academic (now Wowchemy) theme keep changing rapidly. Since I originally posted this, things changed. I tried to update things to reflect some of the changes. If you find things not working, let me know so I can update.\nThe following are step-by-step instructions for creating your own website using blogdown, Hugo and Netlify.\nIn part 2, you will learn how to add GitHub to your workflow to make things even more automated and efficient.\nRequired skills I assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nWhat this document covers This document is meant to provide you with the minimum required instructions to get your own website up and running quickly. As such, instructions and background information are kept at a minimum. I used a recipe-like approach by giving hopefully detailed and specific enough instructions to get things to work. I\u0026rsquo;m not covering any why here or provide much further explanations. If you decide you like to use this setup for your website, you will likely want to go beyond this document and learn a bit more about the various tools involved in the process. To that end, links to further resources are provided. Those resources also discuss a bit why you would want this setup versus something like e.g. Wordpress.\nWho this is (not) for This way of making and hosting a website might be for you if:\n You are (or would like to be) an R, RMarkdown and GitHub user. This is a method of creating a website using those tools which very efficiently fits into such a workflow. You want a way to host a website where all the content is fully controlled by you, and the website can be hosted without much resources (and for free). You are curious about R/RMarkdown/GitHub, how to use it to build a website, and you\u0026rsquo;ve got a bit of time to spare and want to give it a try.  This way of making and hosting a website might not be for you if:\n Your main workflow is MS Word, Powerpoint, etc. and you are not interested in R/Markdown/GitHub. You want everything accessible through a graphical interface.  Motivating Examples If you want to see a few nice examples of websites written with blogdown/Hugo, check these out:\n Allison Hill Amber Thomas Malcolm Barrett Flip Tanedo  Many more examples of blogdown/Hugo websites are out there. See e.g. here.\nPointers to more resources and alternative tutorials (should you not like this one) can be found at the bottom of this post.\nQuick tool overview A few different software tools are working together for this website built. In most cases, they play nicely together and you don\u0026rsquo;t need to worry about details! But it is still good to know the names of the different components.\nR and RStudio we won\u0026rsquo;t really use those for their normal purpose as statistics/coding software/platform, but since blogdown is part of the RMarkdown family of packages, we need to use R and RStudio to access blogdown.\nblogdown is an R package that interfaces with Hugo and allows you to create websites easily using RMarkdown syntax.\nHugo is software that operates in the background. It takes all the content you create for your website (your text, your pictures, etc.), applies layout to your site based on a chosen theme, and turns everything into files that can be placed on Netlify and read by anyone\u0026rsquo;s web browser. You will likely not have to think or learn much about Hugo, at least not initially.\nNetlify is the service we\u0026rsquo;ll use to host the website. It offers a free plan, which is great for a starter website (you can always upgrade once you become famous and everyone visits your website. Netlify is very easy to set up and use, and it can automatically pull information from GitHub to build your site every time you update it (see part 2 for this).\nWhile Netlify and RStudio have paid options, for our purpose we can use their free versions. Hugo, blogdown and R are completely free.\nPre-requisites First, you need to set up accounts and install some software for the tools we will be using.\nGet a Netlify account Go to the Netlify website and sign up for an account. Follow the sign-up steps to set up your account.\nInstall R and RStudio If you don\u0026rsquo;t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it\u0026rsquo;s a recent version. If you have an older version of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach).\nStarting your website With the above registration and installation bits out of the way, you can get started with your website. To do so, follow these steps:\n Open RStudio. Install the blogdown package by typing install.packages('blogdown') in the R console (the lower left window in RStudio). Note that all R commands are case sensitive. You need to be online for this to work. You should see blogdown and several other packages being installed. You only need to do this step once, unless you upgrade R or change computers. If for some reason you already had blogdown installed, make sure it\u0026rsquo;s current and if needed, update it to the latest version. You can update this (and all) R packages by clicking on the Packages tab in the bottom right window of RStudio, then Update. Load blogdown by typing library(blogdown) into the R console (you need to do this every time you open/re-load R/RStudio). Under File, choose New Project, New Directory, Website using blogdown. On the following screen, specify a meaningful directory name (e.g. YOURNAMEWEBSITE), place it somewhere on your computer, check all boxes. In the Hugo theme box, enter wowchemy/starter-academic.  The figure below shows the screen and choices you should make. Adjust for your own setup.\nNow click Create Project. This creates a new website and installs and sets up the chosen theme, complete with a small sample site. Be patient, it might take some time. As long as you see a little red stop sign in the top left corner of the R Console window, you know things are still running. Once everything is done, you will see a lot of new files have shown up in your project/repository folder.\nThe command above installed a small example website. We can build the site and look at it. First, make sure blogdown has been loaded with library(blogdown). The build the site by typing build_site() into the R console. Follow that with serve_site() to see a live preview of the site. If everything works, the bottom right window, which is the RStudio viewer, shows your new website.\nClick on the symbol next to the broom in the top left corner of that window to see the website in your browser. This is your new site. Now it\u0026rsquo;s time to customize and create content.\nA word about themes Pretty much all modern websites follow the concept of splitting content from layout and only merging them at some point in the process of building a website. The formatting that gives a site a specific layout is often called a theme. The theme determines the look of your website. Hugo has a lot and quickly growing number of themes (though not as many as Wordpress). The idea is that you can switch themes and thus layout without too much difficulty. In practice, there is still often a good bit of adjustment needed when you go from one theme to another. Thus, it is worth spending some time deciding on a theme before you go \u0026ldquo;all in\u0026rdquo;.\nThe default theme hugo-lithium is a rather basic one. I don\u0026rsquo;t actually know too many websites that use it. A popular theme is Wowchemy (formerly called Academic). I use it for both my personal and research group sites. I\u0026rsquo;m using it for this tutorial too. The Wowchemy theme has a lot of features and customization options. But it has downsides. First, it is fairly complex. Second, it is currently changing rapidly, and those changes often break things. I hope that when you read this, it works. If not, let me know. There are other promising themes in the works, such as Hugo Prose and Hugo Apero, made by the same folks who are involved with the blogdown development. I wouldn\u0026rsquo;t be surprised if in the future a lot of folks, including myself, will switch to one of those themes.\nA brief explanation of your new website The previous steps have given you a website with some example starter content. Any complex website has a certain structure consisting of folders and files. These files and folders either live only on a website server and you never see them (e.g. Wordpress), or they exist locally, as is the case for Hugo (and similar such website generators, e.g. Jekyll.)\nSome of the files and folders you do not need to care much about, others you will need to interact with. To get a bit of an idea what lives where, here is a quick overview. Some of this depends on the Hugo theme you are using, but most of them have a rather similar structure. Open the directory where you placed your website, either through the Files pane in RStudio, or by using File Explorer (or whatever that is called on a Mac\\Linux üòÉ).\n The folder config contains files that control some of the overall website look and feel. We\u0026rsquo;ll visit them later. In Hugo, those files that control configuration are called either TOML or YAML files. The current trend for Hugo is to switch everything to YAML, but you\u0026rsquo;ll keep seeing a mix for a while. Many files have bits of TOML/YAML and other content in them, as you will see below. The folder content and its subfolders contain almost all the actual content for your website. We\u0026rsquo;ll edit some of that soon. The folder public is the one that contains your finished website once Hugo has processed your content and applied the layout from your chosen theme. That folder is the one which you will place online e.g. on Netlify. The folder static contains subfolders where you can store images and other media (e.g. pdf files or video/audio, etc) that you want to be part of your site. The folder themes includes the different Hugo themes you installed. You can have several themes installed and switch between them. Note however that themes differ in their details of how they do things, so it is a good idea to check out a few themes first, then settle on one and build your website for real once you picked one. The other folders that might exist are ones you don\u0026rsquo;t need to worry about, at least not initially. Some of those folders can contain your own formatting and layout choices overwriting the main theme. We won\u0026rsquo;t discuss them here but once you really want to start configuring your site, those will become important.  Editing content Now that you know a little bit how your website is structured, let\u0026rsquo;s dig in and add and modify it. We\u0026rsquo;ll start with some modifications, followed by creating new content.\nPlaying with widgets All (or at least most) content goes into the content folder and its subfolders. Content is generally written in (R)Markdown. For this tutorial, you don\u0026rsquo;t need to know much Rmarkdown, but at some point you will have to learn it. Fortunately, (R)Markdown is very easy to learn. See e.g. the RMarkdown section on this page, check out this nice interactive tutorial or this cheatsheet.\nThe Wowchemy/Academic theme, and many other modern websites, use a layout that employs widgets, which are components of a website that are individually formatted and styled. On the demo site you just created, you see many different sections, each is a widget. Which widgets you want is controlled by files in the /content/home/ folder. Go into that folder (from within RStudio) and open the demo.md file. You will see a bunch of text. Some commands are between +++ signs, this is called the TOML (or YAML if it\u0026rsquo;s 3 ---) header. These are instructions for the layout. The text below is what is actually shown on the site.\nAs you stare at the content of the file, you might recognize that it corresponds to the 2nd block of the demo website with the dark bubble content. Let\u0026rsquo;s say you don\u0026rsquo;t want this particular widget on your home page. The easiest way is to set active = false. (You can also delete the whole file if you are sure you don\u0026rsquo;t want it). Do that. You should notice 2 things. In the bottom-left RStudio window (the R console) you should see a bit of code showing that the website was re-built and updated in real time. If you re-load the page in your browser, the widget and its content should be gone. You can try turning off other sections of the main page using this approach.\nMaking things personal Now let\u0026rsquo;s open the about.md file. You will notice that it doesn\u0026rsquo;t really contain any content. Instead, it pulls the content from another location, namely content in the authors folder. Go into /content/authors/admin/ and open the _index.md file. There you see the content that is displayed on the main page. Modify it with your personal information. Once you save your changes, you should see the website automatically being rebuilt. If you have, add a picture of yourself and replace the current avatar.jpg file. (Your picture needs to have that name). Also, while not required, you might want to rename the folder from admin to your name. Make sure this corresponds to the name you list in the _index.md file.\nCleaning up for now Let\u0026rsquo;s turn off all other parts of the main site apart from the about widget. The easiest way is to remove all files apart from the index.md and about.md files. You probably don\u0026rsquo;t want to completely delete them (since you might want to use them later), thus I recommend you move them to some other folder on your computer. For instance you can make a folder called myfiles as a subfolder of your website folder and move the files into that folder.\nIf all of this worked, there should be a main page containing only a brief description of yourself.\nA word on error messages. If you make some changes that break the site, you will see an error message in the R console and the site won\u0026rsquo;t re-compile until you fix the problem. You often have to be careful to write things exactly as specified, and often with the right indentation, etc. Some fiddling is at times required. If you are stuck and think you broke it too badly, you can either look in the Wowchemy theme documentation or go into the themes/starter-academic/exampleSite folder and find the corresponding file you are editing there and see how it needs to look.\nEditing settings config.yaml I mentioned above that TOML/YAML is a language/structure used by Hugo to control all kinds of things. Most files have some TOML/YAML part, a few files are nothing but TOML and control a lot of settings. Let\u0026rsquo;s look at the most important files. The first one is config.yaml (sometimes also called config.toml) located in the main website folder. Find and open it. You will see that it lists as title Academic. Change that to e.g. Website of YOURNAME. You will see this change show up on the main site. You can try what happens if you write something in the copyright section. The rest of this file doesn\u0026rsquo;t need further editing for now.\nmenus.toml Let\u0026rsquo;s go into the config/_default/ folder and open the menus.toml file. You\u0026rsquo;ll see that those correspond to the menu buttons on the main page. Most of them don\u0026rsquo;t work since we removed the widgets. For now, let\u0026rsquo;s go ahead and disable (by placing # symbols in front) all entries apart from the Posts block of text.\nparams.toml Open params.toml. This file lets you specify and control a lot of things. Try setting a different theme. Then read through the other parts. We won\u0026rsquo;t change them for now, but you might want to come back to them.\nCreate new content Let\u0026rsquo;s create a blog post. First, let\u0026rsquo;s move the posts.md file back into the home folder. If successful, you should see several blog posts show up on the site. The actual files containing those posts are in the content/post folder. (Academic is unfortunately not very good about consistently using singular or plural). You will see one file (both .Rmd and .html) in the main /post folder. That file was placed there by RStudio/blogdown. You will also see several folders, each contain a .md file and one or more images. Those are placed there by the creator of Academic. Both ways of placing blog posts is ok but separate folders tends to be more organized. Let\u0026rsquo;s first remove the existing posts. You can either delete them, or to be safe, move everything (apart from the _index.md file) to the folder you created earlier. You should see all posts disappear from the main page.\nNow let\u0026rsquo;s create our first blog post. We can use an RStudio addin for this. Since I have found that sometimes the serve_site() command interferes with the addin functionality, let\u0026rsquo;s stop the serving (continuous building and displaying) of the website for now by restarting R (under Session).\nOnce you have restarted R, go to the Addins button (approximately under the Tools button) and choose New Post. Add a title for your blog post. You can leave all other settings as they are. At some point you will want to think more about the way you want to set up your website, including what kind of file format you want. See the More Information section below. For now, just give it a title and use the defaults for the rest.\nOnce you click done, the file should open. You will see that the things you specified in the boxes show up in the YANL area. Write some text below that area (below the ---). The text you write should be formatted as (R)Markdown. For now, you can just write some plain text. Or try some Markdown formatting using the cheat sheet listed above.\nOnce you finished writing your new blog post, save your changes, then start the server again by re-loading the blogdown package (library(blogdown)), then type serve_site() into the R console.\nMore edits Congratulations, you have built a website and wrote a blog post! Of course there is a lot you can do next. Write more posts, look at all the different elements/widgets you can turn on and modify, etc. As mentioned, the Academic theme has a lot of features. If you like what you see, continue exploring. If you think you want something simpler, check out other Hugo themes until you find one you like, then customize it. A lot of things are very similar across all Hugo themes (e.g. the TOML/YAML bits and the folder structure), but some details differ, so it\u0026rsquo;s good to pick a theme before you really start customizing it.\nBut for now, we\u0026rsquo;ll leave it at this. There is one more crucial step missing though.\nMaking your website public So far, everything happened on your local computer. In a final step, we want to put the website online so everyone can see it. With Netlify, this is fairly simple. Log into your Netlify account. Under Sites, you should see a white box that says \u0026ldquo;drag and drop your site folder here\u0026rdquo;. Take the folder named public inside your website folder and drag it into that box. If things work right, after a few seconds you should see that your site is deployed. It will have a weird name and a URL (mine when I tried it just now was called https://cranky-lamport-fff53a.netlify.com/). Click on it and you will see your new website publicly online for everyone to see!\nIf you want, you can change the name of your site under Site settings. More advanced (not free but also not too expensive and not that hard) is to get your domain name, e.g. www.yourname.com or something like that.\nIf you want to add GitHub to the mix and have the deployment of your website automated, move on to part 2 of this post.\nMore Information This post is just to get you up and running. If you like what you see and want to keep using blogdown/Hugo, you\u0026rsquo;ll likely need to learn more. There is a lot of good information on blogdown/Hugo/Academic available. I recommend the following:\n A short introductory course is Alison Hill\u0026rsquo;s summer of blogdown. Alison also has several blog posts on her website discussing Hugo and blogdown in a lot of detail, check out those materials. A lot of details and further information can be found in the blogdown book. For the Wowchemy theme, check out the docs for the theme. While they could be improved and at times a bit confusing, they are still a good source of information.  Further information and tutorials can easily be found with some online searching. Let me know if you find any especially helpful ones and I\u0026rsquo;ll add them here.\n","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"c13bdb169f66f761551bc3c121766997","permalink":"https://www.andreashandel.com/posts/blogdown-website-1/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/posts/blogdown-website-1/","section":"posts","summary":"The following are step-by-step instructions for creating your own website using [blogdown](https://bookdown.org/yihui/blogdown/), [Hugo](https://gohugo.io/) and [Netlify](https://www.netlify.com/).","tags":["R","blogdown","hugo","wowchemy"],"title":"Creating a website in less than 1 hour - part 1","type":"posts"},{"authors":null,"categories":["Teaching","Research"],"content":"I have been collecting lists of links related to research, teaching and (academic) career topics in a Github repository. I finally got around to doing some cleanup. I turned the repository into a small website. You can find this site here.\n","date":1578441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578441600,"objectID":"e78c8dc116e185e7a9c35db6c79c8ab6","permalink":"https://www.andreashandel.com/posts/resource-website/","publishdate":"2020-01-08T00:00:00Z","relpermalink":"/posts/resource-website/","section":"posts","summary":"I posted my lists with materials related to research and teaching as a website.","tags":["Teaching","Research"],"title":"Research and teaching resources website","type":"posts"},{"authors":["Andreas Handel"],"categories":["talk"],"content":"We described some of our past and ongoing work in using a combination of models and data to understand the role of inoculum dose on vaccination and infection outcomes. The presentation slides (as pdf) are here.\n","date":1571850000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571850000,"objectID":"1a371a261d1506834658ec80ff402db6","permalink":"https://www.andreashandel.com/presentations/2019-10-paris/","publishdate":"2019-10-23T17:00:00Z","relpermalink":"/presentations/2019-10-paris/","section":"presentations","summary":"Some recent research exploring the role of dose for infection and vaccination.","tags":["research","infectious disease"],"title":"Model-based optimization of vaccine inoculum dose","type":"presentations"},{"authors":null,"categories":[],"content":"\r\r\r\r\rOverview Our center for teaching and learning administered a mid-semester survey to the students in my fall 2019 online Modern Applied Data Analysis course. I figured it would make for a nice and topical exercise if I performed some analysis of the survey results. Students agreed to have the - fully anonymous - results posted publicly. This is my quick and simple text analysis.\nData loading Load and take a look.\ndata_raw \u0026lt;- read_xlsx(\u0026quot;Handel_CTL_Survey.xlsx\u0026quot;)\rd \u0026lt;- data_raw\rdim(d)\r ## [1] 15 11\r Some cleaning actions d \u0026lt;- d %\u0026gt;% clean_names() #clean column names, which are the full questions\rorig_quest \u0026lt;- data.frame(Number = paste0('Q',1:11), Question = names(d)) #save names and replace with simpler ones for now\rnames(d) = paste0('Q',1:11) #just call each column as Q1, Q2,... originallly asked question is stored in orig_quest\rkable(orig_quest) %\u0026gt;% kable_styling() #print them here for further reference  \r\rNumber \rQuestion \r\r\r\r\rQ1 \rwhats_working_well_in_this_class_what_are_the_strengths_of_the_class_and_which_aspects_are_having_a_positive_impact_on_your_learning \r\r\rQ2 \rwhat_aos_not_working_so_well_in_this_class_what_aspects_are_having_a_less_positive_impact_on_your_learning \r\r\rQ3 \rwhat_specific_changes_do_you_think_should_be_made_to_improve_your_experience_in_this_class \r\r\rQ4 \ri_think_the_pace_of_this_class_is \r\r\rQ5 \rare_there_specific_modules_that_should_be_adjusted_and_how_5 \r\r\rQ6 \rthe_quantity_of_material_covered_in_each_module_is \r\r\rQ7 \rare_there_specific_modules_that_should_be_adjusted_and_how_7 \r\r\rQ8 \rthe_level_of_difficult_of_each_module_is \r\r\rQ9 \rare_there_specific_modules_that_should_be_adjusted_and_how_9 \r\r\rQ10 \ron_average_i_spend_this_many_hours_per_week_doing_work_for_this_course \r\r\rQ11 \rfinally_what_is_your_gold_star_top_choice_number_one_recommendation_for_a_constructive_change_your_instructor_can_make_in_this_course \r\r\r\rMore cleaning\nvisdat::vis_dat(d) #missing values\r #looks like a few students left some entries blank. Should be ok. One student only answered 1 question. Quick look at entry.\rprint(d[12,2])\r ## # A tibble: 1 x 1\r## Q2 ## \u0026lt;chr\u0026gt; ## 1 Elc system not work well for online class\r #ok, not too useful (though I agree with the statement). Let's remove that student/observation.\rd\u0026lt;- d[-12,]\r# most questions were free text, but some were specific choices, so should be grouped as factor.\rd \u0026lt;- d %\u0026gt;% dplyr::mutate_at(c(\u0026quot;Q4\u0026quot;, \u0026quot;Q6\u0026quot;,\u0026quot;Q8\u0026quot;), factor)\r#Q10 is number, should be numeric but was text field so different entries exist\r#small enough to print here\rprint(d$Q10)\r ## [1] \u0026quot;20\u0026quot; \u0026quot;15-20\u0026quot; \u0026quot;15 or more\u0026quot; \u0026quot;15\u0026quot; ## [5] \u0026quot;14-16\u0026quot; \u0026quot;12\u0026quot; \u0026quot;15\u0026quot; \u0026quot;10\u0026quot; ## [9] \u0026quot;30\u0026quot; \u0026quot;20\u0026quot; \u0026quot;10 to 12 hours\u0026quot; \u0026quot;\u0026gt;10 hours\u0026quot; ## [13] \u0026quot;2\u0026quot; \u0026quot;20\u0026quot;\r #ok, this is kinda bad style, but the dataset is so small that it's easiest to replace the non-numeric values by hand. I'll set them to their mean or the specified limit.\rd$Q10[c(2,3,5,11,12)] \u0026lt;- c(17.5,15,15,11,10)\rd$Q10 \u0026lt;- as.numeric(d$Q10)\rprint(d$Q10)\r ## [1] 20.0 17.5 15.0 15.0 15.0 12.0 15.0 10.0 30.0 20.0 11.0 10.0 2.0 20.0\r Drawing first conclusions kable(orig_quest[c(4,6,8),]) %\u0026gt;% kable_styling()\r \r\r\rNumber \rQuestion \r\r\r\r\r4 \rQ4 \ri_think_the_pace_of_this_class_is \r\r\r6 \rQ6 \rthe_quantity_of_material_covered_in_each_module_is \r\r\r8 \rQ8 \rthe_level_of_difficult_of_each_module_is \r\r\r\rd %\u0026gt;% dplyr::select(Q4, Q6, Q8) %\u0026gt;% summary()\r ## Q4 Q6 Q8 ## just right:10 right amount:4 just right :10 ## too fast : 4 too much :9 too difficult: 3 ## NA's :1 NA's : 1\r plot(1:14,d$Q10, ylab = 'Time spent per week')\rlines(1:14,rep(12,14))\r Based on answers to questions 4,6 and 8, the majority of students think the pace and level of difficulty of the course is right but the amount of material covered is too much. Based on answer to Q10, students spend more time than my target (12 hours, solid line). Even accounting for some \u0026ldquo;inflation factor\u0026rdquo; (people generally over-estimate the time they spend on tasks like these, counting all the other things they do at the same time e.g., texting/email/FB/drinknig coffee/\u0026hellip;), the overall amount seems too high, and it agrees with Q6 answers about too much material.\nFirst conclusion: Reduce weekly workload, probably best by reducing assigned reading (see text answers which I already glimpsed at üòÉ).\nManual text analysis #dropping the question/variables analyzed above\rd \u0026lt;- d %\u0026gt;% dplyr::select( -c(\u0026quot;Q4\u0026quot;, \u0026quot;Q6\u0026quot;, \u0026quot;Q8\u0026quot;, \u0026quot;Q10\u0026quot;) )\r Questions 5, 7 and 9 ask how modules should be adjusted regarding pace, quantity and difficulty, so it\u0026rsquo;s worth looking at those questions on their own.\nd2 \u0026lt;- d %\u0026gt;% dplyr::select( Q5, Q7, Q9)\ris.na(d2) #some students didn't write anything for any of those questions, remove before printing content.\r ## Q5 Q7 Q9\r## [1,] FALSE TRUE FALSE\r## [2,] FALSE FALSE FALSE\r## [3,] TRUE TRUE TRUE\r## [4,] FALSE FALSE FALSE\r## [5,] FALSE FALSE FALSE\r## [6,] TRUE TRUE TRUE\r## [7,] FALSE FALSE FALSE\r## [8,] TRUE TRUE TRUE\r## [9,] TRUE TRUE TRUE\r## [10,] TRUE TRUE TRUE\r## [11,] FALSE FALSE FALSE\r## [12,] TRUE TRUE TRUE\r## [13,] FALSE TRUE TRUE\r## [14,] TRUE FALSE TRUE\r d2 \u0026lt;- d2[which(rowSums(is.na(d2)) != 3),] #remove all rows/obersvations that have NA to all 3 questions\rnames(d2) \u0026lt;- c('too fast','too much','too hard')\rknitr::kable(d2) %\u0026gt;% kable_styling() ##show rest\r \r\rtoo fast \rtoo much \rtoo hard \r\r\r\r\rAll of them are to long except for module 2. \rNA \rThe quizzes are becoming ridiculously difficult. \r\r\rNone, I have enjoyed the pace. \rNone, I think the content is appropriate. \rThe toughest module thus far in the course was the strings module. Though it should be noted, I think the content was very appropriate to cover the concepts of this module as they are quite difficult. I really benefited from completing some of the Regex crosswords while working through this chapter. \r\r\rI think all modules need to be adjusted. \rConsidering we have only done the first half of the class and the shortest amount of time I have spent in one week on this class was the first module and I still did more than 9 hours of work for what should have been a half week seems unreasonable. For all other modules I spend over 12 hours each week on this class sometimes upwards of 25 hours in a single week. \rI don't think the modules themselves are difficult just the content is being squeezed in and the time it takes doesn't correlate to the amount I feel like I'm learning, where I should be learning/proficient in much more than I currently am based on the amount of time spent. \r\r\rThe visualization module is pretty hefty in terms of how much time is spent. But it's all the same topic so I don't know how that would be split up. \rsee above \rThe modules are just difficult enough that the first couple times I try I struggle and it's hard, but after giving it a go it becomes much easier and I get it. So its enjoyably difficult. \r\r\rI feel like I like the order and pace of the course materials. We can learn all of the material in the module in one week - it's just exhausting to do so. \rTrim the reading in the modules and add more exploratory exercises. \rI feel like the modules scale up in difficulty each week, but proportionally to the growth in our skills. I feel like I am really learning R for the first time! (After three other courses in R...) \r\r\rIn the tidyverse module, we used ggplot, but then we actually learned about how to do ggplot which would have been helpful before tidyverse. The R module was definitely a lot of material and the coding exercise was a steep learning curve for the first true coding exercise of the course. I understand sometimes the best way to learn something is struggling through, but I think it can be difficult for new students to make the sudden leap from follow a specific script in a book chapter to make up your own code. Maybe something as in altering existing scripts first? \rWhile now there is a distinction on what is going to be on the quiz, there are multiple chapters and tutorials I haven‚Äö√Ñ√¥t read through yet. I‚Äö√Ñ√¥m actively searching to see what to prioritize first in a module. Module 4/5 also had a lot of (useful) material to work through. \rI think working through the material for the quizzes themselves is just the right difficulty but the assessments range from not so difficult (Tidyverse) to spending a couple of hours googling and troubleshooting (first R and Visualization). While you said to spend no more than 1 or 2 hours on Visualization, it still took me 4 hours to figure out. \r\r\rNot that I see so far. \rNA \rNA \r\r\rNA \rMost modules waver between just the right amount and a little too much. However, this goes hand in hand with what I have identified also as a strength of the course (great curation of resources), so perhaps adjustment isn't necessary... maybe it would be useful to have a more organized list of required and optional readings. \rNA \r\r\r\rConclusions from anwers to those questions: Overall too much material (see above), level of difficulty overall ok but too fast/crowded. Again, solution is to reduce (required) material.\nNext, let\u0026rsquo;s look at \u0026ldquo;whats working/not working\u0026rdquo; questions.\nd2 \u0026lt;- d %\u0026gt;% dplyr::select( Q1, Q2)\rnames(d2) \u0026lt;- c('good','bad')\rknitr::kable(d2) %\u0026gt;% kable_styling()  \r\rgood \rbad \r\r\r\r\rThe positive impact would be that I am learning completely new things I have had no exposure too. \rThis class is extremely overwhelming. It is online and the professor is the worst time estimator I have ever seen. He assigns way to much with absolutely no emphasis on what is important. As he is the ‚Äö√Ñ√∫expert‚Äö√Ñ√π in this subject he should be able to narrow it down and make this course more reasonable. I should not be spending 20-25 hours a week on one course that is 3 credits. \r\r\rI really enjoy the exercises assigned for each module. I am receiving a good amount of background information from the modules/reading, however I feel I really start to understand the material once I have used it in practice. Additionally, a good portion of the exercises thus far are directly applicable to the analysis I will be using for my personal research which is excellent practice.\rI also really appreciate that the structure of the course. I really struggled with the basic work flow of R scripts and GitHub initially as I am newer to the program. However, the progressive flow that this course has created has allowed me to learn the process step-by-step and frequently connected back to information that was previously used in an earlier module. As a result, I have become very comfortable with the basic usage/flow of R and I am excited to move into the more detailed functions of the program in the latter half of the semester! \nThe only small issue I have encountered thus far with this course is with the quiz time limits. I am sometimes having trouble sifting through all of my notes quickly enough to properly answer the quiz questions. I take approximately 10-20 pages of notes on each module (depending on the size and amount of exercises) and I can sometime struggle to locate the specific material in my notes within these time constraints. I absolutely understand the need for a time limit with an online class structure, however 20 minutes can be a bit tight for some of the quizzes. A simple bump from a 20-30 minute time range to 30-40 minute time range would be more than enough to address all questions adequately, while still ensuring students review the module content beforehand. \r\r\rI love the tidyverse package and thanks to instructions on R primer.\rI love R primer, because it raise the problem and then have a space to write code to try, and available solution. \nSome materials (R data science, IDS) are useful too. However, I do not like them. The reason is that they give me the knowledge, and explain some simple code. Then, I have exercise part with more complicated questions. I do not know how to do it sometimes and got stuck. One example is chapter about regular expressions.\rAnother thing is that this class took me so much time per week. It is one third of my week. I still have other three classes, and research duties.\nI expect that taking class is the quicker and better way to learn than learning by myself. This class is not what I expect. It is too time-consuming because I mainly have to learn by myself. \n\r\rThe R primers are very useful for understanding the material, the exercises you give us, and some of the exercises in the IDS book (however, they need to be narrowed down to what is actually important). \rHaving 5+ chapters of reading each week isn't useful. The readings in general aren't useful for coding, at least for me. They should supplement and be a reference for the actual coding we are doing but not be the entire basis of the quizzes. I spend so much time reading and I don't actually understand any of it unless I'm doing the coding. However, expecting to do every exercise that comes with some of the reading isn't working because I currently spend at least 12 hours every week just doing the exercises that go with readings and I end up not retaining much because I'm overwhelmed. \r\r\rI really enjoy having a structured way to learn r for data analysis instead of just learning it on my own. It makes it much more manageable and mentally-forgiving when there are other people learning/struggling at the same time as you. All of the data/resources are in one place and present in a timeline that makes it easier to understand and learn. I also really enjoyed how he makes us go back to the other student\u0026rsquo;s repositories and work with them. It helps foster the feeling/attitude that we\u0026rsquo;re classmates and get to know each other more even though it\u0026rsquo;s an online course. \nThe resources are a lot to work through. I don't have previous R experience but this class is taking a lot of my time to work through the R for data science book. \r\r\rThe amount of information available. Strengths of the class are being able to work with other students to complete assignments and get ideas from one another. \rThe amount of work we have to do every week, including reading, quizzes, and lengthy assignments. Someone new to coding might find this extremely overwhelming considering they have other classes to work on as well as their own laboratory work. The quizzes make me feel like I have not learned anything because they are very weird, specific questions that I have to spend a long time going back through the reading to hunt for. \r\r\rThe course website is very thorough. It is very clear that Dr. Handel has put a lot of thought and effort into building the materials and course for this class. The writing is very clear and accessible. The site and materials are very consistent, which is helpful. The RPrimers, IDS, and R4DS chapters are helpful (in that order, respectively). The class exercises are very helpful. I am finding myself coding more often and drafting unique codes, which was my goal from the start. This is the most well-rounded course in R I have taken.\nDr. Handel\u0026rsquo;s exercises are by far the most helpful. We get to think through the material and use resources as-needed. These are exercises I will need in my own analysis. I have been using these codes as guidelines for working through my own data sets.\nI love the R-Primers as an introduction, and then the Exercises. I think we could take a stab at the exercises without the IDS and R4DS readings, or have those readings embedded into the exercise.\nSuch as\u0026hellip;\u0026ldquo;Try to make this figure\u0026rdquo; If you are stuck, Chapter 4.5 might be a good a reference. \nI'm lucky in that there are a five or six students in my department all taking this course. Since we see each other regularly, we can help each other along and make sense of the material. I can't imagine working through this course alone, with no face-to-face interaction. The group dynamic helps me read into what is important and helps trouble-shoot when there are problems. The eLC discussion board is not great for connecting with others...\rOutside of these few people that I am lucky to see in person, I think it is hard to connect with the other students to gauge if I am \u0026ldquo;on the right track\u0026rdquo; or \u0026ldquo;falling behind\u0026rdquo; or plain bad at coding. We don\u0026rsquo;t engage much with other students, so it\u0026rsquo;s hard to tell if they\u0026rsquo;re succeeding or struggling as much as we are. I keep thinking - \u0026ldquo;am I the only one feeling this way?\u0026rdquo;\nThe modules seem to be thick/dense for just one week\u0026rsquo;s worth of work. As soon as I finish a module, I am exhausted by the material, but then have to start right back up again. I don\u0026rsquo;t feel like I have enough time to play around with all of the functions we just learned before it\u0026rsquo;s time to start learning new ones. I\u0026rsquo;d like more time to apply them to data sets in example exercises.\nI\u0026rsquo;ve been to a few meetings with my advisor now where I say that most of my week has been taken up with this course. Perhaps I need to work on my own time management and expectations for this course work. Of course, I do think the time investment will pay off! I am excited to start analyzing my own data set!\nTo get by, I feel like I have started skimming the readings, or reading them \u0026ldquo;diagonally\u0026rdquo; as some people may say. I have \u0026ldquo;CTRL-F\u0026rdquo; through the readings just for the quizzes. I don\u0026rsquo;t think this is ideal, and I\u0026rsquo;m sure Dr. Handel wouldn\u0026rsquo;t want us to resort to this. \n\r\rI like the exercises in R. While my lack of previous experience means I have to spend more time trying to understand how to put complex displays together, what I learn from this stays with me a lot better. \rI know that I review the material quite a bit for the quizzes yet continue to be surprised at what actually shows up on the quiz. This may be a personal problem as the class distribution shows that other individuals are doing well. However, I know that it continues to be frustrating. Also, the MADA course on GitHub is still a bit difficult to navigate and the info given for each module is a lot to absorb without there being an application element. \r\r\rThe supplementary readings are useful, but that's about all that's having a positive impact on my learning in this class. \rThe class being online is convenient, but if the class were in person and I could see step by step instructions, and everything would make more sense. Yes I understand you learn by making mistakes but taking 20+ hours every single time I do an assignment or reading is absolutely ridiculous due to the fact that all of my instructions are typed out, and there's no \"teaching\" in my opinion. This class structure is not conducive to my learning style. There aren't even any videos of the professor lecturing and walking us through lessons. I'm not absorbing any material by reading over 50+ pages then being thrown an assignment and an overly detailed quiz that doesn't even focus on main points but extremely detailed findings that DO NOT showcase what I've learned. \r\r\rThe class is quite well structured with nice use of different video resources. \rSome of the assignments are very much like busy work and are time consuming without being particularly helpful and just end up putting one under pressure during the week whenever there are so many other draws on ones time at graduate school. \r\r\rI do like the material we cover since it‚Äö√Ñ√¥s mostly relevant, I feel like I‚Äö√Ñ√¥m learning more of the ins and outs of R each week! I think the set up of the class with readings then an assessment works well. \rThere is a lot of material to cover each week. While I enjoy it and I‚Äö√Ñ√¥m interested, it is all mostly new to me and I feel like I have to rush through the readings to do the quiz by Friday. \r\r\rThis class requires a lot of coding, and I had no previous experience with coding, but we were provided with a lot of resources and exercises to start from the beginning. \rThere are too much reading assignments. It really takes a lot of time for this class. I cold barely finish these assignments every week, not alone to digest them and use them. \r\r\rI think this course is set up very well. The modules are constructed in a manner that makes the content accessible and the exercises are well guided. \rI would say that nothing related to the class is having a negative impact on my learning. \r\r\rWe receive a ton of excellent resources. Dr. Handel is very responsive to my feedback, which I greatly appreciate. I was very apprehensive about taking an online course, but my fears have been completely assuaged. This class has already been incredibly beneficial to me. \rAs I've already shared with Dr. Handel, the online quizzes are sometimes frustrating in that we are only asked 6 to 7 very specific questions on a great deal of material. I don't feel we're given enough time to complete it, and partial credit isn't awarded. However, as I mentioned above, Dr. Handel is very receptive to feedback and I have already spoken with him about my concerns with this issues. \r\r\r\rConclusions from anwers to those questions: Overall too much material, especially too much reading. R primers are good. Other resources are hit or miss. Quizzes are not working, need to be ditched or altered. Maybe more exercises. Find better alternative to eLC.\nFinally, the 2 remaining questions are about improvements, phrased in 2 different ways. Let\u0026rsquo;s look at them together.\nd2 \u0026lt;- d %\u0026gt;% dplyr::select( Q3, Q11)\rd2 \u0026lt;- d2[-12,] #this student didn't provide answers to either question\rnames(d2) \u0026lt;- c('specific suggested changes','number one recommendation')\rknitr::kable(d2) %\u0026gt;% kable_styling()  \r\rspecific suggested changes \rnumber one recommendation \r\r\r\r\rLess information per module. Most of these modules should be split into two weeks. \rYou need to put yourself in the eyes of a graduate student who is new to R and has a minimal stats background. As a graduate student in general public health field, what are the most important things to prepare is for publishing papers, understanding data, etc. We should have spent two weeks on data wrangling and two weeks on data visualization. These are EXTREMELY important topics, and they were rushed through in one week. I do not think my understanding of how to do these things is good, and instead we are moving onto other topics that are less important. \r\r\rAs mentioned above, just a bit more time on the quizzes would be great! \rA small increase in the time allotted to take the weekly quizzes. \r\r\rI suggest that you can give the solution for this chapter ahead. We can learn through solution if we get stuck.\rTo reduce time to learn by myself, I suggest that professor can create R scripts with coding and explain what is the purpose of the code by text or better having a video to show how the code works in purpose.I think that this way will be better than the way that students have to go through many chapters of reading. If going through many chapters, students have to learn the new things and be able to finalize the knowledge at the same. This process is hard. So, I suggest that professor finalize the knowledge, then show it to us. You can indicate book in case they are still confused or do not understand, and need to read further. \nFor the knowledge that I learn through the R primer, I am happy with them. However, for other chapters that have to read through R4DS, and IDS, it is too time consuming and not an efficient way to learn. If you can have a better resources for the knowledge covered in R4DS and IDS, I am happy with this course. \r\r\rMore homework assignments that mimic the material that is in the readings, where we can use the readings as a reference/starting point. I feel like if we replace the quizzes/readings with an assignment that goes over what is generally important from the chapters you assign it would be more worthwhile. \rNo quizzes - replace with short assignments that reinforce the material presented in the reading each week. Still makes it so you have to read but not that you have to spend hours upon hours doings so. Also as a side note - maybe introduce folders in the class github so that its organized by assignment so we can easily find things. \n\r\rSpecifically not much. \rEstablishing a working discussion board from the beginning would probably be the best thing. \r\r\rI think that there should be more time allotted for the quizzes, simply because of the complexity of the questions as well as the vast amount of reading required. \rBe a little more lenient on quizzes and giving out massive assignments. For an online course in coding, you're asking a lot from us. \r\r\rIt might be helpful to have \"coding drop in sessions\" where students can meet in a room, bring snacks, and discuss the course as a group. It would be helpful if each modules was spread out over two weeks. It would be helpful to have more of Dr. Handel\u0026rsquo;s exercises (not necessarily the IDS R4DS ones). \nTrim the modules \r\r\rI know that I do significantly better on the exercise portions of the class as I can see where the material is applicable but, again, this may be a personal preference. One blanket recommendation would be improving the navigational ability of the GitHub course site and breaking down the sections within each module to highlight where outside links need to be used. \rNA \r\r\rIf this class is going to remain online I think the professor should record himself lecturing and have a split screen showing him using R instead of reading everything to get instructions. I think the supplementary reading/interactive learnings are helpful, but they should be in addition to teaching, not the only mode of learning. At this point I don't even feel like my professor is teaching me, a website and book are (barely). \rEither change back to in person instruction or have video lectures with a computer split screen to show examples, THEN supplement with readings and interactive learning. The entire course just can't be a written instruction list and a website with definitions. \r\r\rMake the assignments more relevant rather than busy work or at least have a required part and then then optional parts because to have to do a time consuming assignment each week isn't beneficial \rNA \r\r\rI liked the quiz this week (Module 7) because you specifically said that the exercises were optional in the reading so it did take the pressure off of having to go through multiple chapters. If the purpose of the quizzes are to make sure we are reading the material, then it was nice to focus on the content of the chapters. (And not lose 30 minutes to solving one exercise) \rexercises from chapters not be covered on quizzes. :) \r\r\rAgain, the class is great. The only improvements are needed by me. \rI do think the tidyverse is so massive an undertaking for a beginner that it could be split into two modules. \r\r\rNothing in particular comes to mind at the moment, as I've felt very comfortable voicing recommendations to Dr. Handel as they arise :) \rAlternative way of testing for our reading each week? As of right now, the quizzes are precarious (one wrong answer can doom a grade) and time-constrained, and a pretty significant source of stress for me. However, as mentioned earlier, Dr. Handel and I are in communication about this! \r\r\r\rConlusions from these answers: Reduce content per module (or alternatively increase time). Adjust or drop quizzes. More exercises. Record some lectures or provide links to recordings.\nAutomated text analysis So this is likely not too useful, but I wanted to play around with some automated text analysis. Maybe the computer can figure out things I can\u0026rsquo;t?\nI don\u0026rsquo;t actually know how to do text analysis, so I\u0026rsquo;ll have to peek at the tidytext tutorial. Getting some ideas from this tutorial and the Text Mining with R book.\nTurn all answers into a long dataframe of words\nd2 \u0026lt;- d %\u0026gt;% tidyr::pivot_longer(cols = starts_with('Q'), names_to =\u0026quot;question\u0026quot;, values_to = \u0026quot;answers\u0026quot;) %\u0026gt;% drop_na() %\u0026gt;%\runnest_tokens(word, answers, token = \u0026quot;words\u0026quot;)\r Look at most frequent words.\nd2 %\u0026gt;% count(word, sort = TRUE)  ## # A tibble: 835 x 2\r## word n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 the 215\r## 2 i 134\r## 3 to 126\r## 4 and 90\r## 5 of 81\r## 6 a 75\r## 7 is 59\r## 8 in 57\r## 9 that 48\r## 10 are 47\r## # ... with 825 more rows\r The usual words are the most frequent.\nSentiment analysis Sentiment analysis, look at most frequent positive and negative words.\nbing \u0026lt;- get_sentiments(\u0026quot;bing\u0026quot;)\rpositive \u0026lt;- bing %\u0026gt;% filter(sentiment == \u0026quot;positive\u0026quot;)\rd2 %\u0026gt;% semi_join(positive) %\u0026gt;% nrow()\r ## [1] 158\r negative \u0026lt;- get_sentiments(\u0026quot;bing\u0026quot;) %\u0026gt;% filter(sentiment == \u0026quot;negative\u0026quot;)\rd2 %\u0026gt;% semi_join(negative) %\u0026gt;% nrow()\r ## [1] 72\r bing_word_counts \u0026lt;- d2 %\u0026gt;%\rinner_join(bing) %\u0026gt;%\rcount(word, sentiment, sort = TRUE) %\u0026gt;%\rmutate(n = ifelse(sentiment == \u0026quot;negative\u0026quot;, -n, n)) %\u0026gt;%\rmutate(word = reorder(word, n))  Plot positive and negative words.\nbing_word_counts %\u0026gt;% ggplot(aes(word, n, fill = sentiment)) +\rgeom_bar( stat = \u0026quot;identity\u0026quot;) +\rcoord_flip() +\rlabs(y = \u0026quot;Counts\u0026quot;)\r About twice as many positive as negative words, i guess that\u0026rsquo;s good üòÉ. And the most frequent negative words do reflect that things are \u0026ldquo;too much\u0026rdquo;.\nLet\u0026rsquo;s look at sentiment per question. Higher values are more positive.\nquestion_sentiment \u0026lt;- d2 %\u0026gt;%\rinner_join(bing) %\u0026gt;%\rcount(question, sentiment) %\u0026gt;%\rspread(sentiment, n, fill = 0) %\u0026gt;%\rmutate(sentiment = positive - negative)\rggplot(question_sentiment, aes(question, sentiment)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE)  Not surprising, the 1st question \u0026ldquo;what is working well\u0026rdquo; has lots of positive. Surprisingly, question 2, \u0026ldquo;what\u0026rsquo;s not working well\u0026rdquo; has fairly high positive sentiment. One problem could be that what I\u0026rsquo;m plotting is total counts, but I should probably normalize by total words written per question. Let\u0026rsquo;s try:\nwords_per_q \u0026lt;- d2 %\u0026gt;% group_by(question) %\u0026gt;% count()\rprint(words_per_q)\r ## # A tibble: 7 x 2\r## # Groups: question [7]\r## question n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 Q1 799\r## 2 Q11 393\r## 3 Q2 1304\r## 4 Q3 582\r## 5 Q5 193\r## 6 Q7 202\r## 7 Q9 240\r Yep, looks like most words were written by far for Q2. Maybe not a good sign? But maybe ok, since this specifically solicited feedback on all aspects. So let\u0026rsquo;s replot sentiment, normalized by number of words.\nquestion_sentiment \u0026lt;- question_sentiment %\u0026gt;% mutate(sent_per_word = sentiment / words_per_q$n)\rggplot(question_sentiment, aes(question, sent_per_word)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE)  Ok, changed things a bit but not a lot. Q2 drop (expected) is most noticable change. Still, even for the \u0026ldquo;what\u0026rsquo;s not good\u0026rdquo; section, positive words dominate. That either means the course is quite good, or students are very optimistic or polite, or it might mean nothing at all.\nWordclouds Why not? Everyone loves a wordcloud, even if they are just fun to look at, right?\nd2 %\u0026gt;%\rinner_join(bing) %\u0026gt;%\rcount(word, sentiment, sort = TRUE) %\u0026gt;%\racast(word ~ sentiment, value.var = \u0026quot;n\u0026quot;, fill = 0) %\u0026gt;%\rwordcloud::comparison.cloud(colors = c(\u0026quot;#F8766D\u0026quot;, \u0026quot;#00BFC4\u0026quot;),\rmax.words = 100)\r At this point, I ran out of ideas for further text analysis. I didn\u0026rsquo;t think analysis by word pairs, or sentences, or such alternatives would lead to any further interesting results. I looked in the Text Mining with R book for some more ideas of what kind of analyses might be useful, but can\u0026rsquo;t come up with anything else. Not that the above ones are that useful either, but it was fun to try some text analysis, which is a type of data analysis I\u0026rsquo;m not very familiar with. So, I\u0026rsquo;ll stop this here. Feel free to play around yourself, you have access to the raw data and this script in the GitHub repository.\nOverall conclusions and my commentary There seem to be some clear themes to me, I\u0026rsquo;ll list them here and add my thoughts:\n1. Less material per week/module: I\u0026rsquo;ll adjust reading and will move almost all external sources to the optional category, only require certain readings if I consider them essential. In a future version of the course, I\u0026rsquo;ll adjust all modules accordingly.\n2. Change quizzes: There is a good bit of evidence showing that testing/quizzing helps learning. But I think the way I did it is covering too much per quiz, which didn\u0026rsquo;t work. Ideally, I wanted to build more learnr based content (like the one in module 10 and the R primers) which allow simple quizzes embedded (though currently no easy way of grading). I just didn\u0026rsquo;t have time to make a lot of those for this class. For the rest of the course, the quizzes I already created (up to M12) will be there, but only cover a limited amount of material, I won\u0026rsquo;t do any quizzes for the rest of the course.\n3. More exercises: I had already planned a good bit of exercises for the rest of the class, now I\u0026rsquo;ll certainly make sure to focus on those. For future courses, I\u0026rsquo;ll add more of them.\n4. Lecture recordings: I\u0026rsquo;m not too interested in recording myself going through things. It seems to me there are already 100s of such videos out there. I\u0026rsquo;ll try to find some good ones and place them in the Resources section. Suggestions appreciated.\nFurther thoughts: The theme of \u0026ldquo;too much\u0026rdquo; is clear. There are 2 main ways one can go. Reduce the amount of material covered, or increase time spent. The first will increase the survey nature of this course. I currently intend this to be a \u0026lsquo;broad but shallow\u0026rsquo; survey course, the idea is to go through everything in one semester. Which is quite ambitious. At some point, I\u0026rsquo;m not sure if that works anymore or if the course becomes too superficial. The other option - and that\u0026rsquo;s common in other places that teach this material - is to split the course into separate ones. E.g. 1 semester/course only on R/Coding, 1 course on wrangling/visualization, 1 course on machine learning/model fitting. Or even further splits (e.g. the Coursera Data Analysis concentration has I think 9 courses.) That would allow more depth per course, but woudn\u0026rsquo;t allow students to get everything in a single course. I\u0026rsquo;m currently leaning toward sticking with the survey/intro/everything course and just further reduce materials. But I\u0026rsquo;d love to hear everyone\u0026rsquo;s opinion.\nAlso further note: In the future, once our EPID 7500 (R Coding) course is fully up and running and regularly offered, that course will likely become a pre-requisite for MADA. In that sense, a bit of sequencing will be introduced.\n","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"d8ca62caa51b0d35776fc7698b3cd850","permalink":"https://www.andreashandel.com/posts/mada-survey-analysis/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/posts/mada-survey-analysis/","section":"posts","summary":"Some simple data analysis applied to a mid-semester student survey of my [Modern Applied Data Analysis course.](https://andreashandel.github.io/MADAcourse/)","tags":[],"title":"Text analysis of a mid-semester course survey","type":"posts"},{"authors":null,"categories":["R","Data Analysis"],"content":"\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rThis analysis was performed as part of an exercise for my Modern Applied Data Analysis course.\nWhen I taught the course in fall 2019, one of the weekly assignments for the students was to participate in TidyTuesday. I did the exercise as well, this is my product. You can get the R Markdown file to re-run the analysis here.\nIntroduction If you are not familiar with TidyTuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week‚Äôs data was all about Pizza. More on the data is here.\nLoading packages library('readr')\rlibrary('ggplot2')\rlibrary(\u0026quot;dplyr\u0026quot;)\rlibrary(\u0026quot;cowplot\u0026quot;)\rlibrary(\u0026quot;plotly\u0026quot;)\rlibrary(\u0026quot;forcats\u0026quot;)\rlibrary(\u0026quot;geosphere\u0026quot;)\r Data loading Load date following TidyTueday instructions.\npizza_jared \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv\u0026quot;)\rpizza_barstool \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv\u0026quot;)\rpizza_datafiniti \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv\u0026quot;)\r Analysis Ideas See the TidyTuesday website for a codebook. These are 3 datasets. Looks like the 1st dataset is ratings of pizza places through some (online?) survey/poll, the 2nd dataset again has ratings of pizza places from various sources, and the 3rd dataset seems to have fairly overlapping information to the 2nd dataset.\nNote: When I looked at the website, the codebook for the 3rd dataset seemed mislabeled. Might be fixed by now.\nPossibly interesting questions I can think of:\n For a given pizza restaurant, how do the different ratings/scores agree or differ? Are more expensive restaurants overall rated higher? Is there some systematic dependence of rating on location? Do restaurants located in a certain area in general get rated higher/lower compared to others?  I think those are good enough questions to figure out, let‚Äôs see how far we get.\nInitial data exploration Start with a quick renaming and general check.\n#saves typing\rd1 \u0026lt;- pizza_jared d2 \u0026lt;- pizza_barstool d3 \u0026lt;- pizza_datafiniti glimpse(d1)\r ## Rows: 375\r## Columns: 9\r## $ polla_qid \u0026lt;dbl\u0026gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5~\r## $ answer \u0026lt;chr\u0026gt; \u0026quot;Excellent\u0026quot;, \u0026quot;Good\u0026quot;, \u0026quot;Average\u0026quot;, \u0026quot;Poor\u0026quot;, \u0026quot;Never Again\u0026quot;, \u0026quot;Ex~\r## $ votes \u0026lt;dbl\u0026gt; 0, 6, 4, 1, 2, 1, 1, 3, 1, 1, 4, 2, 1, 1, 0, 1, 1, 0, 3, 0~\r## $ pollq_id \u0026lt;dbl\u0026gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5~\r## $ question \u0026lt;chr\u0026gt; \u0026quot;How was Pizza Mercato?\u0026quot;, \u0026quot;How was Pizza Mercato?\u0026quot;, \u0026quot;How w~\r## $ place \u0026lt;chr\u0026gt; \u0026quot;Pizza Mercato\u0026quot;, \u0026quot;Pizza Mercato\u0026quot;, \u0026quot;Pizza Mercato\u0026quot;, \u0026quot;Pizza ~\r## $ time \u0026lt;dbl\u0026gt; 1344361527, 1344361527, 1344361527, 1344361527, 1344361527~\r## $ total_votes \u0026lt;dbl\u0026gt; 13, 13, 13, 13, 13, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 5, 5, 5,~\r## $ percent \u0026lt;dbl\u0026gt; 0.0000, 0.4615, 0.3077, 0.0769, 0.1538, 0.1429, 0.1429, 0.~\r glimpse(d2)\r ## Rows: 463\r## Columns: 22\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Pugsley's Pizza\u0026quot;, \u0026quot;Williamsburg ~\r## $ address1 \u0026lt;chr\u0026gt; \u0026quot;590 E 191st St\u0026quot;, \u0026quot;265 Union Ave\u0026quot;~\r## $ city \u0026lt;chr\u0026gt; \u0026quot;Bronx\u0026quot;, \u0026quot;Brooklyn\u0026quot;, \u0026quot;New York\u0026quot;, ~\r## $ zip \u0026lt;dbl\u0026gt; 10458, 11211, 10017, 10036, 10003~\r## $ country \u0026lt;chr\u0026gt; \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US~\r## $ latitude \u0026lt;dbl\u0026gt; 40.85877, 40.70808, 40.75370, 40.~\r## $ longitude \u0026lt;dbl\u0026gt; -73.88484, -73.95090, -73.97411, ~\r## $ price_level \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, ~\r## $ provider_rating \u0026lt;dbl\u0026gt; 4.5, 3.0, 4.0, 4.0, 3.0, 3.5, 3.0~\r## $ provider_review_count \u0026lt;dbl\u0026gt; 121, 281, 118, 1055, 143, 28, 95,~\r## $ review_stats_all_average_score \u0026lt;dbl\u0026gt; 8.011111, 7.774074, 5.666667, 5.6~\r## $ review_stats_all_count \u0026lt;dbl\u0026gt; 27, 27, 9, 2, 1, 4, 5, 17, 14, 6,~\r## $ review_stats_all_total_score \u0026lt;dbl\u0026gt; 216.3, 209.9, 51.0, 11.2, 7.1, 16~\r## $ review_stats_community_average_score \u0026lt;dbl\u0026gt; 7.992000, 7.742308, 5.762500, 0.0~\r## $ review_stats_community_count \u0026lt;dbl\u0026gt; 25, 26, 8, 0, 0, 3, 4, 16, 13, 4,~\r## $ review_stats_community_total_score \u0026lt;dbl\u0026gt; 199.8, 201.3, 46.1, 0.0, 0.0, 13.~\r## $ review_stats_critic_average_score \u0026lt;dbl\u0026gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0~\r## $ review_stats_critic_count \u0026lt;dbl\u0026gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ~\r## $ review_stats_critic_total_score \u0026lt;dbl\u0026gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0~\r## $ review_stats_dave_average_score \u0026lt;dbl\u0026gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1~\r## $ review_stats_dave_count \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r## $ review_stats_dave_total_score \u0026lt;dbl\u0026gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1~\r glimpse(d3)\r ## Rows: 10,000\r## Columns: 10\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Shotgun Dans Pizza\u0026quot;, \u0026quot;Sauce Pizza Wine\u0026quot;, \u0026quot;Mios Pizzer~\r## $ address \u0026lt;chr\u0026gt; \u0026quot;4203 E Kiehl Ave\u0026quot;, \u0026quot;25 E Camelback Rd\u0026quot;, \u0026quot;3703 Paxton ~\r## $ city \u0026lt;chr\u0026gt; \u0026quot;Sherwood\u0026quot;, \u0026quot;Phoenix\u0026quot;, \u0026quot;Cincinnati\u0026quot;, \u0026quot;Madison Heights\u0026quot;~\r## $ country \u0026lt;chr\u0026gt; \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, \u0026quot;US\u0026quot;, ~\r## $ province \u0026lt;chr\u0026gt; \u0026quot;AR\u0026quot;, \u0026quot;AZ\u0026quot;, \u0026quot;OH\u0026quot;, \u0026quot;MI\u0026quot;, \u0026quot;MD\u0026quot;, \u0026quot;MD\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;FL\u0026quot;, ~\r## $ latitude \u0026lt;dbl\u0026gt; 34.83230, 33.50927, 39.14488, 42.51667, 39.28663, 39.2~\r## $ longitude \u0026lt;dbl\u0026gt; -92.18380, -112.07304, -84.43269, -83.10663, -76.56698~\r## $ categories \u0026lt;chr\u0026gt; \u0026quot;Pizza,Restaurant,American restaurants,Pizza Place,Res~\r## $ price_range_min \u0026lt;dbl\u0026gt; 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 25, 25, 25, 25, 0, 0, 0~\r## $ price_range_max \u0026lt;dbl\u0026gt; 25, 25, 25, 40, 25, 25, 25, 25, 25, 25, 40, 40, 40, 40~\r The first question I have is if the pizza places in the 3 datasets are the same or at least if there is decent overlap. If not, then one can‚Äôt combine the data.\nd1names = unique(d1$place)\rd2names = unique(d2$name)\rd3names = unique(d3$name)\rsum(d1names %in% d2names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\r ## [1] 22\r sum(d1names %in% d3names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\r ## [1] 9\r sum(d2names %in% d3names)\r ## [1] 66\r 22 restaurants out of 56 in dataset 1 are also in dataset 2. Only 9 overlap between dataset 1 and 3. 66 are shared between datasets 2 and 3.\nThe last dataset has no ratings, and if I look at the overlap of dataset 1 and 2, I only get a few observations. So I think for now I‚Äôll focus on dataset 2 and see if I can address the 3 questions I posed above with just that dataset. Maybe I‚Äôll have ideas for the other 2 datasets as I go along (would be a shame to not use them.)\nRatings agreement analysis Ok, I‚Äôll focus on dataset 2 now and look closer at the scores/rating. From the codebook, it‚Äôs not quite clear to me what the different scores and counts in dataset 2 actually mean, so let‚Äôs look closer to try and figure that out.\nFrom the glimpse function above, I can‚Äôt see much of a difference between average and total score. Let‚Äôs look at that. Here are a few plots comparing the different score-related variables.\nplot(d2$review_stats_community_total_score,d2$review_stats_community_average_score)\r plot(d2$review_stats_community_total_score - d2$review_stats_community_average_score* d2$review_stats_community_count)\r plot(d2$review_stats_critic_total_score-d2$review_stats_critic_average_score)\r plot(d2$review_stats_dave_total_score-d2$review_stats_dave_average_score)\r plot(d2$review_stats_all_total_score- (d2$review_stats_community_total_score+d2$review_stats_critic_total_score+d2$review_stats_dave_total_score))  Ok, so based on the plots above, and a few other things I tried, it seems that average score is total score divided by number of counts, and the all score is just the sum of dave, critic and community.\nSo to address my first question, I‚Äôll look at correlations between average scores for the 3 types of reviewers, namely dave, critic and community.\nHowever, while playing around with the data in the last section, I noticed a problem. Look at the counts for say critics and the average score.\ntable(d2$review_stats_critic_count)\r ## ## 0 1 5 ## 401 61 1\r table(d2$review_stats_critic_average_score)\r ## ## 0 4 4.3 4.5 4.8 5 5.1 5.4 5.5 5.7 5.8 5.9 5.96 6 6.2 6.31 ## 401 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 ## 6.5 6.6 6.7 6.76 6.8 6.9 7 7.2 7.3 7.4 7.6 7.76 7.8 7.9 8 8.1 ## 3 1 1 1 2 1 5 2 2 1 1 1 2 1 4 2 ## 8.5 8.7 8.8 9 9.3 9.4 9.8 10 11 ## 3 1 1 1 1 2 1 4 1\r A lot of restaurants did not get reviewed by critics, and the score is coded as 0. That‚Äôs a problem since if we take averages and such, it will mess up things. This should really be counted as NA. So let‚Äôs create new average scores such that any restaurant with no visits/reviews gets an NA as score.\nd2 \u0026lt;- d2 %\u0026gt;% mutate( comm_score = ifelse(review_stats_community_count == 0 ,NA,review_stats_community_average_score)) %\u0026gt;%\rmutate( crit_score = ifelse(review_stats_critic_count == 0 ,NA,review_stats_critic_average_score)) %\u0026gt;%\rmutate( dave_score = ifelse(review_stats_dave_count == 0 ,NA,review_stats_dave_average_score))  Now let‚Äôs plot the 3.\np1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=comm_score, y = crit_score)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\rp2 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=comm_score, y = dave_score)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\rp3 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=crit_score, y = dave_score)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\r ## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r Looks like there is some agreement between Dave, the critics and the community on the ratings of various pizza places, though there is a good bit of variation.\nI think it would be fun to be able to click on specific points to see for a given score which restaurant that is. For instance I‚Äôm curious which restaurant has a close to zero score from both the community and Dave (bottom left of plot B).\nI think that can be done with plotly, let‚Äôs google it.\nOk, figured it out. This re-creates the 3 scatterplots from above and when one moves over the dots, it shows restaurant name.\nplot_ly(d2, x = ~comm_score, y = ~crit_score, text = ~paste('Restaurant: ', name))\r ## No trace type specified:\r## Based on info supplied, a 'scatter' trace seems appropriate.\r## Read more about this trace type -\u0026gt; https://plotly.com/r/reference/#scatter\r## No scatter mode specifed:\r## Setting the mode to markers\r## Read more about this attribute -\u0026gt; https://plotly.com/r/reference/#scatter-mode\r \r{\"x\":{\"visdat\":{\"71c04e903cb6\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"71c04e903cb6\",\"attrs\":{\"71c04e903cb6\":{\"x\":{},\"y\":{},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"comm_score\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"crit_score\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[7.992,6.725,6.5,6.55,5.9,6.8125,6.25,6.375,3,7.45,7.625,6.1,6.675,5.725,6.75,6.475,4.4,5.1,7.00392156862745,6.5,7.77142857142857,6.36428571428571,7.25,7.14285714285714,6.8,8.85862068965517,5.76666666666667,7.4,5.95,5.25,7.3,8.46,5.97272727272727,8.23333333333333,7.025,7.16,6.75,6.4,7.13684210526316,8.28571428571429,8.45,7.63333333333333,7.62307692307692,7.8,8.6,7.55555555555555,7.15,6.85,8.40504201680672,5.92,6.46666666666667,6.0625,7.3,8.225,5.87894736842105,7.2,6.9],\"y\":[8.8,6.2,4.5,5.8,8.7,8,5,4,6.6,6,9.4,7.2,6.5,10,6.8,7.4,8,8,10,6.31,8.1,7,7.6,8.5,4.8,8.1,5,6.76,10,9,5,7.76,5.5,7.2,6.5,6.8,10,8.5,8,6.7,8.5,7.8,9.3,9.8,11,5.96,5.1,7.3,7.8,7,7,6.9,5.7,9.4,7.3,6.5,7],\"text\":[\"Restaurant: Pugsley's Pizza\",\"Restaurant: Bond 45\",\"Restaurant: Dulono's Pizza\",\"Restaurant: Fat Lorenzo's\",\"Restaurant: Red's Savoy Pizza\",\"Restaurant: Frank From Philly \u0026 Andrea Pizza\",\"Restaurant: LA Traviata Pizzeria\",\"Restaurant: Percy's Pizza\",\"Restaurant: Mediterraneo Restaurant\",\"Restaurant: Tre Sorelle\",\"Restaurant: Napolese Pizzeria\",\"Restaurant: Bearno's By the Bridge\",\"Restaurant: The Original Impellizeri's Pizza\",\"Restaurant: Marcello's Pizza \u0026 Subs\",\"Restaurant: Piu Bello Pizzeria Restaurant\",\"Restaurant: Buckhead Pizza\",\"Restaurant: Picasso Pizza\",\"Restaurant: Pomodoro Pizza\",\"Restaurant: Majestic Pizza\",\"Restaurant: Friendly Gourmet Pizza\",\"Restaurant: Harry's Italian\",\"Restaurant: Little Italy Pizza\",\"Restaurant: Phil's Pizza\",\"Restaurant: Kest√© Pizza \u0026 Vino\",\"Restaurant: Napoli Pizza\",\"Restaurant: Lucali\",\"Restaurant: Snacks\",\"Restaurant: Canal Pizza\",\"Restaurant: Melani Pizzeria\",\"Restaurant: Il Piccolo Bufalo\",\"Restaurant: Pomodoro Ristorante \u0026 Pizzeria\",\"Restaurant: Santarpio's Pizza\",\"Restaurant: Regina Pizzeria\",\"Restaurant: Nick's Pizza\",\"Restaurant: PQR\",\"Restaurant: Vinnie's Pizzeria\",\"Restaurant: La Mia Pizza\",\"Restaurant: Manhattan Brick Oven Pizza\",\"Restaurant: Francesco's Pizzeria\",\"Restaurant: Arturo's\",\"Restaurant: Song E Napule\",\"Restaurant: Sfila Pizza\",\"Restaurant: Merilu Pizza Al Metro\",\"Restaurant: Bricco Ristorante Italiano\",\"Restaurant: Serafina Broadway\",\"Restaurant: Don Antonio\",\"Restaurant: Daniela Trattoria\",\"Restaurant: Patzeria Perfect Pizza\",\"Restaurant: John's of Times Square\",\"Restaurant: Radio City Pizza\",\"Restaurant: Kiss My Slice\",\"Restaurant: Cassiano's Pizza\",\"Restaurant: Angelo Bellini\",\"Restaurant: Jiannetto's Pizza Truck\",\"Restaurant: Prova Pizzabar\",\"Restaurant: Giuseppe's Pizza\",\"Restaurant: Posto\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\rplot_ly(d2, x = ~comm_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\r ## No trace type specified:\r## Based on info supplied, a 'scatter' trace seems appropriate.\r## Read more about this trace type -\u0026gt; https://plotly.com/r/reference/#scatter\r## No scatter mode specifed:\r## Setting the mode to markers\r## Read more about this attribute -\u0026gt; https://plotly.com/r/reference/#scatter-mode\r \r{\"x\":{\"visdat\":{\"71c017945fd0\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"71c017945fd0\",\"attrs\":{\"71c017945fd0\":{\"x\":{},\"y\":{},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"comm_score\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"dave_score\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[7.992,7.74230769230769,5.7625,4.36666666666667,6.125,7.45,7.35384615384615,6.725,5.79,5.7,6.725,7.23333333333333,8.44444444444444,8.06153846153846,7.47777777777778,8.35714285714286,6.5,5,6.55,5.9,6.8125,8.15454545454545,7.33684210526316,5.72857142857143,7.4,8.14666666666667,6.25,6.375,7.31428571428571,5.94166666666667,5.4,3,7.36666666666667,7.45,5.3,7.1625,6.88,8.4,6.3,6.25,7.92777777777778,4.6,3.1,9.16923076923077,8.07575757575758,6.97857142857143,7.75581395348837,3.33333333333333,7.77777777777778,8.00454545454545,7.425,7.77222222222222,5.2375,7.625,7.73333333333333,10,6.1,6.675,6.92,4.925,5.76666666666667,6.77142857142857,5.42857142857143,8.05,8.1,6.9625,6.97222222222222,6.9,7.6,7.89333333333333,8.01071428571429,7.61428571428571,7.8,6.1,8.1,7.65714285714286,4.25,7.99130434782609,8.16923076923077,7.6,7.8,5.725,6.75,6.475,7.2625,7.2,8.27708333333333,6.1,7.13333333333333,6.3,7.83636363636364,7.48181818181818,6.17894736842105,8.07948717948718,7.84444444444444,8.28141592920354,5.525,7.80606060606061,8.41538461538462,7.75,8.35681818181818,7.6125,6.2,8.29189189189189,7.48888888888889,7.2,8.03076923076923,8.2125,8.44444444444444,7.58684210526316,6.79523809523809,7.73846153846154,4.4,5.1,7.9,3.95714285714286,7.2,5.95,5.82857142857143,6.8,7.64,7.23636363636364,5.8,6.70588235294118,7.00392156862745,7.21666666666667,7.38,6.77333333333333,6.5,6.8,7.77142857142857,7.7,6.36428571428571,9.07380952380952,7.5,4.63333333333333,7.4,7.25,7.97692307692308,7.878125,7.71,7.71111111111111,7.05,7.32826086956522,8.05,7.775,8.88236331569665,7.56666666666667,7.9,8.8,7.14285714285714,6.7,7.54666666666667,7.03846153846154,7.07777777777778,6.44,7.80740740740741,6.8,7.70833333333333,7.6,8.85862068965517,8.53478260869565,5.82857142857143,8.875,8.79473684210526,8.34823529411765,8.92535211267606,6.34,6.97333333333333,8.412,5.16666666666667,5.925,7.84523809523809,7.88181818181818,4.825,4.08181818181818,6.78888888888889,6.12105263157895,5.76666666666667,3.2,7.4,8.19,6.21666666666667,7.10285714285714,5.13333333333333,7.53333333333333,8.4,6.675,6.45,6.82,7.80666666666667,7.47272727272727,7.5,7.25,7.88108108108108,8.40526315789474,8.35694444444444,8.17818181818182,7.64545454545454,8.6,7.78333333333333,5,8.57619047619048,7.85454545454546,7.64545454545454,7.475,7.58333333333333,7.95238095238095,8.52056074766355,8.43703703703704,8.264,8.34590163934426,9.1,4.4,7.26666666666667,7.4,5.95,8.36,7.55454545454545,8.11612903225806,3.65,5.9,7.51111111111111,7.075,6.14545454545454,6.63333333333333,8.2,4.15,5.25,7.2,4.5,7.80714285714286,7.73793103448276,8.1,8.25,7.71818181818182,7.2,7.3,8.60567375886525,8.7025641025641,7.29090909090909,7.8,7.15,7.53684210526316,7.29,8.30697674418605,8.66923076923077,7.63333333333333,8.125,6.8375,8.125,7.58571428571429,8.32,7.24285714285714,7.4,8.1,7.3,6.3,8.06,7.89,8.14,8.70769230769231,6.4,5.95,4.2,8.46,7.5,6.73888888888889,8.1,7.25333333333333,8.71076923076923,7.56666666666667,6.91666666666667,7.50123456790124,7.70416666666667,8.1037037037037,6.68333333333333,7.47619047619048,7.94285714285714,8.67941176470588,3.3625,6.7,8.94117647058824,8.25185185185185,8.545,8.66818181818182,8.2,6.9,6.225,5.97272727272727,8.65443786982248,8.72331606217617,8.94117647058824,7.89791666666667,7.44615384615385,8.02083333333333,9.06363636363636,7.44285714285714,8.23333333333333,7.42,6.7,7.025,7.16,7.80909090909091,6.75,7.25,6.7,7.45769230769231,5.8,7.14285714285714,6.63076923076923,6.4,5.67142857142857,8.08717948717949,7.95714285714286,3.55,5.96666666666667,7.64,8.33333333333333,7.60909090909091,7.13684210526316,5.725,8.38571428571429,8.28571428571429,8.45,7.16666666666667,8.16129032258065,8.29583333333333,7.9,7.63333333333333,7.76521739130435,7.62307692307692,7.8,6.83333333333333,5.6,7.70555555555555,7.975,6.4,7,5.46666666666667,6.86666666666667,8.6,6.55,7.55555555555555,8.63055555555556,4.15,7.5,5.58,3.11666666666667,7.71578947368421,7.15,8.03333333333333,6.85,6.5,8.40504201680672,5.92,4,7.46153846153846,8.94233870967742,3.7,7.88333333333333,7.7,7.225,8.4,4.95,6.16666666666667,6.9,5.6,6.375,7.65,4.03333333333333,5.76666666666667,6.0625,6.3,7.95,6.53333333333333,6.46666666666667,7.62142857142857,7.54444444444445,7.7,6.88571428571429,6.0625,6.0875,4.725,7.73333333333333,6.01111111111111,7.88828828828829,6.3,5.7,6.02857142857143,6.3,6.25,7.3,7.2,6.775,7.2125,6.6,8.36666666666667,6.5,8.225,6.22307692307692,5.02,6.81764705882353,5.87894736842105,7.2,7.64,7.53214285714286,7.86875,6.964,6.1,1,8.2,8.50761904761905,6.18333333333333,6.9,7.68571428571429,7.37,5.23333333333333,6.64,7.721875,6.15,7.03333333333333],\"y\":[7.7,8.6,4.9,3.2,6.1,7.2,6.8,6.5,6.5,6.4,6.6,7.5,8.6,7.8,7.2,7.5,6.3,3.4,4.1,7.1,6.3,7.6,6.8,5.3,6.8,7.1,5.4,6.1,7.6,4.2,6.2,5.8,7.1,6.7,3.4,6.8,6.9,7.2,6.7,6.3,7.7,8.5,3.2,6.8,8.4,6.2,8.2,1.1,7.1,7.4,7.1,8.1,5.1,4.1,7.1,5.2,6.4,5.8,0.2,3.1,5.4,6.8,1.2,7.6,7.8,5.4,6.9,6.4,1.2,7.4,7.8,6.5,6.1,6.4,7.2,7.1,7.2,8.3,7.1,6.8,6.1,7.4,5.9,7.2,7.1,7.2,8.3,6.8,6.4,6.6,7.5,7.5,6.5,8.3,7.5,8.9,1.9,7.5,7.9,6.6,8.1,7.8,7.2,7.9,6.8,6.8,6.2,7.2,7.6,8.1,6.8,7.4,4.2,4.2,6.8,2.2,7.4,3.3,6.4,5.9,7.4,4.8,2.5,7.1,7.1,5.9,8.1,7.4,4.8,7.2,7.7,7.8,6.3,9.3,7.9,3.4,7.8,7.7,8.2,7.9,8.2,7.2,6.2,6.9,7.6,8.1,9.3,7.4,7.8,8.2,6.4,2.4,7.1,5.8,4.8,5.5,7.9,6.6,5.1,6.9,8.1,8.9,6.9,2.2,8.1,7.4,9.4,7.5,7.4,8.1,6.7,5.4,8.3,8.2,3.2,3.2,6.2,3.4,5,7.8,5.6,7.2,5.9,7.3,7.8,8.2,8.6,7.4,6.1,7.3,8.1,7.2,6.3,7.2,7.9,8.1,7.8,8.6,7.4,6.7,7.4,6.6,8,6.8,7.8,6.7,7.9,7.9,9.1,8,6.9,8.7,7.4,6.2,7.4,1.7,6.8,7.3,8.3,8.6,2.4,6.1,8,5.8,5.1,6.2,7.4,4.8,7.4,5.8,5.9,8.5,8.1,7.4,7.3,6.2,8.1,4.2,8.6,8.1,7.2,8.1,4.8,8.2,7.3,8,8.4,6.6,7.1,7.3,7.7,7.5,9.3,8,7.4,8.6,7.2,5.9,6.2,7.6,10,8.2,6.8,3.8,6.1,8.2,6.9,6.7,6.6,5.8,9.1,7.74,6.8,9.2,8.2,9.2,5.2,7.1,6.7,8,2.4,6.2,8.5,7.5,9,9.4,8.5,6.6,6.4,6.4,8.8,8.5,9.2,7.4,8.2,8.4,9.1,6.2,6.6,7.3,6.9,6.1,7.1,7.8,6.7,7.7,6.2,8.4,1.8,4,7.2,7.9,6.5,8.5,7.9,7.1,6.2,8.1,8.3,8.1,8.1,6.2,7.6,7.9,7.7,7,9,8.5,7.2,7.7,8.1,8.2,6.8,7.1,3.7,8.4,8.1,6.3,7.1,5.8,2.1,7.3,6.4,6.4,9.3,3.1,4.8,4.1,4.1,8.1,3.6,7.1,5,4.8,8.4,6.1,6.3,8.4,9.1,8.1,7.8,7.7,4.6,6.9,6.9,6.2,6.9,1.7,1.4,6.9,4.2,6.2,7.8,6.6,7.4,7.3,6.3,7.3,7.4,7.9,7.3,6.6,6.7,6.4,7.1,7.8,8.2,7.7,7.9,5.4,5.2,4.3,5.7,6.6,6.2,7.8,6.8,8.6,8.4,8.1,5.1,4.9,7.5,7.4,6.2,6.8,7.1,8.1,7.4,5.7,0.08,8.1,9.3,2.8,6.8,7.9,7.7,5.9,3.9,8.2,6.8,8.2],\"text\":[\"Restaurant: Pugsley's Pizza\",\"Restaurant: Williamsburg Pizza\",\"Restaurant: 99 Cent Fresh Pizza\",\"Restaurant: La Gusto Pizza\",\"Restaurant: Cheesy Pizza\",\"Restaurant: Sal \u0026 Carmine's Pizza\",\"Restaurant: MAMA'S TOO!\",\"Restaurant: Bond 45\",\"Restaurant: Casa D'amici\",\"Restaurant: Pizza Al's\",\"Restaurant: Colasante's Ristorante \u0026 Pub\",\"Restaurant: Love \u0026 Dough\",\"Restaurant: Patsy's Pizzeria\",\"Restaurant: Louie \u0026 Ernie's Pizza\",\"Restaurant: Pizza Barn\",\"Restaurant: Mama's Pizza\",\"Restaurant: Dulono's Pizza\",\"Restaurant: Cheetah Pizza\",\"Restaurant: Fat Lorenzo's\",\"Restaurant: Red's Savoy Pizza\",\"Restaurant: Frank From Philly \u0026 Andrea Pizza\",\"Restaurant: Young Joni\",\"Restaurant: Cossetta Alimentari\",\"Restaurant: Pizza Luce\",\"Restaurant: Football Pizza\",\"Restaurant: Star Tavern\",\"Restaurant: LA Traviata Pizzeria\",\"Restaurant: Percy's Pizza\",\"Restaurant: Pizza Land\",\"Restaurant: Little Italy Pizza\",\"Restaurant: B \u0026 W Deli \u0026 Pizzeria\",\"Restaurant: Mediterraneo Restaurant\",\"Restaurant: La Bellezza Pizza\",\"Restaurant: Tre Sorelle\",\"Restaurant: Justino's Pizzeria\",\"Restaurant: Casabianca Pizzeria\",\"Restaurant: Mariella Pizza\",\"Restaurant: Bella Blu\",\"Restaurant: John \u0026 Tony's\",\"Restaurant: Il Forno Pizza\",\"Restaurant: Scarr's Pizza\",\"Restaurant: Double Zero\",\"Restaurant: Rico's Pizza\",\"Restaurant: Palermo's 95th\",\"Restaurant: Giordano's\",\"Restaurant: Gino's East - Magnificent Mile\",\"Restaurant: Lou Malnati's Pizzeria\",\"Restaurant: Georgio's Gourmet Pizza\",\"Restaurant: Armand's Pizzeria\",\"Restaurant: Pequod's Pizzeria\",\"Restaurant: Geo's Pizza\",\"Restaurant: Goodfellas Pizzeria\",\"Restaurant: BazbeauxPizza\",\"Restaurant: Napolese Pizzeria\",\"Restaurant: The Wig \u0026 Pen Pizza Pub\",\"Restaurant: Casey's General Store\",\"Restaurant: Bearno's By the Bridge\",\"Restaurant: The Original Impellizeri's Pizza\",\"Restaurant: The Post\",\"Restaurant: Cottage Inn Pizza - Ann Arbor\",\"Restaurant: New York Pizza Depot\",\"Restaurant: Pizza House\",\"Restaurant: Backroom Pizza\",\"Restaurant: Buddy's Pizza\",\"Restaurant: Leone's Pizza\",\"Restaurant: Adriatico's New York Style Pizza\",\"Restaurant: Sicilia Fine Italian Specialties\",\"Restaurant: The O Patio \u0026 Pub\",\"Restaurant: Catfish Biff's Pizza \u0026 Subs\",\"Restaurant: GoreMade Pizza\",\"Restaurant: Pies \u0026 Pints - Lexington\",\"Restaurant: Pazzo's Pizza Pub\",\"Restaurant: PieTana\",\"Restaurant: Bruno Bros Pizza\",\"Restaurant: Wedgewood Fernando's Pizza - Austintown\",\"Restaurant: Avalon Downtown Pizzeria\",\"Restaurant: Sparky's Pizzeria and Grill\",\"Restaurant: Black Sheep Pizza - Minneapolis\",\"Restaurant: Benny Marzano's\",\"Restaurant: Imperial Pizza\",\"Restaurant: Mellow Mushroom\",\"Restaurant: Marcello's Pizza \u0026 Subs\",\"Restaurant: Piu Bello Pizzeria Restaurant\",\"Restaurant: Buckhead Pizza\",\"Restaurant: Cosmo's Original Little Italy Pizza\",\"Restaurant: Fellini's Pizza\",\"Restaurant: Antico Pizza\",\"Restaurant: Slice Downtown\",\"Restaurant: Hawthorne's NY Pizza \u0026 Bar\",\"Restaurant: Pizza Joint\",\"Restaurant: Empire Slice House\",\"Restaurant: Santucci's Original Square Pizza\",\"Restaurant: Lorenzo \u0026 Sons Pizza\",\"Restaurant: Santillo's Brick Oven Pizza\",\"Restaurant: Kinchley's Tavern\",\"Restaurant: Ralph's Pizzeria\",\"Restaurant: Costco\",\"Restaurant: Pizza Town USA\",\"Restaurant: Brother's Pizzeria\",\"Restaurant: Mama Rosa Pizza and Pasta\",\"Restaurant: Denino's Pizzeria Tavern\",\"Restaurant: The New Park Tavern\",\"Restaurant: Vesta Wood Fired Pizza \u0026 Bar\",\"Restaurant: Lee's Tavern\",\"Restaurant: The Original Goodfella's Brick Oven Pizza\",\"Restaurant: Tony Boloney's\",\"Restaurant: Krispy Pizza\",\"Restaurant: Mario's Classic Pizza\",\"Restaurant: Johnny Pepperoni\",\"Restaurant: Basile's Pizza\",\"Restaurant: Benny Tudino's Pizzeria\",\"Restaurant: Napoli's Brick Oven Pizza\",\"Restaurant: Picasso Pizza\",\"Restaurant: Pomodoro Pizza\",\"Restaurant: SIM√í PIZZA\",\"Restaurant: Steve's Pizza\",\"Restaurant: The Woodstock\",\"Restaurant: The Grotto Pizzeria \u0026 Ristorante\",\"Restaurant: Georgio Pizzeria\",\"Restaurant: Pranzo Pizza \u0026 Pasta\",\"Restaurant: Justino's Pizzeria\",\"Restaurant: Adrienne's Pizzabar\",\"Restaurant: Da Vinci Pizza\",\"Restaurant: Cucina Bene\",\"Restaurant: Majestic Pizza\",\"Restaurant: Underground Pizza\",\"Restaurant: Neapolitan Express\",\"Restaurant: Stage Door Pizza\",\"Restaurant: Friendly Gourmet Pizza\",\"Restaurant: Dona Bella Pizza\",\"Restaurant: Harry's Italian\",\"Restaurant: Adoro Lei\",\"Restaurant: Little Italy Pizza\",\"Restaurant: Lucali\",\"Restaurant: Il Mattone West Village\",\"Restaurant: 5 Boroughs Pizza\",\"Restaurant: Serafina Meatpacking\",\"Restaurant: Phil's Pizza\",\"Restaurant: Saluggi's\",\"Restaurant: Artichoke Basille's Pizza\",\"Restaurant: Brunetti Pizza\",\"Restaurant: Emily - West Village\",\"Restaurant: Black Square Pizza\",\"Restaurant: Bleecker Street Pizza\",\"Restaurant: Enzo Bruni La Pizza Gourmet\",\"Restaurant: Famous Ben's Pizza\",\"Restaurant: John's of Bleecker Street\",\"Restaurant: Filaga Pizzeria\",\"Restaurant: Emmett's\",\"Restaurant: Numero 28 Pizzeria - West Village\",\"Restaurant: Kest√© Pizza \u0026 Vino\",\"Restaurant: King's Pizza\",\"Restaurant: Eddie \u0026 Sam's NY Pizza\",\"Restaurant: Home Slice Pizza\",\"Restaurant: Big Lou's Pizza\",\"Restaurant: Pizzarita's\",\"Restaurant: Julian's Italian Pizzeria \u0026 Kitchen\",\"Restaurant: Napoli Pizza\",\"Restaurant: Biggie's Pizza\",\"Restaurant: Steve's Pizza\",\"Restaurant: Lucali\",\"Restaurant: Mister O1- South Beach\",\"Restaurant: Pizza Girls WPB\",\"Restaurant: Pizza Al Fresco\",\"Restaurant: Totonno's\",\"Restaurant: L \u0026 B Spumoni Gardens\",\"Restaurant: Di Fara Pizza\",\"Restaurant: Da Nonna Rosa\",\"Restaurant: Ciro's Pizzeria \u0026 Beerhouse\",\"Restaurant: Bronx Pizza\",\"Restaurant: Filippi's Pizza Grotto Pacific Beach\",\"Restaurant: Manhattan Pizzeria\",\"Restaurant: Joe's Pizza\",\"Restaurant: Vito's Pizza\",\"Restaurant: Bonanno's New York Pizzeria\",\"Restaurant: Bonanno's New York Pizzeria\",\"Restaurant: Five50 Pizza Bar\",\"Restaurant: Secret Pizza\",\"Restaurant: Snacks\",\"Restaurant: XS Nightclub\",\"Restaurant: Naked City Pizza Shop\",\"Restaurant: Good Pie\",\"Restaurant: Evel Pie\",\"Restaurant: Pizza Rock\",\"Restaurant: Acquolina\",\"Restaurant: Pope's Italian Restaurant\",\"Restaurant: Nove Italian Restaurant\",\"Restaurant: Mama Mia's Restaurant\",\"Restaurant: Marino's Pizza\",\"Restaurant: Gennaro's Pizza Parlor\",\"Restaurant: Caputo's Pizzeria\",\"Restaurant: Corner Slice\",\"Restaurant: Francesca's Restaurant \u0026 Pizzeria\",\"Restaurant: Caf√© Crust\",\"Restaurant: Borrelli's\",\"Restaurant: King Umberto's of Elmont\",\"Restaurant: New Park Pizza\",\"Restaurant: Umberto's Pizzeria \u0026 Restaurant\",\"Restaurant: Dani's House of Pizza\",\"Restaurant: Rosa's Pizza\",\"Restaurant: Brooklyn's Homeslice Pizzeria\",\"Restaurant: Not Ray's Pizza\",\"Restaurant: Roberta's\",\"Restaurant: Little Vincent's Pizza\",\"Restaurant: Carmine's Pizzeria\",\"Restaurant: Sal's Pizzeria\",\"Restaurant: Tony's Pizza\",\"Restaurant: L'industrie Pizzeria\",\"Restaurant: Best Pizza\",\"Restaurant: Grimaldi's\",\"Restaurant: Juliana's Pizza\",\"Restaurant: Ignazio's\",\"Restaurant: La Nonna Krispy Krust Pizza\",\"Restaurant: Alphonso's Pizzeria \u0026 Trattoria\",\"Restaurant: Paulie Gee's Slice Shop\",\"Restaurant: Canal Pizza\",\"Restaurant: Melani Pizzeria\",\"Restaurant: Paulie Gee's\",\"Restaurant: Speedy Romeo\",\"Restaurant: Rizzo's Fine Pizza\",\"Restaurant: Stanton Pizza\",\"Restaurant: Solo Pizza\",\"Restaurant: La Margarita Pizza\",\"Restaurant: Luna Ristorante\",\"Restaurant: Rosario's Pizza\",\"Restaurant: Da Gennaro\",\"Restaurant: Margherita NYC\",\"Restaurant: Yankee Pizza\",\"Restaurant: Il Piccolo Bufalo\",\"Restaurant: Da Nico Ristorante\",\"Restaurant: Little Gio's Pizza\",\"Restaurant: Lil' Frankie's\",\"Restaurant: Sal's Little Italy\",\"Restaurant: Gnocco\",\"Restaurant: Pasquale Jones\",\"Restaurant: Lombardi's Pizza\",\"Restaurant: Champion Pizza\",\"Restaurant: Pomodoro Ristorante \u0026 Pizzeria\",\"Restaurant: Prince Street Pizza\",\"Restaurant: Rubirosa\",\"Restaurant: Proto's Pizza\",\"Restaurant: 310 Bowery Bar\",\"Restaurant: La Rossa\",\"Restaurant: Stromboli Pizza\",\"Restaurant: Baker's Pizza\",\"Restaurant: East Village Pizza\",\"Restaurant: Joe \u0026 Pat's Pizzeria\",\"Restaurant: Muzzarella Pizza\",\"Restaurant: Sorbillo Pizzeria\",\"Restaurant: Joey's Pizzeria\",\"Restaurant: Pizza Barbone\",\"Restaurant: Palio Pizzeria\",\"Restaurant: Oath Pizza - Nantucket\",\"Restaurant: Steamboat Wharf Pizza\",\"Restaurant: Fusaro's\",\"Restaurant: Pi Pizzeria\",\"Restaurant: Sophie T's Pizza\",\"Restaurant: Muse Pizza\",\"Restaurant: Moto Pizza\",\"Restaurant: Poopsie's\",\"Restaurant: Monte's Restaurant\",\"Restaurant: Bianchi's Pizza\",\"Restaurant: Giordano's Restaurant\",\"Restaurant: Slice Of Edgartown\",\"Restaurant: Isola\",\"Restaurant: Santarpio's Pizza\",\"Restaurant: Rocco's Pizzeria\",\"Restaurant: Rinas Pizzeria \u0026 Cafe\",\"Restaurant: Galleria Umberto\",\"Restaurant: Pushcart Pizzeria\",\"Restaurant: Regina Pizzeria\",\"Restaurant: Ducali\",\"Restaurant: 1000 Degrees Neapolitan Pizzeria\",\"Restaurant: Halftime Pizza\",\"Restaurant: Florina Pizzeria \u0026 Paninoteca\",\"Restaurant: Rosie's Subs and Pizza\",\"Restaurant: Felcaro Pizzeria\",\"Restaurant: Dirty Water Dough\",\"Restaurant: Eataly Boston\",\"Restaurant: Lynwood Cafe\",\"Restaurant: Fenway Park\",\"Restaurant: Baldie's Pizza \u0026 Subs\",\"Restaurant: Cape Cod Cafe\",\"Restaurant: Louie's Pizza\",\"Restaurant: Town Spa Pizza\",\"Restaurant: Frank Pepe Pizzeria Napoletana - Chestnut Hill\",\"Restaurant: Tony's Place\",\"Restaurant: Chilmark General Store\",\"Restaurant: Bill's Pizzeria Kitchen + Grille\",\"Restaurant: Regina Pizzeria\",\"Restaurant: Modern Apizza\",\"Restaurant: Frank Pepe Pizzeria Napoletana\",\"Restaurant: Sally's Apizza\",\"Restaurant: BAR\",\"Restaurant: Riko's Pizza\",\"Restaurant: Colony Grill\",\"Restaurant: Johnny's Pizzeria\",\"Restaurant: Full Moon Pizzeria\",\"Restaurant: Nick's Pizza\",\"Restaurant: Pie Pie Pizza\",\"Restaurant: Mama's Pizzeria\",\"Restaurant: PQR\",\"Restaurant: Vinnie's Pizzeria\",\"Restaurant: Salvo's Pizzabar\",\"Restaurant: La Mia Pizza\",\"Restaurant: Italian Village Pizzeria\",\"Restaurant: Broadway Pizza \u0026 Restaurant\",\"Restaurant: Saba's Pizza\",\"Restaurant: Cafe Viva\",\"Restaurant: Pizza Park\",\"Restaurant: Pizza Pete's\",\"Restaurant: Manhattan Brick Oven Pizza\",\"Restaurant: Oath Pizza - East 67th Street\",\"Restaurant: Brooklyn Pizza Masters\",\"Restaurant: Made In New York Pizza\",\"Restaurant: Kiss My Slice\",\"Restaurant: Farinella Bakery Pizza\",\"Restaurant: Cafe Daniello's\",\"Restaurant: Gato\",\"Restaurant: Freddie \u0026 Pepper's\",\"Restaurant: Francesco's Pizzeria\",\"Restaurant: Iggy's Pizzeria\",\"Restaurant: Motorino\",\"Restaurant: Arturo's\",\"Restaurant: Song E Napule\",\"Restaurant: Fiore's Pizza\",\"Restaurant: Denino's Greenwich Village\",\"Restaurant: Joe's Pizza\",\"Restaurant: Cafe Fiorello\",\"Restaurant: Sfila Pizza\",\"Restaurant: Sacco Pizza\",\"Restaurant: Merilu Pizza Al Metro\",\"Restaurant: Bricco Ristorante Italiano\",\"Restaurant: New York Sal's Pizza\",\"Restaurant: Mariella Pizza\",\"Restaurant: Luigi's Pizza\",\"Restaurant: Luigi's Gourmet Grill\",\"Restaurant: B Side Pizza \u0026 Wine Bar\",\"Restaurant: Uncle Mario's Brick Oven Pizza\",\"Restaurant: Bella Vita Pizzeria\",\"Restaurant: IPizzaNY\",\"Restaurant: Serafina Broadway\",\"Restaurant: Claudio Pizzeria\",\"Restaurant: Don Antonio\",\"Restaurant: Angelo's Coal Oven Pizzeria\",\"Restaurant: Z Deli Pizzeria\",\"Restaurant: Pazza Notte\",\"Restaurant: Carve\",\"Restaurant: A Slice of New York\",\"Restaurant: PizzArte\",\"Restaurant: Daniela Trattoria\",\"Restaurant: Capizzi\",\"Restaurant: Patzeria Perfect Pizza\",\"Restaurant: Pasta Lovers\",\"Restaurant: John's of Times Square\",\"Restaurant: Radio City Pizza\",\"Restaurant: Tavola\",\"Restaurant: Vinny Vincenz\",\"Restaurant: Sauce Pizzeria\",\"Restaurant: Pisa Pizza\",\"Restaurant: Frank's Trattoria\",\"Restaurant: Pepe Giallo\",\"Restaurant: Don Giovanni Ristorante\",\"Restaurant: Ovest Pizzoteca by Luzzo's\",\"Restaurant: 10th Avenue Pizza \u0026 Cafe\",\"Restaurant: Stella's Pizza\",\"Restaurant: Gotham Pizza\",\"Restaurant: Highline Pizza\",\"Restaurant: Rocky's Pizza 14th St\",\"Restaurant: Village Pizza\",\"Restaurant: Pizza Italia\",\"Restaurant: Famous Original Ray's Pizza\",\"Restaurant: Rivoli Pizza\",\"Restaurant: Pizza Rollio\",\"Restaurant: Rosemary's Pizza\",\"Restaurant: Teresa's Brick Oven Pizza \u0026 Cafe\",\"Restaurant: Kiss My Slice\",\"Restaurant: Harry's Italian Pizza Bar\",\"Restaurant: Upside Pizza\",\"Restaurant: Sofia Pizza Shoppe\",\"Restaurant: Marinara Pizza\",\"Restaurant: Cassiano's Pizza\",\"Restaurant: Primavera Pizza \u0026 Pasta\",\"Restaurant: Belmora Pizza \u0026 Restaurant\",\"Restaurant: Zia Maria-Chelsea\",\"Restaurant: Joe's Pizza\",\"Restaurant: NY Pizza Suprema\",\"Restaurant: 3 Giovani\",\"Restaurant: Rock Pizza Scissors\",\"Restaurant: Ben's Pizzeria\",\"Restaurant: Olio e Pi√∫\",\"Restaurant: My Pie Pizzeria Romana\",\"Restaurant: Angelo Bellini\",\"Restaurant: Pizza By Cert√©\",\"Restaurant: La Vera Pizzeria \u0026 Restaurant\",\"Restaurant: La Bellezza\",\"Restaurant: La Trattoria\",\"Restaurant: Patsy's Pizzeria\",\"Restaurant: Picciotto NYC\",\"Restaurant: Jiannetto's Pizza Truck\",\"Restaurant: Uncle Paul's Pizza\",\"Restaurant: Previti Pizza\",\"Restaurant: Royal Pizza\",\"Restaurant: Prova Pizzabar\",\"Restaurant: Giuseppe's Pizza\",\"Restaurant: Garlic New York Pizza Bar\",\"Restaurant: Lions \u0026 Tigers \u0026 Squares Detroit Pizza\",\"Restaurant: Lombardi's pizza\",\"Restaurant: Famous Famiglia Pizzeria\",\"Restaurant: Famous Amadeus Pizza\",\"Restaurant: Amtrak\",\"Restaurant: Nicoletta Pizzeria\",\"Restaurant: Lazzara's Pizza\",\"Restaurant: Rocky's Number II\",\"Restaurant: Posto\",\"Restaurant: Patrizia's Pizza and Pasta\",\"Restaurant: Mike's Pizza\",\"Restaurant: Joey Pepperoni's Pizza\",\"Restaurant: Libretto's Pizzeria\",\"Restaurant: Brick Oven Pizza 33\",\"Restaurant: J's Pizza\",\"Restaurant: Rocco's Pizza Joint\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\rplot_ly(d2, x = ~crit_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\r ## No trace type specified:\r## Based on info supplied, a 'scatter' trace seems appropriate.\r## Read more about this trace type -\u0026gt; https://plotly.com/r/reference/#scatter\r## No scatter mode specifed:\r## Setting the mode to markers\r## Read more about this attribute -\u0026gt; https://plotly.com/r/reference/#scatter-mode\r \r{\"x\":{\"visdat\":{\"71c0714d2df2\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"71c0714d2df2\",\"attrs\":{\"71c0714d2df2\":{\"x\":{},\"y\":{},\"text\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"crit_score\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"dave_score\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[8.8,4.3,6.2,4.5,5.8,8.7,8,5,4,7.9,6.6,6,7,9.4,7.2,6.5,10,6.8,7.4,8,8,10,6.31,8.1,7,7.6,8.5,4.8,8.1,5,5.4,6.76,10,9,5,7.76,5.5,7.2,6.5,6.8,10,8.5,8,6.7,8.5,7.8,9.3,9.8,11,5.96,5.1,7.3,7.8,7,7,6.9,5.7,9.4,5.9,7.3,6.5,7],\"y\":[7.7,6.9,6.5,6.3,4.1,7.1,6.3,5.4,6.1,7.1,5.8,6.7,1.2,4.1,6.4,5.8,7.4,5.9,7.2,4.2,4.2,7.1,4.8,7.7,6.3,7.7,6.4,6.6,8.1,5,5.4,1.7,6.8,7.4,4.2,8.2,6.4,6.6,6.1,7.1,6.7,7.9,8.1,7.9,7.7,7.7,8.2,6.8,7.3,6.4,3.6,5,8.4,6.1,6.3,6.6,5.7,8.1,6.1,7.4,6.2,6.8],\"text\":[\"Restaurant: Pugsley's Pizza\",\"Restaurant: Nino's 46\",\"Restaurant: Bond 45\",\"Restaurant: Dulono's Pizza\",\"Restaurant: Fat Lorenzo's\",\"Restaurant: Red's Savoy Pizza\",\"Restaurant: Frank From Philly \u0026 Andrea Pizza\",\"Restaurant: LA Traviata Pizzeria\",\"Restaurant: Percy's Pizza\",\"Restaurant: Re Sette\",\"Restaurant: Mediterraneo Restaurant\",\"Restaurant: Tre Sorelle\",\"Restaurant: Buca di Beppo Italian Restaurant\",\"Restaurant: Napolese Pizzeria\",\"Restaurant: Bearno's By the Bridge\",\"Restaurant: The Original Impellizeri's Pizza\",\"Restaurant: Marcello's Pizza \u0026 Subs\",\"Restaurant: Piu Bello Pizzeria Restaurant\",\"Restaurant: Buckhead Pizza\",\"Restaurant: Picasso Pizza\",\"Restaurant: Pomodoro Pizza\",\"Restaurant: Majestic Pizza\",\"Restaurant: Friendly Gourmet Pizza\",\"Restaurant: Harry's Italian\",\"Restaurant: Little Italy Pizza\",\"Restaurant: Phil's Pizza\",\"Restaurant: Kest√© Pizza \u0026 Vino\",\"Restaurant: Napoli Pizza\",\"Restaurant: Lucali\",\"Restaurant: Snacks\",\"Restaurant: 42nd Street Pizza\",\"Restaurant: Canal Pizza\",\"Restaurant: Melani Pizzeria\",\"Restaurant: Il Piccolo Bufalo\",\"Restaurant: Pomodoro Ristorante \u0026 Pizzeria\",\"Restaurant: Santarpio's Pizza\",\"Restaurant: Regina Pizzeria\",\"Restaurant: Nick's Pizza\",\"Restaurant: PQR\",\"Restaurant: Vinnie's Pizzeria\",\"Restaurant: La Mia Pizza\",\"Restaurant: Manhattan Brick Oven Pizza\",\"Restaurant: Francesco's Pizzeria\",\"Restaurant: Arturo's\",\"Restaurant: Song E Napule\",\"Restaurant: Sfila Pizza\",\"Restaurant: Merilu Pizza Al Metro\",\"Restaurant: Bricco Ristorante Italiano\",\"Restaurant: Serafina Broadway\",\"Restaurant: Don Antonio\",\"Restaurant: Daniela Trattoria\",\"Restaurant: Patzeria Perfect Pizza\",\"Restaurant: John's of Times Square\",\"Restaurant: Radio City Pizza\",\"Restaurant: Kiss My Slice\",\"Restaurant: Cassiano's Pizza\",\"Restaurant: Angelo Bellini\",\"Restaurant: Jiannetto's Pizza Truck\",\"Restaurant: Deli On Madison\",\"Restaurant: Prova Pizzabar\",\"Restaurant: Giuseppe's Pizza\",\"Restaurant: Posto\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\rSo apparently the lousy restaurant that got a 1 from the community and almost 0 from Dave is called Amtrak. I‚Äôm wondering if that refers to pizza on Amtrak trains? Just for the heck of it and because I‚Äôm curious, let‚Äôs look at that entry.\nd2 %\u0026gt;% filter(name == \u0026quot;Amtrak\u0026quot;) %\u0026gt;% knitr::kable()\r    name address1 city zip country latitude longitude price_level provider_rating provider_review_count review_stats_all_average_score review_stats_all_count review_stats_all_total_score review_stats_community_average_score review_stats_community_count review_stats_community_total_score review_stats_critic_average_score review_stats_critic_count review_stats_critic_total_score review_stats_dave_average_score review_stats_dave_count review_stats_dave_total_score comm_score crit_score dave_score     Amtrak 234 W 31st St New York 10001 US 40.74965 -73.9934 0 3 345 0.54 2 1.08 1 1 1 0 0 0 0.08 1 0.08 1 NA 0.08    I googled the address, and it seems to be indeed Amtrak. Note to self: Never order pizza on an Amtrak train.\nPrice vs ratings analysis Next, let‚Äôs look at possible impact of restaurant price level on rating.\ntable(d2$price_level)\r ## ## 0 1 2 3 ## 21 216 218 8\r There isn‚Äôt much spread, most pizza places are in the middle. Maybe not too surprising. Let‚Äôs look at a few plots to see if there is a pattern. First, we should recode price level as a factor.\nd2 \u0026lt;- d2 %\u0026gt;% mutate(price = as.factor(price_level))\r p1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=price, y=comm_score)) + geom_violin() + geom_point()\rp2 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=price, y=crit_score)) + geom_violin() + geom_point()\rp3 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=price, y=dave_score)) + geom_violin() + geom_point()\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\r Hard to tell if there‚Äôs a trend. Could do some stats to look in more detail, but since this exercise focuses on exploring, I won‚Äôt do that. Instead I‚Äôll leave it at that.\nRating versus location Ok, on to the last of the questions I started out with. Maybe there are some areas where restaurants are in general better? Or maybe an area where diners are more critical? Let‚Äôs see if there is some correlation between ratings and location.\ntable(d2$country)\r ## ## US ## 463\r sort(table(d2$city))\r ## ## Alpharetta Augusta Austin Austintown ## 1 1 1 1 ## Blacksburg Braintree Brockton Buffalo ## 1 1 1 1 ## Charleston Charlotte Chestnut Hill Chilmark ## 1 1 1 1 ## Clearwater Clifton Coralville Daytona Beach ## 1 1 1 1 ## Dearborn Dennis Port DUMBO East Meadow ## 1 1 1 1 ## Edina Elizabeth Elmont Gansevoort ## 1 1 1 1 ## Gary Hampton Bays Hopkinton Howard Beach ## 1 1 1 1 ## Huntington Iowa City Jackson Jacksonville Beach ## 1 1 1 1 ## Jersey City Kew Gardens Lakeville Lawrenceville ## 1 1 1 1 ## Lynn Manhattan Beach Mashantucket Miami ## 1 1 1 1 ## Middle Village Mount Vernon New Hyde Park New York City ## 1 1 1 1 ## North Arlington Nutley Oak Bluffs Oak Lawn ## 1 1 1 1 ## Oklahoma City Orange Palm Beach Pembroke ## 1 1 1 1 ## Princeton Ramsey Randolph Revere ## 1 1 1 1 ## Rutherford San Francisco Sandwich Southampton ## 1 1 1 1 ## Stoughton Tampa Vineyard Haven West Melbourne ## 1 1 1 1 ## West Palm Beach West Roxbury Woburn Yonkers ## 1 1 1 1 ## East Rutherford Edgartown Elmwood Park Hyannis ## 2 2 2 2 ## Miami Beach Philadelphia Saint Paul Santa Monica ## 2 2 2 2 ## Stamford Bronx Indianapolis Lexington ## 2 3 3 3 ## Morgantown San Antonio San Diego Ann Arbor ## 3 3 3 4 ## Louisville New Haven Staten Island Youngstown ## 4 4 4 4 ## Atlanta Chicago Columbus Hoboken ## 6 6 6 6 ## Nantucket Saratoga Springs Minneapolis Las Vegas ## 6 6 8 11 ## Boston Brooklyn New York ## 13 20 251\r Ok so all restaurants are in the US, and most are in New York. We could look at NY versus ‚Äúrest of the cities‚Äù. Though isn‚Äôt Brooklyn (the 2nd largest entry) basically a part of New York? I‚Äôm not enough of an expert on all things NY to be sure (for any real analysis, you need to know a good bit about the subject matter, or work closely with a subject matter expert. If not, more likely than not something dumb will happen).\nFor now, I assume that it‚Äôs different enough, and make 2 categories, NY and ‚Äúother‚Äù and see if there are differences. Let‚Äôs try.\np1 \u0026lt;- d2 %\u0026gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %\u0026gt;%\rggplot(aes(x=newcity, y = comm_score)) + geom_violin() + geom_point()\rp2 \u0026lt;- d2 %\u0026gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %\u0026gt;%\rggplot(aes(x=newcity, y = crit_score)) + geom_violin() + geom_point()\rp3 \u0026lt;- d2 %\u0026gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %\u0026gt;%\rggplot(aes(x=newcity, y = dave_score)) + geom_violin() + geom_point()\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\r Looks like the community in NY gives lower scores compared to other locations, less noticeable difference for critics and Dave.\nOk, the next analysis might not make much sense, but why not check if there is a North-South or East-West trend related to ratings. Maybe restaurants are better in one of those directions? Or people in the South are more polite and give better scores? üòÅ. I‚Äôm mostly doing this because longitude and latitude are continuous variables, so I can make a few more scatterplots. I don‚Äôt have any real goal for this otherwise.\np1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=longitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\rp2 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=longitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\rp3 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=longitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\r ## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r So as we go from the west (-120) to the east (-70), there is a trend in restaurants getting higher scores, by all 3 groups. I guess as we are moving closer to Italy, the pizza quality goes up? üòÉ.\nNext, let‚Äôs look at latitude.\np1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=latitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\rp2 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=latitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\rp3 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=latitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\r ## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r So doesn‚Äôt seem as much of a trend going from South (25) to North (45). That finding of course fully confirms our ‚Äúcloser to Italy‚Äù theory!\nOk, I was going to leave it at that with location, but since I‚Äôm already going down a crazy rabbit hole regarding Italy, let‚Äôs do it for real: We‚Äôll take both longitude and latitude of each restaurant and use it compute the distance of each location to Naples, the home of Pizza. And then we‚Äôll plot that and see.\nSince I have no idea how to do that, I need Google. Fortunately, the first hit worked, found this one: https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r\nLet‚Äôs try.\ncoord_naples=cbind(rep(14.2,nrow(d2)),rep(40.8,nrow(d2))) #location of naples\rcoord_restaurants = cbind(d2$longitude,d2$latitude)\rdistvec = rep(0,nrow(d2))\rfor (n in 1:nrow(d2))\r{\rdistvec[n] = distm( coord_restaurants[n,], coord_naples[n,], fun = distGeo)\r}\rd2$distvec = distvec / 1609 #convert to miles since we are in the US :)\r It‚Äôs not tidyverse style, which I tried first but couldn‚Äôt get it to work. The trusty old for-loop seems to always work for me. I checked the numbers in distvec, they look reasonable.\nOk, let‚Äôs redo the plots above, now with distance to Naples.\np1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=distvec, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\rp2 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=distvec, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\rp3 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=distvec, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\rcowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12, nrow = 3)\r ## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r## `geom_smooth()` using formula 'y ~ x'\r Hm ok, no smoking gun. Looks like there is a bit of a trend that the further away you are from Naples, the lower the score. But really not much.\nHyping our result But since this distance-from-Naples makes such a good story, let‚Äôs see if I can hype it.\nFirst, to increase potential statistical strength, I‚Äôll combine all 3 scores into an overall mean, i.e.¬†similar ot the all variable in the original. I don‚Äôt trust that one since I don‚Äôt know if they averaged over 0 instead of properly treating it as NA. Of course I could check, but I‚Äôm just re-creating it here.\nd2$all_score = rowMeans(cbind(d2$dave_score,d2$crit_score,d2$comm_score),na.rm=TRUE)\r Ok, let‚Äôs check if correlation between this new score and distance is significant!\n#compute a linear fit and p-value (it's significant!)\rfit=lm(d2$all_score ~ d2$distvec, data = d2)\rsummary(fit)\r ## ## Call:\r## lm(formula = d2$all_score ~ d2$distvec, data = d2)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -6.7854 -0.5866 0.3027 0.9612 2.3686 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 8.9895014 0.7008802 12.826 \u0026lt; 2e-16 ***\r## d2$distvec -0.0004772 0.0001525 -3.129 0.00187 ** ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 1.478 on 459 degrees of freedom\r## (2 observations deleted due to missingness)\r## Multiple R-squared: 0.02089, Adjusted R-squared: 0.01875 ## F-statistic: 9.791 on 1 and 459 DF, p-value: 0.001865\r pval=anova(fit)$`Pr(\u0026gt;F)`[1]\rprint(pval)\r ## [1] 0.001865357\r It is signficant, p\u0026lt;0.05! We hit pay dirt! Let‚Äôs make a great looking figure and go tell the press!\n#make final plot\rp1 \u0026lt;- d2 %\u0026gt;% ggplot(aes(x=distvec, y = all_score)) + geom_point(shape = 21, colour = \u0026quot;black\u0026quot;, fill = \u0026quot;red\u0026quot;, size = 2 ) + geom_smooth(method = 'lm', se = TRUE, color = \u0026quot;darkgreen\u0026quot;, size = 2) + xlab('Distance from Naples (miles)') + ylab('Pizza Quality (score)') + ylim(c(2.5,max(d2$all_score))) + theme_bw() +theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=\u0026quot;bold\u0026quot;)) + annotate(\u0026quot;text\u0026quot;, x=6000, y=9, label= paste(\u0026quot;p =\u0026quot;,round(pval,4)),size = 12) ggsave('pizzadistance.png')\rknitr::include_graphics(\u0026quot;pizzadistance.png\u0026quot;)\r The ‚Äúpress release‚Äù A novel study of pizza restaurants in the US found a clear, statistically significant correlation between the distance of the restaurant to Naples and the quality of the pizza as determined by the community and expert restaurant critics. The study authors attribute the finding to the ability of restaurants that are closer to Naples to more easily get genuine fresh and high quality ingredients, such as the famous San Marzano tomatoes.\nSummary That was a fun exploration. It was the first time I played with the tidyverse data. I had no idea which direction it was going to go, and ideas just came as I was doing it. I‚Äôm sure there is interesting stuff in datasets 1 and 3 as well, but I already spent several hours on this and will therefore call it quits now.\nWhile the exercise was supposed to focus on cleaning/wrangling and visualizing, I couldn‚Äôt resist going all the way at the end and producing a statistically significant and somewhat plausible sounding finding. If this were a ‚Äúreal‚Äù study/analysis, such a nice result would be happily accepted by most analysts/authors, hyped by a university press release and - if the result is somewhat interesting/cute, picked up by various media outlets.\nI had no idea at the beginning what I was going to analyze, I did that longitude/latitude analysis on a whim, and if I hadn‚Äôt found this correlation and had that crazy distance to Italy idea, nothing would have happened. But now that I have a significant result and a good story to go with, I can publish! It‚Äôs not really much sillier than for instance the Chocolate and Nobel Laureates paper paper.\nWhat I illustrated here (without having had any plan to do so), is a big, general problem in secondary data analysis. It‚Äôs perfectly ok to do secondary analyses, and computing significance is also (kinda) ok, but selling exploratory (fishing) results as inferential/causal/confirmatory is wrong - and incredibly widespread. If you want to sharpen your critical thinking skills related to all those supposed significant and real findings in science we see a lot, a great (though at times sobering) read is Andrew Gelman‚Äôs blog where he regularly picks apart studies/results like the one I did here or the chocolate and Nobel laureates one. And now I‚Äôll go eat some chocolate so I can increase my chances for a Nobel prize.\n","date":1570838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"15316578ab3df2f454378e784ca4a7e3","permalink":"https://www.andreashandel.com/posts/tidytuesday-analysis/","publishdate":"2019-10-12T00:00:00Z","relpermalink":"/posts/tidytuesday-analysis/","section":"posts","summary":"An analysis of TidyTuesday data for pizza restaurants and their ratings.","tags":["R","Data Analysis"],"title":"Analysis of pizza restaurants","type":"posts"},{"authors":["Andreas Handel"],"categories":["talk"],"content":"Preparation  Install R and (optional) Rstudio. Install the DSAIRM R package and take a brief look. This website provides instructions.  Outline I cover the following topics in this workshop (my co-teacher Paul Thomas covers the immunology part):\n Introduction to modeling Some example models How to use simulation models Sources of uncertainty Types of models How to build (good) models How to assess modeling studies  Presentation Slides All pdf slides as zip file\n","date":1563321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563321600,"objectID":"392e7d2e94fc0ccfad1f3efe00ed4487","permalink":"https://www.andreashandel.com/presentations/2019-07-sismid/","publishdate":"2019-07-17T00:00:00Z","relpermalink":"/presentations/2019-07-sismid/","section":"presentations","summary":"An introductory workshop on Infectious Diseases, Immunology and Within-Host Models.","tags":["teaching","infectious disease","immunology"],"title":"Introduction to within-host modeling","type":"presentations"},{"authors":["Andreas Handel"],"categories":["Presentation","teaching","Infectious disease modeling"],"content":"Preparation  Install R and (optional) Rstudio. Install the DSAIDE R package and take a brief look. This website provides instructions. Optional: Read this paper which provides background reading for the \u0026ldquo;ID Control of multiple outbreaks\u0026rdquo; app in DSAIDE.  Outline The following topics are covered in this workshop:\n Introduction to infectious disease modeling Some example models How to use simulation models Types of models Sources of uncertainty How to build (good) models How to assess modeling studies Active learning of infectious disease epidemiology  Presentation Slides All pdf slides as zip file\n","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562716800,"objectID":"c8b1829a459aa2fdce92f080b8d8decf","permalink":"https://www.andreashandel.com/presentations/2019-07-hangzhou/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/presentations/2019-07-hangzhou/","section":"presentations","summary":"An introductory workshop on infectious disease modeling.","tags":["teaching","infectious disease"],"title":"Introduction to Infectious Disease Modeling","type":"presentations"},{"authors":null,"categories":null,"content":"Website content is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n   ","date":1561676400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561676400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://www.andreashandel.com/license/","publishdate":"2019-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"Website content is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-NC-SA","type":"page"},{"authors":null,"categories":["software"],"content":"This - still in development - R package allows teachers to easily administer and auto-grade quizzes that students submit online. It replaces the functionality often found in learning management systems (LMS). Student submissions can be fully analyzed to gain insights into problem areas. Learn more about it on the package website.\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"38dc0e48bc04fb156c70014553b0e346","permalink":"https://www.andreashandel.com/my-projects/quizgrader/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/my-projects/quizgrader/","section":"my-projects","summary":"R package that allows easy administration, submission and grading of online quizzes without the need of a learning management system.","tags":["R Package","Teaching Resource","software"],"title":"quizgrader","type":"my-projects"},{"authors":null,"categories":null,"content":"We are developing an R package that allows users to build and analyze compartmental, dynamical, mechanistic models (implemented as differential equations, discrete-time or stochastic), without the need to write computer code. Learn more about it on the package website.\n","date":1552608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552608000,"objectID":"7f084be35cd81ae21397fe212f1982a8","permalink":"https://www.andreashandel.com/my-projects/modelbuilder/","publishdate":"2019-03-15T00:00:00Z","relpermalink":"/my-projects/modelbuilder/","section":"my-projects","summary":"R package that allows building and analysis of simulation models without the need to write code.","tags":["Mechanistic Models","R Package","Teaching Resource","software"],"title":"modelbuilder","type":"my-projects"},{"authors":null,"categories":["software"],"content":"We developed an R package that allows anyone to easily create high-quality ggplot2 based flow diagrams of simulation models (and other flowcharts) with just a few lines of R code. Learn more about it on the package website.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"c1bf68aa401795c7adaf73106119c2ad","permalink":"https://www.andreashandel.com/my-projects/flowdiagramr/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/my-projects/flowdiagramr/","section":"my-projects","summary":"R package that allows easy creation of high-quality flow diagrams.","tags":["Visualization","R Package","software"],"title":"flowdiagramr","type":"my-projects"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://www.andreashandel.com/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A short academic bio","tags":null,"title":"Biography","type":"widget_page"},{"authors":null,"categories":["software"],"content":"An R package that contains several Shiny/learnr based tutorials that teach introductory aspects of biostatistics in an interactive way. Learn more about it on the package website.\n","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"e6ca7a90a57611ee7b32d59451c17815","permalink":"https://www.andreashandel.com/my-projects/iblir/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/my-projects/iblir/","section":"my-projects","summary":"R package containing a collection of online, interactive shiny/learnr based labs that teach basics of biostatistics.","tags":["Biostatistics","R Package","Teaching Resource","software"],"title":"iblir - Introduction to Biostatistics Labs in R","type":"my-projects"},{"authors":null,"categories":["Other"],"content":"A simple website with a collection of lists with links to various resources that are related to my research and teaching, as well as some general (academic) career content.\n","date":1393632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393632000,"objectID":"367dd525e4fa33518a5b897de11c9b86","permalink":"https://www.andreashandel.com/my-projects/resourcelist/","publishdate":"2014-03-01T00:00:00Z","relpermalink":"/my-projects/resourcelist/","section":"my-projects","summary":"A simple website containing lists with links ot teaching and research resources.","tags":["Infectious Disease","Shiny","Research","Other"],"title":"Resource list website","type":"my-projects"},{"authors":null,"categories":["Other"],"content":"An R/Shiny app that visualizes COVID-19 cases, hospitalizations, deaths and tests for US states and counties, as well as countries around the world. This app was created by William Norfolk, Robbie Richards and Andreas Handel. Check out the app here. The About section of the app contains more information.\n","date":1330560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1330560000,"objectID":"263d98bcc693096e66f1e733ac85a254","permalink":"https://www.andreashandel.com/my-projects/yact/","publishdate":"2012-03-01T00:00:00Z","relpermalink":"/my-projects/yact/","section":"my-projects","summary":"An R/Shiny app that visualizes COVID-19 data.","tags":["Infectious Disease","Shiny","Research","Other"],"title":"COVID-19 tracker","type":"my-projects"}]