---
title: Using R to analyze publications - part 1
subtitle: Some examples using Google Scholar
summary: Some code and examples showing how to process and analyze meta-data for a set of publications using the `scholar` R package. 
author: Andreas Handel
draft: true
date: '2020-02-01'
slug: publications-analysis-1
categories: []
tags: []
featured: no
disable_jquery: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


```{r, include = FALSE}
#remotes::install_github('emo')
#library('emo')
```

# Overview

I needed some information on all my publications for "bean counting" purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications. 

Instead of doing things by hand, there are nicer/faster ways using R. The following shows a few things one can do with the `scholar` package.

# Notes

* As of this writing, the `scholar` R package does seems semi-dormant and not under active development. If Google changes their API for Scholar and the package isn't updated, the below code might stop working. 

* A problem I keep encountering with Google Scholar is that it starts blocking requests, even after what I consider are not that many attempts to retrieve data. I found that happen when I try to pull references from Google Scholar using `JabRef` and also with the code below. If that happens for you, try a different computer, or clear cookies, or try [tutorial 2]().

* To minimize chances of getting locked out by Google, I wrote the code below such that it only sends a request if there isn't a local file already containing that data. To refresh data, delete the local files.

# Required packages

```{r, include = TRUE}
library(scholar)
library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)
```

# Getting all citations for one (or more) individuals

```{r}
#Define the person to analyze
scholar_id="bruHK0YAAAAJ" 
#either load existing file of publications, or get a new one from Google Scholar
#delete the file to force an update
if (file.exists('citations.Rds'))
{
  cites <- readRDS('citations.Rds')
} else {
  cites <- scholar::get_citation_history(scholar_id) #get citations
  saveRDS(cites,'citations.Rds')
}
```

# Compare citations for different time periods

For my purpose, I want to compare citations between 2 time periods (my Assistant Professor time and my Associate Professor time). I'm splitting them into 2.

```{r}
#define who to analyze. Really only needed if more than 1 person's information is pulled above.
period_1_start = 2009
period_2_start = 2015
cites_1 <- cites %>% dplyr::filter(id == scholar_id, (year>=period_1_start & year<period_2_start ))
cites_2 <- cites %>% dplyr::filter(id == scholar_id, (year>=period_2_start & year<2020 )) #remove last year since it's not a full year
```

Fitting a linear model to both time segments to look at increase in citations over both periods.
```{r}
fit1=lm(cites ~ year, data = cites_1)
fit2=lm(cites ~ year, data = cites_2)
inc1 = fit1$coefficients["year"]
inc2 = fit2$coefficients["year"] 
print(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))
```

Making a figure to show citation count increases

```{r}
# combine data above into single data frame
#add a variable to indicate period 1 and period 2
cites_1$group = "1"
cites_2$group = "2"
cites_df = rbind(cites_1,cites_2)

#make the plot and show linear fit lines
p1 <- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) + geom_point(size = I(4)) + geom_smooth(method="lm",aes(group = group), se = F, size=1.5)  
# format axes and legend
p2 <- p1 + scale_x_continuous(name = "Year", breaks = cites_df$year, labels = cites_df$year) + scale_y_continuous("Citations according to Google Scholar") + theme_bw(base_size=14) + theme(legend.position="none") 
# add text to plot
p3 <- p2 + geom_text(aes(NULL,NULL),x=2010.8,y=150,label="Average annual \n increase 23%",color="black",size=5.5)
p4 <- p3 + geom_text(aes(NULL,NULL),x=2017,y=150,label="Average annual \n increase 45%",color="black",size=5.5) 

#open a new graphics window
#note that this is Windows specific. Use quartz() for MacOS
ww=5; wh=5; 
windows(width=ww, height=wh)					
print(p4)
dev.print(device=png,width=ww,height=wh,units="in",res=600,file="citations.png")
```

# Getting list of co-authors

```{r}
#get all pubs for an author (or multiple)
if (file.exists('publications.Rds'))
{
  pubs <- readRDS('publications.Rds')
} else {
  publications <- scholar::get_publications(scholar_id) #get citations
  saveRDS(publications,'publications.Rds')
}
```

Unfortunately, `get_publications()` only pulls from the main page, which cuts off the author list. 
To get all authors, one needs to run through each paper using `get_complete_authors()`. 
The problem is that Google cuts off access if one sends too many queries. If you get error messages, it might be that Google blocked you. Try the suggestions above. 

```{r}
all_authors = list()
if (file.exists('allauthors.Rds'))
{
  allauthors <- readRDS('allauthors.Rds')
} else {
  for (n in 1:nrow(publications)) 
  {
    all_authors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)
    Sys.sleep(2) #wait 2 seconds between each query. Might help with Google not thinking we are querying too much/too fast. Not sure if it really helps though. 
  }
  #saveRDS(all_authors,'allauthors.Rds')
}
```


I want to separately look at publications in the 2 time periods I defined above
```{r}
#pubs_old <- pubs %>% dplyr::filter((year>=period_1_start & year<period_2_start ))
#pubs_new <- pubs %>% dplyr::filter(year>=period_2_start)
```


# Making a table of journals and impact factors

```{r}
#here I only want publications since 2015
pub_reduced <- publications %>% dplyr::filter(year>2014)
ifdata <- get_impactfactor(pub_reduced$journal) 
#sort and remove non-journal entries since Google SCholar collects all kinds of 'publications', including items other than standard peer-reviewed papers
iftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()
knitr::kable(iftable)
```

Ok so this doesn't quite work. I know for instance that I didn't publish anything in _Cancer Journal for Clinicians_ and the 2 _Rheumatology_ entries are workshop presentations. Oddly, when I look at `pubs$journal` there is no Cancer Journal listed. Somehow this is a bug created by the `get_impactfactor()` function. I could fix that by hand. The bigger problem is what to do with all those publications that are not peer-reviewed papers. I could remove them from my Google scholar profile. But I kind of want to keep them there since some of them link to useful stuff. I could alternatively manually clean things at this step. This somewhat defeats the purpose of automation. 
I do keep all my published, peer-reviewed papers in a bibtex bibliography file in my reference manager (I'm using Zotero and/or Jabref). I know that file is "clean" and only contains real peer-reviewed papers. Unfortunately, the `scholar` package can't read in such data.


# Discussion




